{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import math\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from tqdm import tqdm\n",
    "from preprocessing import preprocessing, get_mrcg, get_mfcc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from src.utils.optimization import WarmupLinearSchedule\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000\n",
    "frame_length = 0.025  # 25ms\n",
    "frame_stride = 0.01  # 10ms\n",
    "n_fft = int(round(sr * frame_length))\n",
    "hop_length = int(round(sr * frame_stride))\n",
    "n_mfcc = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_repo = os.path.join('.', 'Data', 'sample_data')\n",
    "\n",
    "samples_0 = glob.glob(os.path.join(sample_data_repo, '0', '**', '*wav'), recursive=True)\n",
    "samples_0 = sorted(samples_0)\n",
    "\n",
    "samples_20 = glob.glob(os.path.join(sample_data_repo, '20', '**', '*wav'), recursive=True)\n",
    "samples_20 = sorted(samples_20)\n",
    "\n",
    "samples_40 = glob.glob(os.path.join(sample_data_repo, '40', '**', '*wav'), recursive=True)\n",
    "samples_40 = sorted(samples_40)\n",
    "\n",
    "samples_60 = glob.glob(os.path.join(sample_data_repo, '60', '**', '*wav'), recursive=True)\n",
    "samples_60 = sorted(samples_60)\n",
    "\n",
    "samples_80 = glob.glob(os.path.join(sample_data_repo, '80', '**', '*wav'), recursive=True)\n",
    "samples_80 = sorted(samples_80)\n",
    "\n",
    "samples_100 = glob.glob(os.path.join(sample_data_repo, '100', '**', '*wav'), recursive=True)\n",
    "samples_100 = sorted(samples_100)\n",
    "\n",
    "samples_120 = glob.glob(os.path.join(sample_data_repo, '120', '**', '*wav'), recursive=True)\n",
    "samples_120 = sorted(samples_120)\n",
    "\n",
    "samples_140 = glob.glob(os.path.join(sample_data_repo, '140', '**', '*wav'), recursive=True)\n",
    "samples_140 = sorted(samples_140)\n",
    "\n",
    "samples_160 = glob.glob(os.path.join(sample_data_repo, '160', '**', '*wav'), recursive=True)\n",
    "samples_160 = sorted(samples_160)\n",
    "\n",
    "samples_180 = glob.glob(os.path.join(sample_data_repo, '180', '**', '*wav'), recursive=True)\n",
    "samples_180 = sorted(samples_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vad_seg_repo = os.path.join('.', 'Data', 'binary_segment')   # 적절하게 변경 필요\n",
    "\n",
    "samples_vad_seg_0 = glob.glob(os.path.join(sample_vad_seg_repo, '0', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg_0 = sorted(samples_vad_seg_0)   \n",
    "\n",
    "samples_vad_seg_20 = glob.glob(os.path.join(sample_vad_seg_repo, '20', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg_20 = sorted(samples_vad_seg_20)   \n",
    "\n",
    "samples_vad_seg_40 = glob.glob(os.path.join(sample_vad_seg_repo, '40', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg_40 = sorted(samples_vad_seg_40)   \n",
    "\n",
    "samples_vad_seg_60 = glob.glob(os.path.join(sample_vad_seg_repo, '60', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg_60 = sorted(samples_vad_seg_60)   \n",
    "\n",
    "samples_vad_seg_80 = glob.glob(os.path.join(sample_vad_seg_repo, '80', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg_80 = sorted(samples_vad_seg_80)   \n",
    "\n",
    "samples_vad_seg_100 = glob.glob(os.path.join(sample_vad_seg_repo, '100', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg_100 = sorted(samples_vad_seg_100)   \n",
    "\n",
    "samples_vad_seg_120 = glob.glob(os.path.join(sample_vad_seg_repo, '120', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg_120 = sorted(samples_vad_seg_120)   \n",
    "\n",
    "samples_vad_seg_140 = glob.glob(os.path.join(sample_vad_seg_repo, '140', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg_140 = sorted(samples_vad_seg_140)   \n",
    "\n",
    "samples_vad_seg_160 = glob.glob(os.path.join(sample_vad_seg_repo, '160', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg_160 = sorted(samples_vad_seg_160)   \n",
    "\n",
    "samples_vad_seg_180 = glob.glob(os.path.join(sample_vad_seg_repo, '180', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg_180 = sorted(samples_vad_seg_180)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatio_tensor_instances(array_2d, seq_len, hop, label):\n",
    "    \"\"\"\n",
    "    array_2d : ndarray.  STFT magnitude or phase.\n",
    "    dest_path : file path\n",
    "    seq_len : number of frames in an instance.\n",
    "    label : segmented labels.  0 and 1's. The same length as original wav file of the audio sample. \n",
    "    \"\"\"\n",
    "    row_size, col_size = array_2d.shape[0], array_2d.shape[1]\n",
    "    ratio = len(label)/col_size  # ratio : how many data points per frame \n",
    "    stack_array = []    # 4D tensor that will hold the instances\n",
    "    label_array = []\n",
    "\n",
    "    j=0\n",
    "    while j <= (col_size - (seq_len+1)): \n",
    "        context_frame = array_2d[:, j:(j+seq_len)]\n",
    "        # seg_label = round( label[int(j*ratio):int((j+seq_len)*ratio)].mean() ) \n",
    "        threshold = 0.5  # if greater than the threshold, then speech \n",
    "        seg_label = 1 if label[int(j*ratio):int((j+seq_len)*ratio)].mean() > threshold else 0\n",
    "        \n",
    "        stack_array.append(context_frame[:,:,np.newaxis])   # make context_frame to 3d tensor & append \n",
    "        label_array.append(seg_label)\n",
    "            \n",
    "        j = j+hop\n",
    "        \n",
    "    return np.stack(stack_array, axis=0), label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((282, 40, 50, 1), (282,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples = len(samples_0) \n",
    "\n",
    "mfcc_instances_0 = []    # elements are ndarrays\n",
    "label_instances_0 = []         # elements are lists\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    voice_noise_label = np.load(samples_vad_seg_0[i])\n",
    "    if('npy' in samples_vad_seg_0[i].split('/')[-1]):\n",
    "        label = voice_noise_label[0]        # use the left channel label.  this take care of 0 degree problem\n",
    "    else:                                   # npz file\n",
    "        label = voice_noise_label[\"label\"]    \n",
    "    mfcc = get_mfcc(samples_0[i], sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # generate instances with 1.16 sec duration (100 frames), at every 0.116 sec apart (10 hops)\n",
    "    mfcc_instances_sub, label_sub = generatio_tensor_instances(mfcc, 50, 10, label)\n",
    "\n",
    "    mfcc_instances_0.append(mfcc_instances_sub)\n",
    "    label_instances_0.append(np.array(label_sub))\n",
    "    \n",
    "np.concatenate(mfcc_instances_0).shape, np.concatenate(label_instances_0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1296, 40, 50, 1), (1296,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples = len(samples_20) \n",
    "\n",
    "mfcc_instances_20 = []    # elements are ndarrays\n",
    "label_instances_20 = []         # elements are lists\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    voice_noise_label = np.load(samples_vad_seg_20[i])\n",
    "    if('npy' in samples_vad_seg_20[i].split('/')[-1]):\n",
    "        label = voice_noise_label[0]        # use the left channel label.  this take care of 0 degree problem\n",
    "    else:                                   # npz file\n",
    "        label = voice_noise_label[\"label\"]    \n",
    "    mfcc = get_mfcc(samples_20[i], sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # generate instances with 1.16 sec duration (100 frames), at every 0.116 sec apart (10 hops)\n",
    "    mfcc_instances_sub, label_sub = generatio_tensor_instances(mfcc, 50, 10, label)\n",
    "    \n",
    "    mfcc_instances_20.append(mfcc_instances_sub)\n",
    "    label_instances_20.append(np.array(label_sub))\n",
    "    \n",
    "np.concatenate(mfcc_instances_20).shape, np.concatenate(label_instances_20).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1404, 40, 50, 1), (1404,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples = len(samples_40) \n",
    "\n",
    "mfcc_instances_40 = []    # elements are ndarrays\n",
    "label_instances_40 = []         # elements are lists\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    voice_noise_label = np.load(samples_vad_seg_40[i])\n",
    "    if('npy' in samples_vad_seg_40[i].split('/')[-1]):\n",
    "        label = voice_noise_label[0]        # use the left channel label.  this take care of 0 degree problem\n",
    "    else:                                   # npz file\n",
    "        label = voice_noise_label[\"label\"]    \n",
    "    mfcc = get_mfcc(samples_40[i], sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # generate instances with 1.16 sec duration (100 frames), at every 0.116 sec apart (10 hops)\n",
    "    mfcc_instances_sub, label_sub = generatio_tensor_instances(mfcc, 50, 10, label)\n",
    "    \n",
    "    mfcc_instances_40.append(mfcc_instances_sub)\n",
    "    label_instances_40.append(np.array(label_sub))\n",
    "    \n",
    "np.concatenate(mfcc_instances_40).shape, np.concatenate(label_instances_40).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((333, 40, 50, 1), (333,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples = len(samples_60) \n",
    "\n",
    "mfcc_instances_60 = []    # elements are ndarrays\n",
    "label_instances_60 = []         # elements are lists\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    voice_noise_label = np.load(samples_vad_seg_60[i])\n",
    "    if('npy' in samples_vad_seg_60[i].split('/')[-1]):\n",
    "        label = voice_noise_label[0]        # use the left channel label.  this take care of 0 degree problem\n",
    "    else:                                   # npz file\n",
    "        label = voice_noise_label[\"label\"]    \n",
    "    mfcc = get_mfcc(samples_60[i], sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # generate instances with 1.16 sec duration (100 frames), at every 0.116 sec apart (10 hops)\n",
    "    mfcc_instances_sub, label_sub = generatio_tensor_instances(mfcc, 50, 10, label)\n",
    "    \n",
    "    mfcc_instances_60.append(mfcc_instances_sub)\n",
    "    label_instances_60.append(np.array(label_sub))\n",
    "    \n",
    "np.concatenate(mfcc_instances_60).shape, np.concatenate(label_instances_60).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1692, 40, 50, 1), (1692,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples = len(samples_80) \n",
    "\n",
    "mfcc_instances_80 = []    # elements are ndarrays\n",
    "label_instances_80 = []         # elements are lists\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    voice_noise_label = np.load(samples_vad_seg_80[i])\n",
    "    if('npy' in samples_vad_seg_80[i].split('/')[-1]):\n",
    "        label = voice_noise_label[0]        # use the left channel label.  this take care of 0 degree problem\n",
    "    else:                                   # npz file\n",
    "        label = voice_noise_label[\"label\"]    \n",
    "    mfcc = get_mfcc(samples_80[i], sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # generate instances with 1.16 sec duration (100 frames), at every 0.116 sec apart (10 hops)\n",
    "    mfcc_instances_sub, label_sub = generatio_tensor_instances(mfcc, 50, 10, label)\n",
    "    \n",
    "    mfcc_instances_80.append(mfcc_instances_sub)\n",
    "    label_instances_80.append(np.array(label_sub))\n",
    "    \n",
    "np.concatenate(mfcc_instances_80).shape, np.concatenate(label_instances_80).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1836, 40, 50, 1), (1836,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples = len(samples_100) \n",
    "\n",
    "mfcc_instances_100 = []    # elements are ndarrays\n",
    "label_instances_100 = []         # elements are lists\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    voice_noise_label = np.load(samples_vad_seg_100[i])\n",
    "    if('npy' in samples_vad_seg_100[i].split('/')[-1]):\n",
    "        label = voice_noise_label[0]        # use the left channel label.  this take care of 0 degree problem\n",
    "    else:                                   # npz file\n",
    "        label = voice_noise_label[\"label\"]    \n",
    "    mfcc = get_mfcc(samples_100[i], sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # generate instances with 1.16 sec duration (100 frames), at every 0.116 sec apart (10 hops)\n",
    "    mfcc_instances_sub, label_sub = generatio_tensor_instances(mfcc, 50, 10, label)\n",
    "    \n",
    "    mfcc_instances_100.append(mfcc_instances_sub)\n",
    "    label_instances_100.append(np.array(label_sub))\n",
    "    \n",
    "np.concatenate(mfcc_instances_100).shape, np.concatenate(label_instances_100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1476, 40, 50, 1), (1476,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples = len(samples_140) \n",
    "\n",
    "mfcc_instances_140 = []    # elements are ndarrays\n",
    "label_instances_140 = []         # elements are lists\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    voice_noise_label = np.load(samples_vad_seg_140[i])\n",
    "    if('npy' in samples_vad_seg_140[i].split('/')[-1]):\n",
    "        label = voice_noise_label[0]        # use the left channel label.  this take care of 0 degree problem\n",
    "    else:                                   # npz file\n",
    "        label = voice_noise_label[\"label\"]    \n",
    "    mfcc = get_mfcc(samples_140[i], sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # generate instances with 1.16 sec duration (100 frames), at every 0.116 sec apart (10 hops)\n",
    "    mfcc_instances_sub, label_sub = generatio_tensor_instances(mfcc, 50, 10, label)\n",
    "    \n",
    "    mfcc_instances_140.append(mfcc_instances_sub)\n",
    "    label_instances_140.append(np.array(label_sub))\n",
    "    \n",
    "np.concatenate(mfcc_instances_140).shape, np.concatenate(label_instances_140).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1836, 40, 50, 1), (1836,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples = len(samples_160) \n",
    "\n",
    "mfcc_instances_160 = []    # elements are ndarrays\n",
    "label_instances_160 = []         # elements are lists\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    voice_noise_label = np.load(samples_vad_seg_160[i])\n",
    "    if('npy' in samples_vad_seg_160[i].split('/')[-1]):\n",
    "        label = voice_noise_label[0]        # use the left channel label.  this take care of 0 degree problem\n",
    "    else:                                   # npz file\n",
    "        label = voice_noise_label[\"label\"]    \n",
    "    mfcc = get_mfcc(samples_160[i], sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # generate instances with 1.16 sec duration (100 frames), at every 0.116 sec apart (10 hops)\n",
    "    mfcc_instances_sub, label_sub = generatio_tensor_instances(mfcc, 50, 10, label)\n",
    "    \n",
    "    mfcc_instances_160.append(mfcc_instances_sub)\n",
    "    label_instances_160.append(np.array(label_sub))\n",
    "    \n",
    "np.concatenate(mfcc_instances_160).shape, np.concatenate(label_instances_160).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((349, 40, 50, 1), (349,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples = len(samples_180) \n",
    "\n",
    "mfcc_instances_180 = []    # elements are ndarrays\n",
    "label_instances_180 = []         # elements are lists\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    voice_noise_label = np.load(samples_vad_seg_180[i])\n",
    "    if('npy' in samples_vad_seg_180[i].split('/')[-1]):\n",
    "        label = voice_noise_label[0]        # use the left channel label.  this take care of 0 degree problem\n",
    "    else:                                   # npz file\n",
    "        label = voice_noise_label[\"label\"]    \n",
    "    mfcc = get_mfcc(samples_180[i], sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # generate instances with 1.16 sec duration (100 frames), at every 0.116 sec apart (10 hops)\n",
    "    mfcc_instances_sub, label_sub = generatio_tensor_instances(mfcc, 50, 10, label)\n",
    "    \n",
    "    mfcc_instances_180.append(mfcc_instances_sub)\n",
    "    label_instances_180.append(np.array(label_sub))\n",
    "    \n",
    "np.concatenate(mfcc_instances_180).shape, np.concatenate(label_instances_180).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_instances_0 = np.concatenate(mfcc_instances_0)\n",
    "mfcc_instances_20 = np.concatenate(mfcc_instances_20)\n",
    "mfcc_instances_40 = np.concatenate(mfcc_instances_40)\n",
    "mfcc_instances_60 = np.concatenate(mfcc_instances_60)\n",
    "mfcc_instances_80 = np.concatenate(mfcc_instances_80)\n",
    "mfcc_instances_100= np.concatenate(mfcc_instances_100)\n",
    "mfcc_instances_140 = np.concatenate(mfcc_instances_140)\n",
    "mfcc_instances_160 = np.concatenate(mfcc_instances_160)\n",
    "mfcc_instances_180 = np.concatenate(mfcc_instances_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_instances_0 = np.concatenate(label_instances_0)\n",
    "label_instances_20 = np.concatenate(label_instances_20)\n",
    "label_instances_40 = np.concatenate(label_instances_40)\n",
    "label_instances_60 = np.concatenate(label_instances_60)\n",
    "label_instances_80 = np.concatenate(label_instances_80)\n",
    "label_instances_100= np.concatenate(label_instances_100)\n",
    "label_instances_140 = np.concatenate(label_instances_140)\n",
    "label_instances_160 = np.concatenate(label_instances_160)\n",
    "label_instances_180 = np.concatenate(label_instances_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_instances = np.concatenate([mfcc_instances_0, mfcc_instances_20, mfcc_instances_40, mfcc_instances_60,mfcc_instances_80, \n",
    "                                  mfcc_instances_100, mfcc_instances_140, mfcc_instances_160, mfcc_instances_180], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_instances = np.concatenate([label_instances_0, label_instances_20, label_instances_40, label_instances_60,label_instances_80, \n",
    "                                  label_instances_100, label_instances_140, label_instances_160, label_instances_180], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mfcc_instances\n",
    "y = label_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2185"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_eval = len(label_instances_160) + len(label_instances_180)\n",
    "num_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:-num_eval]\n",
    "X_eval = X[-num_eval:]\n",
    "y_train = y[:-num_eval]\n",
    "y_eval = y[-num_eval:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8319, 40, 50, 1), (8319,), (2185, 40, 50, 1), (2185,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.transpose(0, 3, 1, 2)\n",
    "X_eval = X_eval.transpose(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).to(device)\n",
    "X_eval = torch.tensor(X_eval).to(device)\n",
    "y_train = torch.tensor(y_train).to(device)\n",
    "y_eval = torch.tensor(y_eval).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.float().unsqueeze(-1)\n",
    "y_eval = y_eval.float().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(X_train, y_train)\n",
    "eval_ds = TensorDataset(X_eval, y_eval)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0, drop_last=True)\n",
    "eval_dataloader = DataLoader(eval_ds, batch_size=128, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, conv_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        if(conv_dim == '1d'):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv1d(1, 4, (11, 1)), # (1, 40, 100) -> (4, 30, 100)\n",
    "                nn.BatchNorm2d(4),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv1d(4, 4, (11, 1)), # (4, 30, 100) -> (4, 20, 100)\n",
    "                nn.BatchNorm2d(4),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.conv3 = nn.Sequential(\n",
    "                nn.Conv1d(4, 8, (11, 1)), # (4, 20, 100) -> (8, 10, 100)\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.conv4 = nn.Sequential(\n",
    "                nn.Conv1d(8, 8, (10, 1)), # (8, 10, 100) -> (8, 1, 100)\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        elif(conv_dim == '2d'):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(1, 4, (5, 3), padding=(0, 1)), # (1, 128, 50) -> (4, 124, 50)\n",
    "                nn.BatchNorm2d(4),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(4, 4, (5, 3), padding=(0, 1)),  # (4, 124, 50) -> (4, 120, 50)\n",
    "                nn.BatchNorm2d(4),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2)  # (4, 120, 50) -> (4, 60, 25)\n",
    "            )\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(4, 8, (5, 3), padding=(0, 1)), # (4, 60, 25) -> (8, 56, 25)\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(8, 8, (5, 3), padding=(0, 1)),  # (8, 56, 25) -> (8, 52, 25)\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2)  # (8, 52, 25) -> (8, 26, 12)\n",
    "            )\n",
    "            self.conv3 = nn.Sequential(\n",
    "                nn.Conv2d(8, 16, (5, 3), padding=0), # (8, 26, 12) -> (16, 22, 10)\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 1), (2, 1)),  # (16, 22, 10) -> (16, 11, 10)\n",
    "                nn.Conv2d(16, 16, 3, padding=(1, 0)),  # (16, 11, 10) -> (16, 11, 8)\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Convolution dimension not found: %s\" % (conv_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(self.conv_dim == '1d'):\n",
    "            out = self.conv1(x)\n",
    "            out = self.conv2(out)\n",
    "            out = self.conv3(out)\n",
    "            out = self.conv4(out)\n",
    "            # out = out.contiguous().view(x.size()[0], -1)  # (800,)\n",
    "        elif(self.conv_dim == '2d'):\n",
    "            out = self.conv1(x)\n",
    "            out = self.conv2(out)\n",
    "            out = self.conv3(out)\n",
    "            # out = out.contiguous().view(x.size()[0], -1)  # (1408,)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_attn_heads, attn_hidden_size, dropout_prob, with_focus_attn):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        self.hidden_size = attn_hidden_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.with_focus_attn = with_focus_attn\n",
    "        \n",
    "        self.attn_head_size = int(self.hidden_size / self.num_attn_heads)\n",
    "        self.all_head_size = self.num_attn_heads * self.attn_head_size\n",
    "\n",
    "        self.query = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        if(with_focus_attn == True):\n",
    "            self.tanh = nn.Tanh()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "            self.linear_focus_query = nn.Linear(num_attn_heads * self.attn_head_size, \n",
    "                                                num_attn_heads * self.attn_head_size)\n",
    "            self.linear_focus_global = nn.Linear(num_attn_heads * self.attn_head_size, \n",
    "                                                 num_attn_heads * self.attn_head_size)\n",
    "            \n",
    "            up = torch.randn(num_attn_heads, 1, self.attn_head_size)\n",
    "            self.up = Variable(up, requires_grad=True).cuda()\n",
    "            torch.nn.init.xavier_uniform_(self.up)\n",
    "            \n",
    "            uz = torch.randn(num_attn_heads, 1, self.attn_head_size)\n",
    "            self.uz = Variable(uz, requires_grad=True).cuda()\n",
    "            torch.nn.init.xavier_uniform_(self.uz)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attn_heads, self.attn_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        key_len = hidden_states.size(1)\n",
    "        \n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        if(self.with_focus_attn == True):\n",
    "            glo = torch.mean(mixed_query_layer, dim=1, keepdim=True)\n",
    "            \n",
    "            c = self.tanh(self.linear_focus_query(mixed_query_layer) + self.linear_focus_global(glo))\n",
    "            c = self.transpose_for_scores(c)\n",
    "            \n",
    "            p = c * self.up\n",
    "            p = p.sum(3).squeeze()\n",
    "            z = c * self.uz\n",
    "            z = z.sum(3).squeeze()\n",
    "            \n",
    "            P = self.sigmoid(p) * key_len\n",
    "            Z = self.sigmoid(z) * key_len\n",
    "            \n",
    "            j = torch.arange(start=0, end=key_len, dtype=P.dtype).unsqueeze(0).unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "            P = P.unsqueeze(-1)\n",
    "            Z = Z.unsqueeze(-1)\n",
    "            \n",
    "            G = -(j - P)**2 * 2 / (Z**2)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attn_head_size)\n",
    "        \n",
    "        if(self.with_focus_attn == True):\n",
    "            attention_scores = attention_scores + G\n",
    "            \n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.o_proj(context_layer)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDNN(nn.Module):\n",
    "    def __init__(self, conv_dim, checkpoint=None, hidden_size=64, num_layers=2,\n",
    "                 bidirectional=True, with_focus_attn=False):\n",
    "        super(CLDNN, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        if(conv_dim == '1d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=8, dropout_prob=0.1,\n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.lstm = nn.LSTM(8, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif(conv_dim == '2d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=176, dropout_prob=0.1, \n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.gap = nn.AdaptiveAvgPool2d((1, 11))\n",
    "            self.lstm = nn.LSTM(11, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Convolution dimension not found: %s\" % (conv_dim))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if(self.conv_dim == '1d'):\n",
    "            out = self.encoder(x)\n",
    "            out = torch.squeeze(out, 2)\n",
    "            out = out.permute(0, 2, 1)  \n",
    "            h = out\n",
    "            out = self.attn(out)  \n",
    "            out = h + out\n",
    "            out = out.permute(1, 0, 2)  \n",
    "            self.lstm.flatten_parameters()\n",
    "            out, _ = self.lstm(out)  \n",
    "            out = out[-1]  \n",
    "            out = self.fc(out)  \n",
    "        elif(self.conv_dim == '2d'):\n",
    "            out = self.encoder(x)  \n",
    "            out = out.permute(0, 3, 1, 2)  \n",
    "            h = out\n",
    "            new_out_shape = out.size()[:2] + (out.size()[2] * out.size()[3],)\n",
    "            out = out.view(*new_out_shape)  \n",
    "            out = self.attn(out)  \n",
    "            out = out.view(h.size())  \n",
    "            out = h + out\n",
    "            out = self.gap(out)  \n",
    "            out = torch.squeeze(out, 2)  \n",
    "            out = out.permute(1, 0, 2)  \n",
    "            self.lstm.flatten_parameters()\n",
    "            out, _ = self.lstm(out)\n",
    "            out = out[-1]\n",
    "            out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLDNN(conv_dim='1d').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, eval_dataloader, epochs):\n",
    "        print('Start training')\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            nb_train_steps = 0\n",
    "            correct = 0\n",
    "            num_samples = 0\n",
    "\n",
    "            for X_batch, y_batch in train_dataloader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "                loss = loss_func(outputs, y_batch)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.mean().item()\n",
    "                nb_train_steps += 1\n",
    "\n",
    "                outputs = (outputs >= 0.5).float()\n",
    "                correct += (outputs == y_batch).float().sum()\n",
    "                num_samples += len(X_batch)\n",
    "\n",
    "            train_loss = train_loss / nb_train_steps\n",
    "            train_accuracy = correct / num_samples\n",
    "\n",
    "            model.eval()\n",
    "            eval_loss = 0\n",
    "            nb_eval_steps = 0\n",
    "            correct = 0\n",
    "            num_samples = 0\n",
    "\n",
    "            for X_batch, y_batch in eval_dataloader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(X_batch)\n",
    "\n",
    "                tmp_eval_loss = loss_func(outputs, y_batch)\n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "                nb_eval_steps += 1\n",
    "\n",
    "                outputs = (outputs >= 0.5).float()\n",
    "                correct += (outputs == y_batch).float().sum()\n",
    "                num_samples += len(X_batch)\n",
    "\n",
    "            eval_loss = eval_loss / nb_eval_steps\n",
    "            eval_accuracy = correct / num_samples\n",
    "\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "            print('epoch: {:3d},    lr={:6f},    loss={:5f},    train_acc={:5f},    eval_loss={:5f},    eval_acc={:5f}'\n",
    "                  .format(epoch+1, lr, train_loss, train_accuracy, eval_loss, eval_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "epoch:   1,    lr=0.000300,    loss=0.678607,    train_acc=0.545654,    eval_loss=0.632309,    eval_acc=0.581801\n",
      "epoch:   2,    lr=0.000300,    loss=0.555032,    train_acc=0.731445,    eval_loss=0.570106,    eval_acc=0.724265\n",
      "epoch:   3,    lr=0.000300,    loss=0.487330,    train_acc=0.786255,    eval_loss=0.574816,    eval_acc=0.739430\n",
      "epoch:   4,    lr=0.000300,    loss=0.406753,    train_acc=0.832886,    eval_loss=0.500673,    eval_acc=0.757812\n",
      "epoch:   5,    lr=0.000300,    loss=0.353022,    train_acc=0.854980,    eval_loss=0.481345,    eval_acc=0.763787\n",
      "epoch:   6,    lr=0.000300,    loss=0.313275,    train_acc=0.869629,    eval_loss=0.554657,    eval_acc=0.744945\n",
      "epoch:   7,    lr=0.000300,    loss=0.283488,    train_acc=0.877563,    eval_loss=0.442086,    eval_acc=0.791360\n",
      "epoch:   8,    lr=0.000300,    loss=0.268458,    train_acc=0.884766,    eval_loss=0.411509,    eval_acc=0.808824\n",
      "epoch:   9,    lr=0.000300,    loss=0.242499,    train_acc=0.896729,    eval_loss=0.414080,    eval_acc=0.815257\n",
      "epoch:  10,    lr=0.000300,    loss=0.222564,    train_acc=0.906738,    eval_loss=0.389872,    eval_acc=0.833180\n",
      "epoch:  11,    lr=0.000300,    loss=0.214047,    train_acc=0.912720,    eval_loss=0.452868,    eval_acc=0.811121\n",
      "epoch:  12,    lr=0.000300,    loss=0.195952,    train_acc=0.920166,    eval_loss=0.419299,    eval_acc=0.827665\n",
      "epoch:  13,    lr=0.000300,    loss=0.187130,    train_acc=0.924316,    eval_loss=0.458890,    eval_acc=0.821691\n",
      "epoch:  14,    lr=0.000300,    loss=0.187845,    train_acc=0.922119,    eval_loss=0.434063,    eval_acc=0.829963\n",
      "epoch:  15,    lr=0.000300,    loss=0.181045,    train_acc=0.927490,    eval_loss=0.515547,    eval_acc=0.811581\n",
      "epoch:  16,    lr=0.000300,    loss=0.178789,    train_acc=0.928955,    eval_loss=0.419157,    eval_acc=0.828585\n",
      "epoch:  17,    lr=0.000300,    loss=0.166680,    train_acc=0.935303,    eval_loss=0.511705,    eval_acc=0.806985\n",
      "epoch:  18,    lr=0.000300,    loss=0.161357,    train_acc=0.934204,    eval_loss=0.492433,    eval_acc=0.822610\n",
      "epoch:  19,    lr=0.000300,    loss=0.161663,    train_acc=0.932739,    eval_loss=0.408219,    eval_acc=0.852941\n",
      "epoch:  20,    lr=0.000300,    loss=0.161280,    train_acc=0.932983,    eval_loss=0.422816,    eval_acc=0.841452\n",
      "epoch:  21,    lr=0.000300,    loss=0.151471,    train_acc=0.938354,    eval_loss=0.429415,    eval_acc=0.833640\n",
      "epoch:  22,    lr=0.000300,    loss=0.151456,    train_acc=0.938599,    eval_loss=0.569977,    eval_acc=0.812040\n",
      "epoch:  23,    lr=0.000300,    loss=0.151068,    train_acc=0.939087,    eval_loss=0.554077,    eval_acc=0.820772\n",
      "epoch:  24,    lr=0.000300,    loss=0.144424,    train_acc=0.940918,    eval_loss=0.452392,    eval_acc=0.835478\n",
      "epoch:  25,    lr=0.000300,    loss=0.131454,    train_acc=0.947632,    eval_loss=0.481617,    eval_acc=0.829044\n",
      "epoch:  26,    lr=0.000300,    loss=0.129580,    train_acc=0.949585,    eval_loss=0.531998,    eval_acc=0.821691\n",
      "epoch:  27,    lr=0.000300,    loss=0.128877,    train_acc=0.947998,    eval_loss=0.628377,    eval_acc=0.816176\n",
      "epoch:  28,    lr=0.000300,    loss=0.128269,    train_acc=0.947510,    eval_loss=0.587883,    eval_acc=0.821691\n",
      "epoch:  29,    lr=0.000300,    loss=0.123538,    train_acc=0.952515,    eval_loss=0.461473,    eval_acc=0.841452\n",
      "epoch:  30,    lr=0.000300,    loss=0.124624,    train_acc=0.950806,    eval_loss=0.519948,    eval_acc=0.830882\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-6edd6d1fd26d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-f7d8a2d09e0a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_dataloader, eval_dataloader, epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mldl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-ebd8e3c9c509>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mldl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mldl\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mldl\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mldl\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[1;32m--> 522\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    523\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, eval_dataloader, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
