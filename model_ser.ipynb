{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from model_ae import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_attn_heads, attn_hidden_size, dropout_prob, with_focus_attn):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        self.hidden_size = attn_hidden_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.with_focus_attn = with_focus_attn\n",
    "        \n",
    "        self.attn_head_size = int(self.hidden_size / self.num_attn_heads)\n",
    "        self.all_head_size = self.num_attn_heads * self.attn_head_size\n",
    "\n",
    "        self.query = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        if(with_focus_attn == True):\n",
    "            self.tanh = nn.Tanh()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "            self.linear_focus_query = nn.Linear(num_attn_heads * self.attn_head_size, \n",
    "                                                num_attn_heads * self.attn_head_size)\n",
    "            self.linear_focus_global = nn.Linear(num_attn_heads * self.attn_head_size, \n",
    "                                                 num_attn_heads * self.attn_head_size)\n",
    "            \n",
    "            up = torch.randn(num_attn_heads, 1, self.attn_head_size)\n",
    "            self.up = Variable(up, requires_grad=True).cuda()\n",
    "            torch.nn.init.xavier_uniform_(self.up)\n",
    "            \n",
    "            uz = torch.randn(num_attn_heads, 1, self.attn_head_size)\n",
    "            self.uz = Variable(uz, requires_grad=True).cuda()\n",
    "            torch.nn.init.xavier_uniform_(self.uz)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attn_heads, self.attn_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        key_len = hidden_states.size(1)\n",
    "        \n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        if(self.with_focus_attn == True):\n",
    "            glo = torch.mean(mixed_query_layer, dim=1, keepdim=True)\n",
    "            \n",
    "            c = self.tanh(self.linear_focus_query(mixed_query_layer) + self.linear_focus_global(glo))\n",
    "            c = self.transpose_for_scores(c)\n",
    "            \n",
    "            p = c * self.up\n",
    "            p = p.sum(3).squeeze()\n",
    "            z = c * self.uz\n",
    "            z = z.sum(3).squeeze()\n",
    "            \n",
    "            P = self.sigmoid(p) * key_len\n",
    "            Z = self.sigmoid(z) * key_len\n",
    "            \n",
    "            j = torch.arange(start=0, end=key_len, dtype=P.dtype).unsqueeze(0).unsqueeze(0).unsqueeze(0).cuda()\n",
    "            P = P.unsqueeze(-1)\n",
    "            Z = Z.unsqueeze(-1)\n",
    "            \n",
    "            G = -(j - P)**2 * 2 / (Z**2)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attn_head_size)\n",
    "        \n",
    "        if(self.with_focus_attn == True):\n",
    "            attention_scores = attention_scores + G\n",
    "            \n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.o_proj(context_layer)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDNN(nn.Module):\n",
    "    def __init__(self, conv_dim, checkpoint=None, hidden_size=64, num_layers=2,\n",
    "                 bidirectional=True, with_focus_attn=False):\n",
    "        super(CLDNN, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        if(conv_dim == '1d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=8, dropout_prob=0.1,\n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.lstm = nn.LSTM(8, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional) \n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif(conv_dim == '2d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=176, dropout_prob=0.1, \n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.gap = nn.AdaptiveAvgPool2d((1, 11))\n",
    "            self.lstm = nn.LSTM(11, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional) \n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Convolution dimension not found: %s\" % (conv_dim))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if(self.conv_dim == '1d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 40, 100) -> (batch, 8, 1, 100)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 100) -> (batch, 8, 100)\n",
    "            out = out.permute(0, 2, 1)  # (batch, 8, 100) -> (batch, 100, 8)\n",
    "            h = out\n",
    "            out = self.attn(out) # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            out = h + out\n",
    "            out = out.permute(1, 0, 2)  # (batch, 100, 8) -> (100, batch, 8)\n",
    "            out, _ = self.lstm(out)  # (100, batch, 8) -> (100, batch, num_directions*hidden_size)\n",
    "            out = out[-1]  # (100, batch, num_directions*hidden_size) -> (batch, num_directions*hidden_size)\n",
    "            out = self.fc(out)  # (batch, num_directions*hidden_size) -> (batch, 1)\n",
    "        elif(self.conv_dim == '2d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 128, 100) -> (batch, 16, 11, 8)\n",
    "            out = out.permute(0, 3, 1, 2)  # (batch, 16, 11, 8) -> (batch, 8, 16, 11)\n",
    "            h = out\n",
    "            new_out_shape = out.size()[:2] + (out.size()[2] * out.size()[3],)\n",
    "            out = out.view(*new_out_shape)  # (batch, 8, 16, 11) -> (batch, 8, 176)\n",
    "            out = self.attn(out)  # (batch, 8, 176) -> (batch, 8, 176)\n",
    "            out = out.view(h.size())  # (batch, 8, 176) -> (batch, 8, 16, 11)\n",
    "            out = h + out\n",
    "            out = self.gap(out)  # (batch, 8, 16, 11) -> (batch, 8, 1, 11)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 11) -> (batch, 8, 11)\n",
    "            out = out.permute(1, 0, 2)  # (batch, 8, 11) -> (8, batch, 11)\n",
    "            out, _ = self.lstm(out)  # (8, batch, 11) -> (8, batch, num_directions*hidden_size)\n",
    "            out = out[-1]  # (8, batch, num_directions*hidden_size) -> (batch, num_directions*hidden_size)\n",
    "            out = self.fc(out)  # (batch, num_directions*hidden_size) -> (batch, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import convert_spectrograms, convert_tensor\n",
    "import os, glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_repo = os.path.join('.', 'wav_data', 'pretrain')\n",
    "\n",
    "samples_data = glob.glob(os.path.join(sample_data_repo, '**', '*wav'), recursive=True)\n",
    "samples_data = sorted(samples_data)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "idx = np.random.permutation(len(samples_data))\n",
    "train_idx = idx[:int(len(samples_data)*0.8)]\n",
    "eval_idx = idx[int(len(samples_data)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = list(np.array(samples_data)[train_idx])\n",
    "eval_samples = list(np.array(samples_data)[eval_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:50<00:00,  5.73it/s]\n"
     ]
    }
   ],
   "source": [
    "#X_train = convert_spectrograms(train_samples, conv_dim=args.conv_dim)\n",
    "X_eval = convert_spectrograms(eval_samples, conv_dim='2d')\n",
    "\n",
    "#X_train = convert_tensor(X_train, device=device)\n",
    "X_eval = convert_tensor(X_eval, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLDNN(conv_dim='2d', checkpoint='./output/aae_2d_step_100.pt', with_focus_attn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(X_eval[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5003]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros(len(X_eval))\n",
    "y[int(len(X_eval)/2):] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(y, device=device).float()\n",
    "y = y.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(X_eval, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(x_batch)\n",
    "            \n",
    "            loss = loss_func(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr = param_group['lr']\n",
    "        print('epoch: {:3d},    lr={:6f},    loss={:5f}'.format(epoch+1, lr, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1,    lr=0.001000,    loss=0.681675\n",
      "epoch:   2,    lr=0.001000,    loss=0.691006\n",
      "epoch:   3,    lr=0.001000,    loss=0.669674\n",
      "epoch:   4,    lr=0.001000,    loss=0.696231\n",
      "epoch:   5,    lr=0.001000,    loss=0.685858\n",
      "epoch:   6,    lr=0.001000,    loss=0.610714\n",
      "epoch:   7,    lr=0.001000,    loss=0.703751\n",
      "epoch:   8,    lr=0.001000,    loss=0.589321\n",
      "epoch:   9,    lr=0.001000,    loss=0.600516\n",
      "epoch:  10,    lr=0.001000,    loss=0.586961\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mldl]",
   "language": "python",
   "name": "conda-env-mldl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
