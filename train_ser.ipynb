{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from preprocessing import convert_spectrograms, convert_tensor\n",
    "from model_ser import CLDNN\n",
    "from utils.optimization import WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dim = '1d'\n",
    "checkpoint = ''\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "bidirectional=True\n",
    "with_focus_attn=False\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "use_warmup = True\n",
    "gradient_accumulation_steps = 1\n",
    "warmup_proportion = 0.1\n",
    "\n",
    "multi_gpu = True\n",
    "\n",
    "normal_data_repo = './wav_data/normal'\n",
    "violence_data_repo = './wav_data/violence'\n",
    "\n",
    "output_dir = 'output'\n",
    "save_checkpoint_steps = 50\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if(multi_gpu):\n",
    "    batch_size = batch_size * torch.cuda.device_count()\n",
    "    \n",
    "if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data_repo = './wav_data/normal/concat'\n",
    "violence_data_repo = './wav_data/violence/concat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = glob.glob(os.path.join(normal_data_repo, '**', '*wav'), recursive=True)\n",
    "normal_data = sorted(normal_data)\n",
    "\n",
    "violence_data = glob.glob(os.path.join(violence_data_repo, '**', '*wav'), recursive=True)\n",
    "violence_data = sorted(violence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "X_normal = convert_spectrograms(normal_data, conv_dim=conv_dim)\n",
    "X_violence = convert_spectrograms(violence_data, conv_dim=conv_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normal = convert_tensor(X_normal, device=device)\n",
    "X_violence = convert_tensor(X_violence, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_normal = np.zeros(len(X_normal))\n",
    "y_violence = np.ones(len(X_violence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_normal = torch.tensor(y_normal, device=device).float().view(-1, 1)\n",
    "y_violence = torch.tensor(y_violence, device=device).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat((X_normal, X_violence), dim=0)\n",
    "y = torch.cat((y_normal, y_violence), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multi_gpu == True):\n",
    "    model = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                 bidirectional=bidirectional, with_focus_attn=with_focus_attn)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "else:\n",
    "    model = CLDNN(conv_dim=conv_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if(use_warmup == True):\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_epochs\n",
    "    opt_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=t_total * warmup_proportion, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        nb_train_steps = 0\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for X_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            loss = loss_func(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            opt_scheduler.step()\n",
    "            \n",
    "            train_loss += loss.mean().item()\n",
    "            nb_train_steps += 1\n",
    "            \n",
    "            outputs = (outputs >= 0.5).float()\n",
    "            correct += (outputs == y_batch).float().sum()\n",
    "            num_samples += len(X_batch)\n",
    "            \n",
    "        train_loss = train_loss / nb_train_steps\n",
    "        accuracy = correct / num_samples\n",
    "            \n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr = param_group['lr']\n",
    "        print('epoch: {:3d},    lr={:6f},    loss={:5f},    accuracy={:5f}'\n",
    "              .format(epoch+1, lr, train_loss, accuracy))\n",
    "        \n",
    "        if((epoch+1) % save_checkpoint_steps == 0):\n",
    "            model_checkpoint = \"%s_%s_step_%d.pt\" % ('CLDNN', conv_dim, epoch+1)\n",
    "            output_model_file = os.path.join(output_dir, model_checkpoint)\n",
    "            if(multi_gpu == True):\n",
    "                torch.save(model.module.state_dict(), output_model_file)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), output_model_file)\n",
    "            print(\"Saving checkpoint %s\" % output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aimaster/anaconda3/envs/mldl/lib/python3.7/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1,    lr=0.000100,    loss=0.635439,    accuracy=0.942383\n",
      "epoch:   2,    lr=0.000200,    loss=0.634247,    accuracy=0.941406\n",
      "epoch:   3,    lr=0.000300,    loss=0.630397,    accuracy=0.941406\n",
      "epoch:   4,    lr=0.000400,    loss=0.624086,    accuracy=0.940918\n",
      "epoch:   5,    lr=0.000500,    loss=0.614684,    accuracy=0.942383\n",
      "epoch:   6,    lr=0.000600,    loss=0.602402,    accuracy=0.942871\n",
      "epoch:   7,    lr=0.000700,    loss=0.586427,    accuracy=0.943359\n",
      "epoch:   8,    lr=0.000800,    loss=0.565916,    accuracy=0.941406\n",
      "epoch:   9,    lr=0.000900,    loss=0.536928,    accuracy=0.942383\n",
      "epoch:  10,    lr=0.001000,    loss=0.494502,    accuracy=0.943359\n",
      "epoch:  11,    lr=0.000989,    loss=0.433899,    accuracy=0.940918\n",
      "epoch:  12,    lr=0.000978,    loss=0.352026,    accuracy=0.942871\n",
      "epoch:  13,    lr=0.000967,    loss=0.275212,    accuracy=0.943848\n",
      "epoch:  14,    lr=0.000956,    loss=0.238669,    accuracy=0.939941\n",
      "epoch:  15,    lr=0.000944,    loss=0.212908,    accuracy=0.945312\n",
      "epoch:  16,    lr=0.000933,    loss=0.228172,    accuracy=0.940918\n",
      "epoch:  17,    lr=0.000922,    loss=0.228517,    accuracy=0.942383\n",
      "epoch:  18,    lr=0.000911,    loss=0.229408,    accuracy=0.942871\n",
      "epoch:  19,    lr=0.000900,    loss=0.229369,    accuracy=0.942871\n",
      "epoch:  20,    lr=0.000889,    loss=0.227449,    accuracy=0.942871\n",
      "epoch:  21,    lr=0.000878,    loss=0.223149,    accuracy=0.943359\n",
      "epoch:  22,    lr=0.000867,    loss=0.220750,    accuracy=0.943359\n",
      "epoch:  23,    lr=0.000856,    loss=0.220326,    accuracy=0.942871\n",
      "epoch:  24,    lr=0.000844,    loss=0.217354,    accuracy=0.943359\n",
      "epoch:  25,    lr=0.000833,    loss=0.222311,    accuracy=0.941406\n",
      "epoch:  26,    lr=0.000822,    loss=0.223476,    accuracy=0.940918\n",
      "epoch:  27,    lr=0.000811,    loss=0.224954,    accuracy=0.940430\n",
      "epoch:  28,    lr=0.000800,    loss=0.222586,    accuracy=0.941406\n",
      "epoch:  29,    lr=0.000789,    loss=0.209925,    accuracy=0.946289\n",
      "epoch:  30,    lr=0.000778,    loss=0.213580,    accuracy=0.944824\n",
      "epoch:  31,    lr=0.000767,    loss=0.219794,    accuracy=0.942383\n",
      "epoch:  32,    lr=0.000756,    loss=0.219476,    accuracy=0.942383\n",
      "epoch:  33,    lr=0.000744,    loss=0.216648,    accuracy=0.943359\n",
      "epoch:  34,    lr=0.000733,    loss=0.217491,    accuracy=0.942871\n",
      "epoch:  35,    lr=0.000722,    loss=0.219954,    accuracy=0.941895\n",
      "epoch:  36,    lr=0.000711,    loss=0.222492,    accuracy=0.940918\n",
      "epoch:  37,    lr=0.000700,    loss=0.217089,    accuracy=0.942871\n",
      "epoch:  38,    lr=0.000689,    loss=0.212564,    accuracy=0.944336\n",
      "epoch:  39,    lr=0.000678,    loss=0.215054,    accuracy=0.943359\n",
      "epoch:  40,    lr=0.000667,    loss=0.212314,    accuracy=0.944336\n",
      "epoch:  41,    lr=0.000656,    loss=0.218892,    accuracy=0.941895\n",
      "epoch:  42,    lr=0.000644,    loss=0.222528,    accuracy=0.940430\n",
      "epoch:  43,    lr=0.000633,    loss=0.223885,    accuracy=0.939941\n",
      "epoch:  44,    lr=0.000622,    loss=0.216640,    accuracy=0.942383\n",
      "epoch:  45,    lr=0.000611,    loss=0.210519,    accuracy=0.944336\n",
      "epoch:  46,    lr=0.000600,    loss=0.211445,    accuracy=0.943848\n",
      "epoch:  47,    lr=0.000589,    loss=0.216610,    accuracy=0.941895\n",
      "epoch:  48,    lr=0.000578,    loss=0.209236,    accuracy=0.944336\n",
      "epoch:  49,    lr=0.000567,    loss=0.215086,    accuracy=0.941895\n",
      "epoch:  50,    lr=0.000556,    loss=0.207319,    accuracy=0.944336\n",
      "Saving checkpoint output/CLDNN_1d_step_50.pt\n",
      "epoch:  51,    lr=0.000544,    loss=0.211799,    accuracy=0.942383\n",
      "epoch:  52,    lr=0.000533,    loss=0.217033,    accuracy=0.939941\n",
      "epoch:  53,    lr=0.000522,    loss=0.210577,    accuracy=0.941895\n",
      "epoch:  54,    lr=0.000511,    loss=0.205116,    accuracy=0.943359\n",
      "epoch:  55,    lr=0.000500,    loss=0.197326,    accuracy=0.945801\n",
      "epoch:  56,    lr=0.000489,    loss=0.198533,    accuracy=0.944336\n",
      "epoch:  57,    lr=0.000478,    loss=0.198450,    accuracy=0.942871\n",
      "epoch:  58,    lr=0.000467,    loss=0.192051,    accuracy=0.943848\n",
      "epoch:  59,    lr=0.000456,    loss=0.194528,    accuracy=0.940918\n",
      "epoch:  60,    lr=0.000444,    loss=0.189920,    accuracy=0.939941\n",
      "epoch:  61,    lr=0.000433,    loss=0.172873,    accuracy=0.945312\n",
      "epoch:  62,    lr=0.000422,    loss=0.171057,    accuracy=0.944824\n",
      "epoch:  63,    lr=0.000411,    loss=0.168839,    accuracy=0.943848\n",
      "epoch:  64,    lr=0.000400,    loss=0.150250,    accuracy=0.950195\n",
      "epoch:  65,    lr=0.000389,    loss=0.142329,    accuracy=0.950684\n",
      "epoch:  66,    lr=0.000378,    loss=0.131288,    accuracy=0.955078\n",
      "epoch:  67,    lr=0.000367,    loss=0.127390,    accuracy=0.954590\n",
      "epoch:  68,    lr=0.000356,    loss=0.120082,    accuracy=0.956543\n",
      "epoch:  69,    lr=0.000344,    loss=0.110323,    accuracy=0.960449\n",
      "epoch:  70,    lr=0.000333,    loss=0.108636,    accuracy=0.963379\n",
      "epoch:  71,    lr=0.000322,    loss=0.105828,    accuracy=0.964355\n",
      "epoch:  72,    lr=0.000311,    loss=0.101631,    accuracy=0.964844\n",
      "epoch:  73,    lr=0.000300,    loss=0.097397,    accuracy=0.968262\n",
      "epoch:  74,    lr=0.000289,    loss=0.090161,    accuracy=0.966309\n",
      "epoch:  75,    lr=0.000278,    loss=0.093293,    accuracy=0.967773\n",
      "epoch:  76,    lr=0.000267,    loss=0.091601,    accuracy=0.967773\n",
      "epoch:  77,    lr=0.000256,    loss=0.086022,    accuracy=0.969727\n",
      "epoch:  78,    lr=0.000244,    loss=0.090935,    accuracy=0.966797\n",
      "epoch:  79,    lr=0.000233,    loss=0.087317,    accuracy=0.968750\n",
      "epoch:  80,    lr=0.000222,    loss=0.082221,    accuracy=0.971191\n",
      "epoch:  81,    lr=0.000211,    loss=0.078745,    accuracy=0.972168\n",
      "epoch:  82,    lr=0.000200,    loss=0.077011,    accuracy=0.971191\n",
      "epoch:  83,    lr=0.000189,    loss=0.078327,    accuracy=0.972656\n",
      "epoch:  84,    lr=0.000178,    loss=0.076795,    accuracy=0.973633\n",
      "epoch:  85,    lr=0.000167,    loss=0.073613,    accuracy=0.974121\n",
      "epoch:  86,    lr=0.000156,    loss=0.074835,    accuracy=0.975586\n",
      "epoch:  87,    lr=0.000144,    loss=0.070117,    accuracy=0.975098\n",
      "epoch:  88,    lr=0.000133,    loss=0.070513,    accuracy=0.976074\n",
      "epoch:  89,    lr=0.000122,    loss=0.067430,    accuracy=0.977051\n",
      "epoch:  90,    lr=0.000111,    loss=0.067937,    accuracy=0.976074\n",
      "epoch:  91,    lr=0.000100,    loss=0.064910,    accuracy=0.980469\n",
      "epoch:  92,    lr=0.000089,    loss=0.066756,    accuracy=0.980469\n",
      "epoch:  93,    lr=0.000078,    loss=0.067421,    accuracy=0.977051\n",
      "epoch:  94,    lr=0.000067,    loss=0.063330,    accuracy=0.980469\n",
      "epoch:  95,    lr=0.000056,    loss=0.065108,    accuracy=0.979980\n",
      "epoch:  96,    lr=0.000044,    loss=0.060845,    accuracy=0.981445\n",
      "epoch:  97,    lr=0.000033,    loss=0.064732,    accuracy=0.980957\n",
      "epoch:  98,    lr=0.000022,    loss=0.063313,    accuracy=0.978516\n",
      "epoch:  99,    lr=0.000011,    loss=0.064644,    accuracy=0.979980\n",
      "epoch: 100,    lr=0.000000,    loss=0.063318,    accuracy=0.979004\n",
      "Saving checkpoint output/CLDNN_1d_step_100.pt\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mldl]",
   "language": "python",
   "name": "conda-env-mldl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
