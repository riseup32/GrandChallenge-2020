{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from preprocessing import preprocessing, convert_spectrograms, convert_tensor\n",
    "from model_ae import Encoder\n",
    "from utils.optimization import WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dim = '1d'\n",
    "checkpoint = './output/aae_1d_step_300.pt'\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "bidirectional = 'true'\n",
    "with_focus_attn = 'true'\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 300\n",
    "learning_rate = 0.0001\n",
    "\n",
    "use_warmup = 'true'\n",
    "data_dir = './wav_data/pretrain/RAVDESS_resample/'\n",
    "multi_task = 'true'\n",
    "augmentation = 'true'\n",
    "\n",
    "bidirectional = True if(bidirectional == 'true') else False\n",
    "with_focus_attn = True if(with_focus_attn == 'true') else False\n",
    "n_mfcc = 40 if(conv_dim == '1d') else 128\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sample_datas = glob.glob(os.path.join(data_dir, '**', '*wav'), recursive=True)\n",
    "sample_datas = sorted(sample_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_attn_heads, attn_hidden_size, dropout_prob, with_focus_attn):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        self.hidden_size = attn_hidden_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.with_focus_attn = with_focus_attn\n",
    "        \n",
    "        self.attn_head_size = int(self.hidden_size / self.num_attn_heads)\n",
    "        self.all_head_size = self.num_attn_heads * self.attn_head_size\n",
    "\n",
    "        self.query = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        if(with_focus_attn == True):\n",
    "            self.tanh = nn.Tanh()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "            self.linear_focus_query = nn.Linear(num_attn_heads * self.attn_head_size, \n",
    "                                                num_attn_heads * self.attn_head_size)\n",
    "            self.linear_focus_global = nn.Linear(num_attn_heads * self.attn_head_size, \n",
    "                                                 num_attn_heads * self.attn_head_size)\n",
    "            \n",
    "            up = torch.randn(num_attn_heads, 1, self.attn_head_size)\n",
    "            self.up = Variable(up, requires_grad=True).cuda()\n",
    "            torch.nn.init.xavier_uniform_(self.up)\n",
    "            \n",
    "            uz = torch.randn(num_attn_heads, 1, self.attn_head_size)\n",
    "            self.uz = Variable(uz, requires_grad=True).cuda()\n",
    "            torch.nn.init.xavier_uniform_(self.uz)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attn_heads, self.attn_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        key_len = hidden_states.size(1)\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        if(self.with_focus_attn == True):\n",
    "            glo = torch.mean(mixed_query_layer, dim=1, keepdim=True)\n",
    "            \n",
    "            c = self.tanh(self.linear_focus_query(mixed_query_layer) + self.linear_focus_global(glo))\n",
    "            c = self.transpose_for_scores(c)\n",
    "            \n",
    "            p = c * self.up\n",
    "            p = p.sum(3).squeeze()\n",
    "            z = c * self.uz\n",
    "            z = z.sum(3).squeeze()\n",
    "            \n",
    "            P = self.sigmoid(p) * key_len\n",
    "            Z = self.sigmoid(z) * key_len\n",
    "            \n",
    "            j = torch.arange(start=0, end=key_len, dtype=P.dtype).unsqueeze(0).unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "            P = P.unsqueeze(-1)\n",
    "            Z = Z.unsqueeze(-1)\n",
    "            \n",
    "            G = -(j - P)**2 * 2 / (Z**2)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attn_head_size)\n",
    "        \n",
    "        if(self.with_focus_attn == True):\n",
    "            attention_scores = attention_scores + G\n",
    "            \n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.o_proj(context_layer)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 8\n",
    "ff_dim = 32\n",
    "dropout_prob = 0.1\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate = self.fc1(x)\n",
    "        ff_out = self.dropout(self.fc2(self.relu(intermediate)))\n",
    "        return ff_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 8\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block, self).__init__()\n",
    "        self.attention_norm = nn.LayerNorm(8, eps=1e-12)\n",
    "        self.ffn_norm = nn.LayerNorm(8, eps=1e-12)\n",
    "        self.ffn = PositionWiseFeedForward()\n",
    "        self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=8, dropout_prob=0.1, \n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention\n",
    "        h = x\n",
    "        x = self.attn(x)\n",
    "        x = h + x\n",
    "        x = self.attention_norm(x)\n",
    "\n",
    "        # FFN\n",
    "        h = x\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        x = self.ffn_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTransformer(nn.Module):\n",
    "    def __init__(self, conv_dim, checkpoint=None, hidden_size=64, num_layers=2,\n",
    "                 bidirectional=True, with_focus_attn=False):\n",
    "        super(CTransformer, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        if(conv_dim == '1d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn1 = Block()\n",
    "            self.attn2 = Block()\n",
    "            self.attn3 = Block()\n",
    "            self.attn4 = Block()\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(400, 8),\n",
    "            )\n",
    "        elif(conv_dim == '2d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=176, dropout_prob=0.1, \n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.gap = nn.AdaptiveAvgPool2d((1, 11))\n",
    "            self.lstm = nn.LSTM(11, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 8),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Convolution dimension not found: %s\" % (conv_dim))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if(self.conv_dim == '1d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 40, 100) -> (batch, 8, 1, 100)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 100) -> (batch, 8, 100)\n",
    "            out = out.permute(0, 2, 1)  # (batch, 8, 100) -> (batch, 100, 8)\n",
    "            out = self.attn1(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            out = self.attn2(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            out = self.attn3(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            out = self.attn4(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            new_out_shape = out.size()[:1] + (out.size()[1] * out.size()[2],)\n",
    "            out = out.view(*new_out_shape)\n",
    "            out = self.fc(out)  # (batch, 800) -> (batch, 8)\n",
    "        elif(self.conv_dim == '2d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 128, 100) -> (batch, 16, 11, 8)\n",
    "            out = out.permute(0, 3, 1, 2)  # (batch, 16, 11, 8) -> (batch, 8, 16, 11)\n",
    "            h = out\n",
    "            new_out_shape = out.size()[:2] + (out.size()[2] * out.size()[3],)\n",
    "            out = out.view(*new_out_shape)  # (batch, 8, 16, 11) -> (batch, 8, 176)\n",
    "            out = self.attn(out)  # (batch, 8, 176) -> (batch, 8, 176)\n",
    "            out = out.view(h.size())  # (batch, 8, 176) -> (batch, 8, 16, 11)\n",
    "            out = h + out\n",
    "            out = self.gap(out)  # (batch, 8, 16, 11) -> (batch, 8, 1, 11)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 11) -> (batch, 8, 11)\n",
    "            out = out.permute(1, 0, 2)  # (batch, 8, 11) -> (8, batch, 11)\n",
    "            self.lstm.flatten_parameters()\n",
    "            out, _ = self.lstm(out)  # (8, batch, 11) -> (8, batch, num_directions*hidden_size)\n",
    "            out = out[-1]  # (8, batch, num_directions*hidden_size) -> (batch, num_directions*hidden_size)\n",
    "            out = self.fc(out)  # (batch, num_directions*hidden_size) -> (batch, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTransformer_G(nn.Module):\n",
    "    def __init__(self, conv_dim, checkpoint=None, hidden_size=64, num_layers=2,\n",
    "                 bidirectional=True, with_focus_attn=False):\n",
    "        super(CTransformer_G, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        if(conv_dim == '1d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn1 = Block()\n",
    "            self.attn2 = Block()\n",
    "            self.attn3 = Block()\n",
    "            self.attn4 = Block()\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(400, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif(conv_dim == '2d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=8, dropout_prob=0.1, \n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.gap = nn.AdaptiveAvgPool2d((1, 11))\n",
    "            self.lstm = nn.LSTM(11, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 8),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Convolution dimension not found: %s\" % (conv_dim))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if(self.conv_dim == '1d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 40, 100) -> (batch, 8, 1, 100)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 100) -> (batch, 8, 100)\n",
    "            out = out.permute(0, 2, 1)  # (batch, 8, 100) -> (batch, 100, 8)\n",
    "            out = self.attn1(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            out = self.attn2(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            out = self.attn3(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            out = self.attn4(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            new_out_shape = out.size()[:1] + (out.size()[1] * out.size()[2],)\n",
    "            out = out.view(*new_out_shape)\n",
    "            out = self.fc(out)  # (batch, 800) -> (batch, 8)\n",
    "        elif(self.conv_dim == '2d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 128, 100) -> (batch, 16, 11, 8)\n",
    "            out = out.permute(0, 3, 1, 2)  # (batch, 16, 11, 8) -> (batch, 8, 16, 11)\n",
    "            h = out\n",
    "            new_out_shape = out.size()[:2] + (out.size()[2] * out.size()[3],)\n",
    "            out = out.view(*new_out_shape)  # (batch, 8, 16, 11) -> (batch, 8, 176)\n",
    "            out = self.attn(out)  # (batch, 8, 176) -> (batch, 8, 176)\n",
    "            out = out.view(h.size())  # (batch, 8, 176) -> (batch, 8, 16, 11)\n",
    "            out = h + out\n",
    "            out = self.gap(out)  # (batch, 8, 16, 11) -> (batch, 8, 1, 11)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 11) -> (batch, 8, 11)\n",
    "            out = out.permute(1, 0, 2)  # (batch, 8, 11) -> (8, batch, 11)\n",
    "            self.lstm.flatten_parameters()\n",
    "            out, _ = self.lstm(out)  # (8, batch, 11) -> (8, batch, num_directions*hidden_size)\n",
    "            out = out[-1]  # (8, batch, num_directions*hidden_size) -> (batch, num_directions*hidden_size)\n",
    "            out = self.fc(out)  # (batch, num_directions*hidden_size) -> (batch, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDNN(nn.Module):\n",
    "    def __init__(self, conv_dim, checkpoint=None, hidden_size=64, num_layers=2,\n",
    "                 bidirectional=True, with_focus_attn=False):\n",
    "        super(CLDNN, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        if(conv_dim == '1d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=8, dropout_prob=0.1,\n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.lstm = nn.LSTM(8, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 8),\n",
    "            )\n",
    "        elif(conv_dim == '2d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=176, dropout_prob=0.1, \n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.gap = nn.AdaptiveAvgPool2d((1, 11))\n",
    "            self.lstm = nn.LSTM(11, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 8),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Convolution dimension not found: %s\" % (conv_dim))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if(self.conv_dim == '1d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 40, 100) -> (batch, 8, 1, 100)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 100) -> (batch, 8, 100)\n",
    "            out = out.permute(0, 2, 1)  # (batch, 8, 100) -> (batch, 100, 8)\n",
    "            h = out\n",
    "            out = self.attn(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            out = h + out\n",
    "            out = out.permute(1, 0, 2)  # (batch, 100, 8) -> (100, batch, 8)\n",
    "            self.lstm.flatten_parameters()\n",
    "            out, _ = self.lstm(out)  # (100, batch, 8) -> (100, batch, num_directions*hidden_size)\n",
    "            out = out[-1]  # (100, batch, num_directions*hidden_size) -> (batch, num_directions*hidden_size)\n",
    "            out = self.fc(out)  # (batch, num_directions*hidden_size) -> (batch, 1)\n",
    "        elif(self.conv_dim == '2d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 128, 100) -> (batch, 16, 11, 8)\n",
    "            out = out.permute(0, 3, 1, 2)  # (batch, 16, 11, 8) -> (batch, 8, 16, 11)\n",
    "            h = out\n",
    "            new_out_shape = out.size()[:2] + (out.size()[2] * out.size()[3],)\n",
    "            out = out.view(*new_out_shape)  # (batch, 8, 16, 11) -> (batch, 8, 176)\n",
    "            out = self.attn(out)  # (batch, 8, 176) -> (batch, 8, 176)\n",
    "            out = out.view(h.size())  # (batch, 8, 176) -> (batch, 8, 16, 11)\n",
    "            out = h + out\n",
    "            out = self.gap(out)  # (batch, 8, 16, 11) -> (batch, 8, 1, 11)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 11) -> (batch, 8, 11)\n",
    "            out = out.permute(1, 0, 2)  # (batch, 8, 11) -> (8, batch, 11)\n",
    "            self.lstm.flatten_parameters()\n",
    "            out, _ = self.lstm(out)  # (8, batch, 11) -> (8, batch, num_directions*hidden_size)\n",
    "            out = out[-1]  # (8, batch, num_directions*hidden_size) -> (batch, num_directions*hidden_size)\n",
    "            out = self.fc(out)  # (batch, num_directions*hidden_size) -> (batch, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDNN_G(nn.Module):\n",
    "    def __init__(self, conv_dim, checkpoint=None, hidden_size=64, num_layers=2,\n",
    "                 bidirectional=True, with_focus_attn=False):\n",
    "        super(CLDNN_G, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        if(conv_dim == '1d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=8, dropout_prob=0.1,\n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.lstm = nn.LSTM(8, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif(conv_dim == '2d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=176, dropout_prob=0.1, \n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.gap = nn.AdaptiveAvgPool2d((1, 11))\n",
    "            self.lstm = nn.LSTM(11, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Convolution dimension not found: %s\" % (conv_dim))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if(self.conv_dim == '1d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 40, 100) -> (batch, 8, 1, 100)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 100) -> (batch, 8, 100)\n",
    "            out = out.permute(0, 2, 1)  # (batch, 8, 100) -> (batch, 100, 8)\n",
    "            h = out\n",
    "            out = self.attn(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            out = h + out\n",
    "            out = out.permute(1, 0, 2)  # (batch, 100, 8) -> (100, batch, 8)\n",
    "            self.lstm.flatten_parameters()\n",
    "            out, _ = self.lstm(out)  # (100, batch, 8) -> (100, batch, num_directions*hidden_size)\n",
    "            out = out[-1]  # (100, batch, num_directions*hidden_size) -> (batch, num_directions*hidden_size)\n",
    "            out = self.fc(out)  # (batch, num_directions*hidden_size) -> (batch, 1)\n",
    "        elif(self.conv_dim == '2d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 128, 100) -> (batch, 16, 11, 8)\n",
    "            out = out.permute(0, 3, 1, 2)  # (batch, 16, 11, 8) -> (batch, 8, 16, 11)\n",
    "            h = out\n",
    "            new_out_shape = out.size()[:2] + (out.size()[2] * out.size()[3],)\n",
    "            out = out.view(*new_out_shape)  # (batch, 8, 16, 11) -> (batch, 8, 176)\n",
    "            out = self.attn(out)  # (batch, 8, 176) -> (batch, 8, 176)\n",
    "            out = out.view(h.size())  # (batch, 8, 176) -> (batch, 8, 16, 11)\n",
    "            out = h + out\n",
    "            out = self.gap(out)  # (batch, 8, 16, 11) -> (batch, 8, 1, 11)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 11) -> (batch, 8, 11)\n",
    "            out = out.permute(1, 0, 2)  # (batch, 8, 11) -> (8, batch, 11)\n",
    "            self.lstm.flatten_parameters()\n",
    "            out, _ = self.lstm(out)  # (8, batch, 11) -> (8, batch, num_directions*hidden_size)\n",
    "            out = out[-1]  # (8, batch, num_directions*hidden_size) -> (batch, num_directions*hidden_size)\n",
    "            out = self.fc(out)  # (batch, num_directions*hidden_size) -> (batch, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idx = np.random.permutation(int(len(sample_datas)/2))  # add\n",
    "\n",
    "train_idx = idx[:int((len(sample_datas)/2)*0.75)]  # add\n",
    "eval_idx = idx[int((len(sample_datas)/2)*0.75):]  # add\n",
    "noise_idx = np.arange((int(len(sample_datas)/2)), len(sample_datas))  # add\n",
    "train_idx = np.r_[train_idx, noise_idx]  # add\n",
    "\n",
    "train_samples = list(np.array(sample_datas)[train_idx])\n",
    "eval_samples = list(np.array(sample_datas)[eval_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(map(lambda x: int(x.split('/')[-1].split('-')[2]) - 1, sample_datas)))\n",
    "y_train = y[train_idx]\n",
    "y_eval = y[eval_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multi_task == 'true'):\n",
    "    speaker = np.array(list(map(lambda x: int(x.split('/')[-1].split('-')[-1].split('.')[0].split('_')[0]), \n",
    "                                sample_datas)))  # add\n",
    "    y_gender = np.array(list(map(lambda x: 1 if x % 2 ==0 else 0, speaker)))\n",
    "\n",
    "    y_g_train = y_gender[train_idx]\n",
    "    y_g_eval = y_gender[eval_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2520it [00:13, 182.85it/s]\n",
      "360it [00:01, 189.69it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = convert_spectrograms(train_samples, conv_dim=conv_dim, sr=16000, labels=y_train)\n",
    "X_eval, y_eval = convert_spectrograms(eval_samples, conv_dim=conv_dim, sr=16000, labels=y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(augmentation == 'true'):\n",
    "    X_train_flip = X_train[:, :, :, ::-1]\n",
    "    y_train_flip = y_train.copy()\n",
    "\n",
    "    X_train = np.concatenate((X_train, X_train_flip), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_train_flip), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = convert_tensor(X_train, y_train)\n",
    "X_eval, y_eval = convert_tensor(X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.long()\n",
    "y_eval = y_eval.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2520it [00:13, 187.38it/s]\n",
      "360it [00:01, 197.91it/s]\n"
     ]
    }
   ],
   "source": [
    "if(multi_task == 'true'):\n",
    "    _, y_g_train = convert_spectrograms(train_samples, conv_dim=conv_dim, sr=16000, labels=y_g_train)\n",
    "    _, y_g_eval = convert_spectrograms(eval_samples, conv_dim=conv_dim, sr=16000, labels=y_g_eval)\n",
    "    \n",
    "    if(augmentation == 'true'):\n",
    "        y_g_train_flip = y_g_train.copy()\n",
    "        y_g_train = np.concatenate((y_g_train, y_g_train_flip))\n",
    "    \n",
    "    y_g_train = torch.tensor(y_g_train).float()\n",
    "    y_g_eval = torch.tensor(y_g_eval).float()\n",
    "\n",
    "    y_g_train = y_g_train.unsqueeze(-1)\n",
    "    y_g_eval = y_g_eval.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([164482, 1, 40, 50]) torch.Size([164482]) torch.Size([164482, 1]) torch.Size([11753, 1, 40, 50]) torch.Size([11753]) torch.Size([11753, 1])\n"
     ]
    }
   ],
   "source": [
    "if(multi_task == 'true'):\n",
    "    print(X_train.shape, y_train.shape, y_g_train.shape, X_eval.shape, y_eval.shape, y_g_eval.shape)\n",
    "else:\n",
    "    print(X_train.shape, y_train.shape, X_eval.shape, y_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multi_task == 'true'):\n",
    "    train_ds = TensorDataset(X_train, y_train, y_g_train)\n",
    "    eval_ds = TensorDataset(X_eval, y_eval, y_g_eval)\n",
    "else:\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    eval_ds = TensorDataset(X_eval, y_eval)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "eval_dataloader = DataLoader(eval_ds, batch_size=batch_size, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multi_task == 'true'):\n",
    "    model_g = CLDNN_G(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "                        num_layers=num_layers, bidirectional=bidirectional,\n",
    "                        with_focus_attn=with_focus_attn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multi_task == 'true'):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loss_func_g = nn.BCELoss()\n",
    "    optimizer = optim.Adam(list(model.parameters()) + list(model_g.parameters()), lr=learning_rate)\n",
    "else:\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_warmup == 'true'):\n",
    "    t_total = len(train_dataloader) // 1 * num_epochs\n",
    "    opt_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=t_total * 0.1, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, eval_dataloader, epochs):\n",
    "        print('Start training')\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            nb_train_steps = 0\n",
    "            correct = 0\n",
    "            num_samples = 0\n",
    "            \n",
    "            if(multi_task == 'true'):\n",
    "                for X_batch, y_batch, y_g_batch in train_dataloader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    y_g_batch = y_g_batch.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    outputs = model(X_batch)\n",
    "                    outputs_g = model_g(X_batch)\n",
    "\n",
    "                    loss_1 = loss_func(outputs, y_batch)\n",
    "                    loss_2 = loss_func_g(outputs_g, y_g_batch)\n",
    "                    loss = loss_1 + 0.8 * loss_2\n",
    "                    loss.backward(retain_graph=True)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    opt_scheduler.step()\n",
    "\n",
    "                    train_loss += loss.mean().item()\n",
    "                    nb_train_steps += 1\n",
    "\n",
    "                    outputs = softmax(outputs)\n",
    "                    outputs = torch.argmax(outputs, dim=1)\n",
    "                    correct += (outputs == y_batch).float().sum()\n",
    "                    num_samples += len(X_batch)\n",
    "\n",
    "                train_loss = train_loss / nb_train_steps\n",
    "                train_accuracy = correct / num_samples\n",
    "\n",
    "                model.eval()\n",
    "                eval_loss = 0\n",
    "                nb_eval_steps = 0\n",
    "                correct = 0\n",
    "                num_samples = 0\n",
    "\n",
    "                for X_batch, y_batch, y_g_batch in eval_dataloader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    y_g_batch = y_g_batch.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(X_batch)\n",
    "                        outputs_g = model_g(X_batch)\n",
    "\n",
    "                    tmp_eval_loss_1 = loss_func(outputs, y_batch)\n",
    "                    tmp_eval_loss_2 = loss_func_g(outputs_g, y_g_batch)\n",
    "                    tmp_eval_loss = tmp_eval_loss_1 + 0.8 * tmp_eval_loss_2\n",
    "                    eval_loss += tmp_eval_loss.mean().item()\n",
    "                    nb_eval_steps += 1\n",
    "\n",
    "                    outputs = softmax(outputs)\n",
    "                    outputs = torch.argmax(outputs, dim=1)\n",
    "                    correct += (outputs == y_batch).float().sum()\n",
    "                    num_samples += len(X_batch)\n",
    "\n",
    "                eval_loss = eval_loss / nb_eval_steps\n",
    "                eval_accuracy = correct / num_samples\n",
    "            else:\n",
    "                for X_batch, y_batch in train_dataloader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    outputs = model(X_batch)\n",
    "\n",
    "                    loss = loss_func(outputs, y_batch)\n",
    "                    loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                    opt_scheduler.step()\n",
    "\n",
    "                    train_loss += loss.mean().item()\n",
    "                    nb_train_steps += 1\n",
    "\n",
    "                    outputs = softmax(outputs)\n",
    "                    outputs = torch.argmax(outputs, dim=1)\n",
    "                    correct += (outputs == y_batch).float().sum()\n",
    "                    num_samples += len(X_batch)\n",
    "\n",
    "                train_loss = train_loss / nb_train_steps\n",
    "                train_accuracy = correct / num_samples\n",
    "\n",
    "                model.eval()\n",
    "                eval_loss = 0\n",
    "                nb_eval_steps = 0\n",
    "                correct = 0\n",
    "                num_samples = 0\n",
    "\n",
    "                for X_batch, y_batch in eval_dataloader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(X_batch)\n",
    "\n",
    "                    tmp_eval_loss = loss_func(outputs, y_batch)\n",
    "                    eval_loss += tmp_eval_loss.mean().item()\n",
    "                    nb_eval_steps += 1\n",
    "\n",
    "                    outputs = softmax(outputs)\n",
    "                    outputs = torch.argmax(outputs, dim=1)\n",
    "                    correct += (outputs == y_batch).float().sum()\n",
    "                    num_samples += len(X_batch)\n",
    "\n",
    "                eval_loss = eval_loss / nb_eval_steps\n",
    "                eval_accuracy = correct / num_samples\n",
    "\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "            print('epoch: {:3d},    lr={:6f},    loss={:5f},    train_acc={:5f},    eval_loss={:5f},    eval_acc={:5f}'\n",
    "                  .format(epoch+1, lr, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
    "\n",
    "            '''\n",
    "            if((epoch+1) % args.save_checkpoint_steps == 0):\n",
    "                model_checkpoint = \"%s_%s_step_%d.pt\" % ('CLDNN', args.conv_dim, epoch+1)\n",
    "                output_model_file = os.path.join(args.output_dir, model_checkpoint)\n",
    "                if(args.multi_gpu == 'true'):\n",
    "                    torch.save(model.module.state_dict(), output_model_file)\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), output_model_file)\n",
    "                print(\"Saving checkpoint %s\" % output_model_file)\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "epoch:   1,    lr=0.000003,    loss=2.632734,    train_acc=0.138053,    eval_loss=2.629200,    eval_acc=0.133156\n",
      "epoch:   2,    lr=0.000007,    loss=2.620503,    train_acc=0.144018,    eval_loss=2.607507,    eval_acc=0.152473\n",
      "epoch:   3,    lr=0.000010,    loss=2.610122,    train_acc=0.146541,    eval_loss=2.607075,    eval_acc=0.181576\n",
      "epoch:   4,    lr=0.000013,    loss=2.585874,    train_acc=0.161479,    eval_loss=2.532885,    eval_acc=0.191878\n",
      "epoch:   5,    lr=0.000017,    loss=2.425351,    train_acc=0.209953,    eval_loss=2.422811,    eval_acc=0.208620\n",
      "epoch:   6,    lr=0.000020,    loss=2.347336,    train_acc=0.222477,    eval_loss=2.428290,    eval_acc=0.218922\n",
      "epoch:   7,    lr=0.000023,    loss=2.295809,    train_acc=0.230569,    eval_loss=2.525782,    eval_acc=0.222012\n",
      "epoch:   8,    lr=0.000027,    loss=2.243804,    train_acc=0.240406,    eval_loss=2.546046,    eval_acc=0.225790\n",
      "epoch:   9,    lr=0.000030,    loss=2.209056,    train_acc=0.246237,    eval_loss=2.593727,    eval_acc=0.229739\n",
      "epoch:  10,    lr=0.000033,    loss=2.180437,    train_acc=0.251003,    eval_loss=2.629153,    eval_acc=0.254979\n",
      "epoch:  11,    lr=0.000037,    loss=2.154953,    train_acc=0.257454,    eval_loss=2.680574,    eval_acc=0.251803\n",
      "epoch:  12,    lr=0.000040,    loss=2.131980,    train_acc=0.264050,    eval_loss=2.636591,    eval_acc=0.264337\n",
      "epoch:  13,    lr=0.000043,    loss=2.115345,    train_acc=0.268738,    eval_loss=2.577675,    eval_acc=0.269059\n",
      "epoch:  14,    lr=0.000047,    loss=2.101994,    train_acc=0.273638,    eval_loss=2.557077,    eval_acc=0.278932\n",
      "epoch:  15,    lr=0.000050,    loss=2.090524,    train_acc=0.278168,    eval_loss=2.492596,    eval_acc=0.285371\n",
      "epoch:  16,    lr=0.000053,    loss=2.074301,    train_acc=0.281250,    eval_loss=2.479807,    eval_acc=0.291295\n",
      "epoch:  17,    lr=0.000057,    loss=2.060330,    train_acc=0.287950,    eval_loss=2.440395,    eval_acc=0.303314\n",
      "epoch:  18,    lr=0.000060,    loss=2.047542,    train_acc=0.290996,    eval_loss=2.410699,    eval_acc=0.299451\n",
      "epoch:  19,    lr=0.000063,    loss=2.035156,    train_acc=0.296541,    eval_loss=2.403133,    eval_acc=0.291981\n",
      "epoch:  20,    lr=0.000067,    loss=2.023877,    train_acc=0.296735,    eval_loss=2.374847,    eval_acc=0.291810\n",
      "epoch:  21,    lr=0.000070,    loss=2.012274,    train_acc=0.300723,    eval_loss=2.357822,    eval_acc=0.317050\n",
      "epoch:  22,    lr=0.000073,    loss=2.005807,    train_acc=0.302821,    eval_loss=2.325610,    eval_acc=0.314560\n",
      "epoch:  23,    lr=0.000077,    loss=1.996792,    train_acc=0.305685,    eval_loss=2.309511,    eval_acc=0.307177\n",
      "epoch:  24,    lr=0.000080,    loss=1.986660,    train_acc=0.306736,    eval_loss=2.336105,    eval_acc=0.296789\n",
      "epoch:  25,    lr=0.000083,    loss=1.981018,    train_acc=0.310536,    eval_loss=2.262135,    eval_acc=0.314217\n",
      "epoch:  26,    lr=0.000087,    loss=1.972913,    train_acc=0.313126,    eval_loss=2.265961,    eval_acc=0.319454\n",
      "epoch:  27,    lr=0.000090,    loss=1.965444,    train_acc=0.316719,    eval_loss=2.217072,    eval_acc=0.319025\n",
      "epoch:  28,    lr=0.000093,    loss=1.956079,    train_acc=0.318987,    eval_loss=2.207386,    eval_acc=0.313788\n",
      "epoch:  29,    lr=0.000097,    loss=1.946964,    train_acc=0.323346,    eval_loss=2.226857,    eval_acc=0.317565\n",
      "epoch:  30,    lr=0.000100,    loss=1.940569,    train_acc=0.328064,    eval_loss=2.141065,    eval_acc=0.339114\n",
      "epoch:  31,    lr=0.000100,    loss=1.924973,    train_acc=0.331888,    eval_loss=2.126089,    eval_acc=0.337054\n",
      "epoch:  32,    lr=0.000099,    loss=1.914545,    train_acc=0.337567,    eval_loss=2.212822,    eval_acc=0.333963\n",
      "epoch:  33,    lr=0.000099,    loss=1.902528,    train_acc=0.341379,    eval_loss=2.148196,    eval_acc=0.333276\n",
      "epoch:  34,    lr=0.000099,    loss=1.892769,    train_acc=0.346772,    eval_loss=2.124693,    eval_acc=0.339543\n",
      "epoch:  35,    lr=0.000098,    loss=1.881928,    train_acc=0.351496,    eval_loss=2.090754,    eval_acc=0.344351\n",
      "epoch:  36,    lr=0.000098,    loss=1.873355,    train_acc=0.352949,    eval_loss=2.092067,    eval_acc=0.349845\n",
      "epoch:  37,    lr=0.000097,    loss=1.862511,    train_acc=0.357788,    eval_loss=2.077392,    eval_acc=0.345467\n",
      "epoch:  38,    lr=0.000097,    loss=1.851743,    train_acc=0.362026,    eval_loss=2.113014,    eval_acc=0.343321\n",
      "epoch:  39,    lr=0.000097,    loss=1.845125,    train_acc=0.364184,    eval_loss=2.067764,    eval_acc=0.356885\n",
      "epoch:  40,    lr=0.000096,    loss=1.833944,    train_acc=0.368501,    eval_loss=2.083350,    eval_acc=0.343836\n",
      "epoch:  41,    lr=0.000096,    loss=1.825357,    train_acc=0.371693,    eval_loss=2.123127,    eval_acc=0.367188\n",
      "epoch:  42,    lr=0.000096,    loss=1.813208,    train_acc=0.376325,    eval_loss=2.050864,    eval_acc=0.352335\n",
      "epoch:  43,    lr=0.000095,    loss=1.804890,    train_acc=0.377517,    eval_loss=2.033172,    eval_acc=0.367016\n",
      "epoch:  44,    lr=0.000095,    loss=1.794596,    train_acc=0.382594,    eval_loss=2.027289,    eval_acc=0.369162\n",
      "epoch:  45,    lr=0.000094,    loss=1.785378,    train_acc=0.385694,    eval_loss=2.020308,    eval_acc=0.365728\n",
      "epoch:  46,    lr=0.000094,    loss=1.774905,    train_acc=0.389312,    eval_loss=2.016368,    eval_acc=0.364870\n",
      "epoch:  47,    lr=0.000094,    loss=1.763210,    train_acc=0.392455,    eval_loss=2.027764,    eval_acc=0.372940\n",
      "epoch:  48,    lr=0.000093,    loss=1.755173,    train_acc=0.397039,    eval_loss=2.001299,    eval_acc=0.371308\n",
      "epoch:  49,    lr=0.000093,    loss=1.743453,    train_acc=0.402639,    eval_loss=2.009209,    eval_acc=0.372596\n",
      "epoch:  50,    lr=0.000093,    loss=1.733270,    train_acc=0.404760,    eval_loss=1.997327,    eval_acc=0.382126\n",
      "epoch:  51,    lr=0.000092,    loss=1.724932,    train_acc=0.409819,    eval_loss=1.973063,    eval_acc=0.386590\n",
      "epoch:  52,    lr=0.000092,    loss=1.713168,    train_acc=0.414403,    eval_loss=2.007181,    eval_acc=0.376717\n",
      "epoch:  53,    lr=0.000091,    loss=1.701946,    train_acc=0.417188,    eval_loss=2.026799,    eval_acc=0.391913\n",
      "epoch:  54,    lr=0.000091,    loss=1.691712,    train_acc=0.422830,    eval_loss=1.993053,    eval_acc=0.391999\n",
      "epoch:  55,    lr=0.000091,    loss=1.681606,    train_acc=0.424514,    eval_loss=1.944981,    eval_acc=0.394145\n",
      "epoch:  56,    lr=0.000090,    loss=1.670134,    train_acc=0.429657,    eval_loss=1.961565,    eval_acc=0.393286\n",
      "epoch:  57,    lr=0.000090,    loss=1.660956,    train_acc=0.433731,    eval_loss=1.991193,    eval_acc=0.386848\n",
      "epoch:  58,    lr=0.000090,    loss=1.652388,    train_acc=0.436697,    eval_loss=1.978478,    eval_acc=0.399210\n",
      "epoch:  59,    lr=0.000089,    loss=1.641494,    train_acc=0.441312,    eval_loss=1.924400,    eval_acc=0.401957\n",
      "epoch:  60,    lr=0.000089,    loss=1.631163,    train_acc=0.445477,    eval_loss=1.956892,    eval_acc=0.401271\n",
      "epoch:  61,    lr=0.000089,    loss=1.621111,    train_acc=0.449204,    eval_loss=1.942912,    eval_acc=0.391569\n",
      "epoch:  62,    lr=0.000088,    loss=1.609969,    train_acc=0.452608,    eval_loss=1.929959,    eval_acc=0.403417\n",
      "epoch:  63,    lr=0.000088,    loss=1.598897,    train_acc=0.457946,    eval_loss=1.909274,    eval_acc=0.411745\n",
      "epoch:  64,    lr=0.000087,    loss=1.587756,    train_acc=0.461029,    eval_loss=1.935699,    eval_acc=0.405220\n",
      "epoch:  65,    lr=0.000087,    loss=1.578723,    train_acc=0.465011,    eval_loss=1.954502,    eval_acc=0.403159\n",
      "epoch:  66,    lr=0.000087,    loss=1.567416,    train_acc=0.468519,    eval_loss=1.895146,    eval_acc=0.402301\n",
      "epoch:  67,    lr=0.000086,    loss=1.556869,    train_acc=0.472945,    eval_loss=1.911271,    eval_acc=0.416981\n",
      "epoch:  68,    lr=0.000086,    loss=1.547921,    train_acc=0.476271,    eval_loss=1.918322,    eval_acc=0.418956\n",
      "epoch:  69,    lr=0.000086,    loss=1.535853,    train_acc=0.480691,    eval_loss=1.930189,    eval_acc=0.415264\n",
      "epoch:  70,    lr=0.000085,    loss=1.525607,    train_acc=0.485062,    eval_loss=1.909822,    eval_acc=0.415350\n",
      "epoch:  71,    lr=0.000085,    loss=1.516232,    train_acc=0.487822,    eval_loss=1.915564,    eval_acc=0.423163\n",
      "epoch:  72,    lr=0.000084,    loss=1.505684,    train_acc=0.491780,    eval_loss=1.893183,    eval_acc=0.424193\n",
      "epoch:  73,    lr=0.000084,    loss=1.495985,    train_acc=0.495708,    eval_loss=1.894475,    eval_acc=0.420072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  74,    lr=0.000084,    loss=1.487203,    train_acc=0.498711,    eval_loss=1.897858,    eval_acc=0.420416\n",
      "epoch:  75,    lr=0.000083,    loss=1.476993,    train_acc=0.502797,    eval_loss=1.950447,    eval_acc=0.419042\n",
      "epoch:  76,    lr=0.000083,    loss=1.468575,    train_acc=0.507594,    eval_loss=1.904272,    eval_acc=0.430031\n",
      "epoch:  77,    lr=0.000083,    loss=1.459561,    train_acc=0.509174,    eval_loss=1.906553,    eval_acc=0.432349\n",
      "epoch:  78,    lr=0.000082,    loss=1.446509,    train_acc=0.513856,    eval_loss=1.925823,    eval_acc=0.429516\n",
      "epoch:  79,    lr=0.000082,    loss=1.439263,    train_acc=0.517449,    eval_loss=1.923069,    eval_acc=0.425996\n",
      "epoch:  80,    lr=0.000081,    loss=1.430059,    train_acc=0.520264,    eval_loss=1.906843,    eval_acc=0.438187\n",
      "epoch:  81,    lr=0.000081,    loss=1.419465,    train_acc=0.524994,    eval_loss=1.919400,    eval_acc=0.433207\n",
      "epoch:  82,    lr=0.000081,    loss=1.410512,    train_acc=0.527365,    eval_loss=1.910300,    eval_acc=0.430117\n",
      "epoch:  83,    lr=0.000080,    loss=1.401275,    train_acc=0.530502,    eval_loss=1.879111,    eval_acc=0.431319\n",
      "epoch:  84,    lr=0.000080,    loss=1.395185,    train_acc=0.532727,    eval_loss=1.925568,    eval_acc=0.427370\n",
      "epoch:  85,    lr=0.000080,    loss=1.384774,    train_acc=0.537099,    eval_loss=1.926759,    eval_acc=0.439732\n",
      "epoch:  86,    lr=0.000079,    loss=1.376191,    train_acc=0.539798,    eval_loss=1.892518,    eval_acc=0.445227\n",
      "epoch:  87,    lr=0.000079,    loss=1.364891,    train_acc=0.543957,    eval_loss=1.920572,    eval_acc=0.445484\n",
      "epoch:  88,    lr=0.000079,    loss=1.359216,    train_acc=0.547744,    eval_loss=1.899748,    eval_acc=0.436126\n",
      "epoch:  89,    lr=0.000078,    loss=1.351311,    train_acc=0.549775,    eval_loss=1.934963,    eval_acc=0.451408\n",
      "epoch:  90,    lr=0.000078,    loss=1.341248,    train_acc=0.552474,    eval_loss=1.930245,    eval_acc=0.451408\n",
      "epoch:  91,    lr=0.000077,    loss=1.331330,    train_acc=0.556584,    eval_loss=1.898772,    eval_acc=0.447030\n",
      "epoch:  92,    lr=0.000077,    loss=1.324251,    train_acc=0.558493,    eval_loss=1.920311,    eval_acc=0.447373\n",
      "epoch:  93,    lr=0.000077,    loss=1.316824,    train_acc=0.560919,    eval_loss=1.925999,    eval_acc=0.446085\n",
      "epoch:  94,    lr=0.000076,    loss=1.306966,    train_acc=0.565637,    eval_loss=1.934211,    eval_acc=0.450721\n",
      "epoch:  95,    lr=0.000076,    loss=1.298747,    train_acc=0.568337,    eval_loss=1.970657,    eval_acc=0.450549\n",
      "epoch:  96,    lr=0.000076,    loss=1.288638,    train_acc=0.572793,    eval_loss=1.920683,    eval_acc=0.466260\n",
      "epoch:  97,    lr=0.000075,    loss=1.281597,    train_acc=0.574787,    eval_loss=1.948490,    eval_acc=0.451580\n",
      "epoch:  98,    lr=0.000075,    loss=1.274486,    train_acc=0.577037,    eval_loss=1.919072,    eval_acc=0.459736\n",
      "epoch:  99,    lr=0.000074,    loss=1.267299,    train_acc=0.579408,    eval_loss=1.922643,    eval_acc=0.454413\n",
      "epoch: 100,    lr=0.000074,    loss=1.262671,    train_acc=0.580910,    eval_loss=1.932081,    eval_acc=0.458620\n",
      "epoch: 101,    lr=0.000074,    loss=1.252316,    train_acc=0.584095,    eval_loss=1.937343,    eval_acc=0.450034\n",
      "epoch: 102,    lr=0.000073,    loss=1.242612,    train_acc=0.587804,    eval_loss=2.016259,    eval_acc=0.450034\n",
      "epoch: 103,    lr=0.000073,    loss=1.238261,    train_acc=0.589750,    eval_loss=1.990834,    eval_acc=0.456302\n",
      "epoch: 104,    lr=0.000073,    loss=1.230925,    train_acc=0.592759,    eval_loss=1.973189,    eval_acc=0.459306\n",
      "epoch: 105,    lr=0.000072,    loss=1.221755,    train_acc=0.595665,    eval_loss=1.938742,    eval_acc=0.459478\n",
      "epoch: 106,    lr=0.000072,    loss=1.216810,    train_acc=0.597203,    eval_loss=1.965317,    eval_acc=0.461624\n",
      "epoch: 107,    lr=0.000071,    loss=1.206638,    train_acc=0.601259,    eval_loss=1.986337,    eval_acc=0.462397\n",
      "epoch: 108,    lr=0.000071,    loss=1.203237,    train_acc=0.602231,    eval_loss=1.939543,    eval_acc=0.464028\n",
      "epoch: 109,    lr=0.000071,    loss=1.197821,    train_acc=0.604049,    eval_loss=1.949902,    eval_acc=0.466432\n",
      "epoch: 110,    lr=0.000070,    loss=1.189918,    train_acc=0.607575,    eval_loss=1.969352,    eval_acc=0.460766\n",
      "epoch: 111,    lr=0.000070,    loss=1.181205,    train_acc=0.609168,    eval_loss=1.953738,    eval_acc=0.466346\n",
      "epoch: 112,    lr=0.000070,    loss=1.177304,    train_acc=0.610755,    eval_loss=1.978847,    eval_acc=0.467205\n",
      "epoch: 113,    lr=0.000069,    loss=1.169599,    train_acc=0.614415,    eval_loss=1.976907,    eval_acc=0.465230\n",
      "epoch: 114,    lr=0.000069,    loss=1.162653,    train_acc=0.616476,    eval_loss=1.964017,    eval_acc=0.473558\n",
      "epoch: 115,    lr=0.000069,    loss=1.157795,    train_acc=0.618598,    eval_loss=1.976817,    eval_acc=0.465573\n",
      "epoch: 116,    lr=0.000068,    loss=1.149722,    train_acc=0.621164,    eval_loss=2.011256,    eval_acc=0.468321\n",
      "epoch: 117,    lr=0.000068,    loss=1.145506,    train_acc=0.621857,    eval_loss=2.009263,    eval_acc=0.466432\n",
      "epoch: 118,    lr=0.000067,    loss=1.140551,    train_acc=0.624951,    eval_loss=1.989384,    eval_acc=0.469609\n",
      "epoch: 119,    lr=0.000067,    loss=1.133965,    train_acc=0.625948,    eval_loss=1.985731,    eval_acc=0.473729\n",
      "epoch: 120,    lr=0.000067,    loss=1.128300,    train_acc=0.629152,    eval_loss=1.979652,    eval_acc=0.473729\n",
      "epoch: 121,    lr=0.000066,    loss=1.122671,    train_acc=0.630399,    eval_loss=1.989059,    eval_acc=0.466260\n",
      "epoch: 122,    lr=0.000066,    loss=1.113914,    train_acc=0.633317,    eval_loss=1.980734,    eval_acc=0.474159\n",
      "epoch: 123,    lr=0.000066,    loss=1.110350,    train_acc=0.635105,    eval_loss=2.022919,    eval_acc=0.478623\n",
      "epoch: 124,    lr=0.000065,    loss=1.103639,    train_acc=0.636558,    eval_loss=2.036230,    eval_acc=0.466861\n",
      "epoch: 125,    lr=0.000065,    loss=1.099198,    train_acc=0.638023,    eval_loss=2.013056,    eval_acc=0.479138\n",
      "epoch: 126,    lr=0.000064,    loss=1.091447,    train_acc=0.641622,    eval_loss=1.998071,    eval_acc=0.479825\n",
      "epoch: 127,    lr=0.000064,    loss=1.088937,    train_acc=0.642017,    eval_loss=2.032392,    eval_acc=0.477078\n",
      "epoch: 128,    lr=0.000064,    loss=1.085533,    train_acc=0.642881,    eval_loss=2.032883,    eval_acc=0.475876\n",
      "epoch: 129,    lr=0.000063,    loss=1.075618,    train_acc=0.646650,    eval_loss=2.029380,    eval_acc=0.474931\n",
      "epoch: 130,    lr=0.000063,    loss=1.073925,    train_acc=0.646984,    eval_loss=2.044648,    eval_acc=0.475189\n",
      "epoch: 131,    lr=0.000063,    loss=1.067849,    train_acc=0.649708,    eval_loss=2.045081,    eval_acc=0.476047\n",
      "epoch: 132,    lr=0.000062,    loss=1.061451,    train_acc=0.651982,    eval_loss=2.072481,    eval_acc=0.471411\n",
      "epoch: 133,    lr=0.000062,    loss=1.058751,    train_acc=0.653417,    eval_loss=2.085987,    eval_acc=0.470467\n",
      "epoch: 134,    lr=0.000061,    loss=1.052077,    train_acc=0.655107,    eval_loss=2.027935,    eval_acc=0.477850\n",
      "epoch: 135,    lr=0.000061,    loss=1.048301,    train_acc=0.655867,    eval_loss=2.073183,    eval_acc=0.478966\n",
      "epoch: 136,    lr=0.000061,    loss=1.044656,    train_acc=0.657125,    eval_loss=2.085347,    eval_acc=0.472957\n",
      "epoch: 137,    lr=0.000060,    loss=1.041009,    train_acc=0.658864,    eval_loss=2.085089,    eval_acc=0.475876\n",
      "epoch: 138,    lr=0.000060,    loss=1.032802,    train_acc=0.661661,    eval_loss=2.073995,    eval_acc=0.476477\n",
      "epoch: 139,    lr=0.000060,    loss=1.034536,    train_acc=0.661047,    eval_loss=2.088854,    eval_acc=0.479567\n",
      "epoch: 140,    lr=0.000059,    loss=1.026327,    train_acc=0.663777,    eval_loss=2.095212,    eval_acc=0.482658\n",
      "epoch: 141,    lr=0.000059,    loss=1.024169,    train_acc=0.663996,    eval_loss=2.109735,    eval_acc=0.481542\n",
      "epoch: 142,    lr=0.000059,    loss=1.017276,    train_acc=0.666950,    eval_loss=2.172852,    eval_acc=0.468664\n",
      "epoch: 143,    lr=0.000058,    loss=1.016814,    train_acc=0.667844,    eval_loss=2.103993,    eval_acc=0.481284\n",
      "epoch: 144,    lr=0.000058,    loss=1.009860,    train_acc=0.670039,    eval_loss=2.106097,    eval_acc=0.477078\n",
      "epoch: 145,    lr=0.000057,    loss=1.008120,    train_acc=0.670744,    eval_loss=2.090320,    eval_acc=0.482229\n",
      "epoch: 146,    lr=0.000057,    loss=1.002853,    train_acc=0.672112,    eval_loss=2.121736,    eval_acc=0.480598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147,    lr=0.000057,    loss=0.998842,    train_acc=0.675152,    eval_loss=2.132060,    eval_acc=0.481027\n",
      "epoch: 148,    lr=0.000056,    loss=0.993906,    train_acc=0.675292,    eval_loss=2.120951,    eval_acc=0.478280\n",
      "epoch: 149,    lr=0.000056,    loss=0.991375,    train_acc=0.676958,    eval_loss=2.130699,    eval_acc=0.481113\n",
      "epoch: 150,    lr=0.000056,    loss=0.985840,    train_acc=0.677468,    eval_loss=2.117728,    eval_acc=0.486350\n",
      "epoch: 151,    lr=0.000055,    loss=0.980751,    train_acc=0.679663,    eval_loss=2.136845,    eval_acc=0.477764\n",
      "epoch: 152,    lr=0.000055,    loss=0.976481,    train_acc=0.680697,    eval_loss=2.144647,    eval_acc=0.481628\n",
      "epoch: 153,    lr=0.000054,    loss=0.973107,    train_acc=0.682515,    eval_loss=2.195138,    eval_acc=0.483602\n",
      "epoch: 154,    lr=0.000054,    loss=0.970835,    train_acc=0.683962,    eval_loss=2.195131,    eval_acc=0.483602\n",
      "epoch: 155,    lr=0.000054,    loss=0.965202,    train_acc=0.685664,    eval_loss=2.162697,    eval_acc=0.485491\n",
      "epoch: 156,    lr=0.000053,    loss=0.963743,    train_acc=0.686193,    eval_loss=2.212413,    eval_acc=0.480340\n",
      "epoch: 157,    lr=0.000053,    loss=0.958900,    train_acc=0.686971,    eval_loss=2.183631,    eval_acc=0.479052\n",
      "epoch: 158,    lr=0.000053,    loss=0.954092,    train_acc=0.688947,    eval_loss=2.178339,    eval_acc=0.478280\n",
      "epoch: 159,    lr=0.000052,    loss=0.950141,    train_acc=0.690060,    eval_loss=2.201166,    eval_acc=0.484547\n",
      "epoch: 160,    lr=0.000052,    loss=0.946419,    train_acc=0.691975,    eval_loss=2.203204,    eval_acc=0.490299\n",
      "epoch: 161,    lr=0.000051,    loss=0.945545,    train_acc=0.691859,    eval_loss=2.184014,    eval_acc=0.485749\n",
      "epoch: 162,    lr=0.000051,    loss=0.937943,    train_acc=0.695428,    eval_loss=2.210034,    eval_acc=0.481542\n",
      "epoch: 163,    lr=0.000051,    loss=0.938584,    train_acc=0.694929,    eval_loss=2.222581,    eval_acc=0.481542\n",
      "epoch: 164,    lr=0.000050,    loss=0.932757,    train_acc=0.696601,    eval_loss=2.195730,    eval_acc=0.481714\n",
      "epoch: 165,    lr=0.000050,    loss=0.935026,    train_acc=0.695908,    eval_loss=2.248170,    eval_acc=0.480512\n",
      "epoch: 166,    lr=0.000050,    loss=0.927654,    train_acc=0.698079,    eval_loss=2.270677,    eval_acc=0.482057\n",
      "epoch: 167,    lr=0.000049,    loss=0.925828,    train_acc=0.699623,    eval_loss=2.213079,    eval_acc=0.484976\n",
      "epoch: 168,    lr=0.000049,    loss=0.921590,    train_acc=0.700608,    eval_loss=2.241291,    eval_acc=0.483173\n",
      "epoch: 169,    lr=0.000049,    loss=0.922290,    train_acc=0.700602,    eval_loss=2.225592,    eval_acc=0.484203\n",
      "epoch: 170,    lr=0.000048,    loss=0.916970,    train_acc=0.702529,    eval_loss=2.273178,    eval_acc=0.482229\n",
      "epoch: 171,    lr=0.000048,    loss=0.910429,    train_acc=0.704134,    eval_loss=2.216104,    eval_acc=0.484461\n",
      "epoch: 172,    lr=0.000047,    loss=0.910807,    train_acc=0.704152,    eval_loss=2.262401,    eval_acc=0.480254\n",
      "epoch: 173,    lr=0.000047,    loss=0.907623,    train_acc=0.704742,    eval_loss=2.264668,    eval_acc=0.485663\n",
      "epoch: 174,    lr=0.000047,    loss=0.906199,    train_acc=0.706159,    eval_loss=2.313648,    eval_acc=0.482830\n",
      "epoch: 175,    lr=0.000046,    loss=0.903324,    train_acc=0.706755,    eval_loss=2.280435,    eval_acc=0.484633\n",
      "epoch: 176,    lr=0.000046,    loss=0.902071,    train_acc=0.707259,    eval_loss=2.283146,    eval_acc=0.486779\n",
      "epoch: 177,    lr=0.000046,    loss=0.896789,    train_acc=0.709527,    eval_loss=2.305861,    eval_acc=0.478880\n",
      "epoch: 178,    lr=0.000045,    loss=0.888585,    train_acc=0.712871,    eval_loss=2.300985,    eval_acc=0.484804\n",
      "epoch: 179,    lr=0.000045,    loss=0.890066,    train_acc=0.710944,    eval_loss=2.298051,    eval_acc=0.484461\n",
      "epoch: 180,    lr=0.000044,    loss=0.887909,    train_acc=0.712470,    eval_loss=2.323187,    eval_acc=0.484461\n",
      "epoch: 181,    lr=0.000044,    loss=0.885771,    train_acc=0.713473,    eval_loss=2.339076,    eval_acc=0.488324\n",
      "epoch: 182,    lr=0.000044,    loss=0.884346,    train_acc=0.713230,    eval_loss=2.305831,    eval_acc=0.485405\n",
      "epoch: 183,    lr=0.000043,    loss=0.878914,    train_acc=0.715680,    eval_loss=2.311087,    eval_acc=0.484117\n",
      "epoch: 184,    lr=0.000043,    loss=0.875163,    train_acc=0.716725,    eval_loss=2.348336,    eval_acc=0.489269\n",
      "epoch: 185,    lr=0.000043,    loss=0.874991,    train_acc=0.716981,    eval_loss=2.371881,    eval_acc=0.482572\n",
      "epoch: 186,    lr=0.000042,    loss=0.871781,    train_acc=0.717856,    eval_loss=2.359055,    eval_acc=0.482658\n",
      "epoch: 187,    lr=0.000042,    loss=0.868249,    train_acc=0.719680,    eval_loss=2.336601,    eval_acc=0.485491\n",
      "epoch: 188,    lr=0.000041,    loss=0.867888,    train_acc=0.720775,    eval_loss=2.362026,    eval_acc=0.490900\n",
      "epoch: 189,    lr=0.000041,    loss=0.864570,    train_acc=0.720750,    eval_loss=2.364139,    eval_acc=0.490470\n",
      "epoch: 190,    lr=0.000041,    loss=0.860643,    train_acc=0.721291,    eval_loss=2.373678,    eval_acc=0.482830\n",
      "epoch: 191,    lr=0.000040,    loss=0.862468,    train_acc=0.721012,    eval_loss=2.416198,    eval_acc=0.476906\n",
      "epoch: 192,    lr=0.000040,    loss=0.857451,    train_acc=0.722118,    eval_loss=2.370592,    eval_acc=0.486521\n",
      "epoch: 193,    lr=0.000040,    loss=0.854138,    train_acc=0.724410,    eval_loss=2.409093,    eval_acc=0.486435\n",
      "epoch: 194,    lr=0.000039,    loss=0.850660,    train_acc=0.725948,    eval_loss=2.427106,    eval_acc=0.484804\n",
      "epoch: 195,    lr=0.000039,    loss=0.850997,    train_acc=0.724854,    eval_loss=2.422657,    eval_acc=0.482572\n",
      "epoch: 196,    lr=0.000039,    loss=0.848335,    train_acc=0.726070,    eval_loss=2.399373,    eval_acc=0.490900\n",
      "epoch: 197,    lr=0.000038,    loss=0.844930,    train_acc=0.727438,    eval_loss=2.427923,    eval_acc=0.484289\n",
      "epoch: 198,    lr=0.000038,    loss=0.843890,    train_acc=0.726696,    eval_loss=2.399468,    eval_acc=0.490814\n",
      "epoch: 199,    lr=0.000037,    loss=0.842414,    train_acc=0.728119,    eval_loss=2.422393,    eval_acc=0.493561\n",
      "epoch: 200,    lr=0.000037,    loss=0.843030,    train_acc=0.727699,    eval_loss=2.409288,    eval_acc=0.486006\n",
      "epoch: 201,    lr=0.000037,    loss=0.838604,    train_acc=0.728289,    eval_loss=2.423967,    eval_acc=0.485148\n",
      "epoch: 202,    lr=0.000036,    loss=0.836715,    train_acc=0.729572,    eval_loss=2.411542,    eval_acc=0.488925\n",
      "epoch: 203,    lr=0.000036,    loss=0.833264,    train_acc=0.731621,    eval_loss=2.410739,    eval_acc=0.486693\n",
      "epoch: 204,    lr=0.000036,    loss=0.830378,    train_acc=0.731821,    eval_loss=2.479370,    eval_acc=0.482315\n",
      "epoch: 205,    lr=0.000035,    loss=0.826531,    train_acc=0.734491,    eval_loss=2.448942,    eval_acc=0.486865\n",
      "epoch: 206,    lr=0.000035,    loss=0.831883,    train_acc=0.731025,    eval_loss=2.442327,    eval_acc=0.486435\n",
      "epoch: 207,    lr=0.000034,    loss=0.827040,    train_acc=0.732186,    eval_loss=2.476812,    eval_acc=0.477850\n",
      "epoch: 208,    lr=0.000034,    loss=0.823737,    train_acc=0.734630,    eval_loss=2.449813,    eval_acc=0.486779\n",
      "epoch: 209,    lr=0.000034,    loss=0.821491,    train_acc=0.735822,    eval_loss=2.481711,    eval_acc=0.485577\n",
      "epoch: 210,    lr=0.000033,    loss=0.817880,    train_acc=0.736740,    eval_loss=2.472961,    eval_acc=0.483345\n",
      "epoch: 211,    lr=0.000033,    loss=0.822483,    train_acc=0.735646,    eval_loss=2.521644,    eval_acc=0.483946\n",
      "epoch: 212,    lr=0.000033,    loss=0.817186,    train_acc=0.735931,    eval_loss=2.501019,    eval_acc=0.485920\n",
      "epoch: 213,    lr=0.000032,    loss=0.815445,    train_acc=0.737403,    eval_loss=2.496866,    eval_acc=0.489097\n",
      "epoch: 214,    lr=0.000032,    loss=0.814122,    train_acc=0.738072,    eval_loss=2.486054,    eval_acc=0.484461\n",
      "epoch: 215,    lr=0.000031,    loss=0.812970,    train_acc=0.738290,    eval_loss=2.506203,    eval_acc=0.487380\n",
      "epoch: 216,    lr=0.000031,    loss=0.807628,    train_acc=0.739829,    eval_loss=2.506370,    eval_acc=0.484117\n",
      "epoch: 217,    lr=0.000031,    loss=0.806007,    train_acc=0.740370,    eval_loss=2.523604,    eval_acc=0.488582\n",
      "epoch: 218,    lr=0.000030,    loss=0.807720,    train_acc=0.740728,    eval_loss=2.503662,    eval_acc=0.485577\n",
      "epoch: 219,    lr=0.000030,    loss=0.803609,    train_acc=0.741336,    eval_loss=2.529744,    eval_acc=0.487466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 220,    lr=0.000030,    loss=0.806187,    train_acc=0.741233,    eval_loss=2.571776,    eval_acc=0.482229\n",
      "epoch: 221,    lr=0.000029,    loss=0.802775,    train_acc=0.741762,    eval_loss=2.546746,    eval_acc=0.484804\n",
      "epoch: 222,    lr=0.000029,    loss=0.799335,    train_acc=0.742947,    eval_loss=2.544183,    eval_acc=0.486264\n",
      "epoch: 223,    lr=0.000029,    loss=0.799088,    train_acc=0.743501,    eval_loss=2.550378,    eval_acc=0.492445\n",
      "epoch: 224,    lr=0.000028,    loss=0.800311,    train_acc=0.742625,    eval_loss=2.558586,    eval_acc=0.487380\n",
      "epoch: 225,    lr=0.000028,    loss=0.795194,    train_acc=0.745282,    eval_loss=2.527855,    eval_acc=0.489354\n",
      "epoch: 226,    lr=0.000027,    loss=0.795928,    train_acc=0.743282,    eval_loss=2.560362,    eval_acc=0.490642\n",
      "epoch: 227,    lr=0.000027,    loss=0.797524,    train_acc=0.742850,    eval_loss=2.560166,    eval_acc=0.488067\n",
      "epoch: 228,    lr=0.000027,    loss=0.789786,    train_acc=0.745313,    eval_loss=2.552467,    eval_acc=0.487895\n",
      "epoch: 229,    lr=0.000026,    loss=0.787805,    train_acc=0.746601,    eval_loss=2.563807,    eval_acc=0.484461\n",
      "epoch: 230,    lr=0.000026,    loss=0.792009,    train_acc=0.745288,    eval_loss=2.547650,    eval_acc=0.489440\n",
      "epoch: 231,    lr=0.000026,    loss=0.787657,    train_acc=0.746802,    eval_loss=2.593853,    eval_acc=0.488238\n",
      "epoch: 232,    lr=0.000025,    loss=0.788761,    train_acc=0.746243,    eval_loss=2.588923,    eval_acc=0.483602\n",
      "epoch: 233,    lr=0.000025,    loss=0.783128,    train_acc=0.747817,    eval_loss=2.595449,    eval_acc=0.483087\n",
      "epoch: 234,    lr=0.000024,    loss=0.784952,    train_acc=0.747981,    eval_loss=2.592474,    eval_acc=0.486264\n",
      "epoch: 235,    lr=0.000024,    loss=0.785125,    train_acc=0.747902,    eval_loss=2.604305,    eval_acc=0.480683\n",
      "epoch: 236,    lr=0.000024,    loss=0.780806,    train_acc=0.748358,    eval_loss=2.593804,    eval_acc=0.491587\n",
      "epoch: 237,    lr=0.000023,    loss=0.780817,    train_acc=0.749003,    eval_loss=2.590699,    eval_acc=0.485749\n",
      "epoch: 238,    lr=0.000023,    loss=0.780331,    train_acc=0.749818,    eval_loss=2.607551,    eval_acc=0.487637\n",
      "epoch: 239,    lr=0.000023,    loss=0.776563,    train_acc=0.751009,    eval_loss=2.608816,    eval_acc=0.489097\n",
      "epoch: 240,    lr=0.000022,    loss=0.776595,    train_acc=0.750736,    eval_loss=2.642835,    eval_acc=0.486607\n",
      "epoch: 241,    lr=0.000022,    loss=0.775187,    train_acc=0.751617,    eval_loss=2.594849,    eval_acc=0.492531\n",
      "epoch: 242,    lr=0.000021,    loss=0.775245,    train_acc=0.750948,    eval_loss=2.604697,    eval_acc=0.488410\n",
      "epoch: 243,    lr=0.000021,    loss=0.770485,    train_acc=0.753107,    eval_loss=2.631475,    eval_acc=0.488839\n",
      "epoch: 244,    lr=0.000021,    loss=0.768565,    train_acc=0.753119,    eval_loss=2.644759,    eval_acc=0.487294\n",
      "epoch: 245,    lr=0.000020,    loss=0.771708,    train_acc=0.753168,    eval_loss=2.647366,    eval_acc=0.488839\n",
      "epoch: 246,    lr=0.000020,    loss=0.769168,    train_acc=0.753374,    eval_loss=2.665012,    eval_acc=0.486264\n",
      "epoch: 247,    lr=0.000020,    loss=0.765196,    train_acc=0.754846,    eval_loss=2.650139,    eval_acc=0.486006\n",
      "epoch: 248,    lr=0.000019,    loss=0.767298,    train_acc=0.753867,    eval_loss=2.684873,    eval_acc=0.484890\n",
      "epoch: 249,    lr=0.000019,    loss=0.765460,    train_acc=0.753934,    eval_loss=2.675963,    eval_acc=0.488152\n",
      "epoch: 250,    lr=0.000019,    loss=0.765324,    train_acc=0.754329,    eval_loss=2.667860,    eval_acc=0.486350\n",
      "epoch: 251,    lr=0.000018,    loss=0.763705,    train_acc=0.755393,    eval_loss=2.643414,    eval_acc=0.490385\n",
      "epoch: 252,    lr=0.000018,    loss=0.761920,    train_acc=0.755648,    eval_loss=2.693552,    eval_acc=0.483173\n",
      "epoch: 253,    lr=0.000017,    loss=0.761521,    train_acc=0.756086,    eval_loss=2.666135,    eval_acc=0.487380\n",
      "epoch: 254,    lr=0.000017,    loss=0.760758,    train_acc=0.755952,    eval_loss=2.693685,    eval_acc=0.486435\n",
      "epoch: 255,    lr=0.000017,    loss=0.757929,    train_acc=0.757721,    eval_loss=2.668205,    eval_acc=0.488067\n",
      "epoch: 256,    lr=0.000016,    loss=0.761383,    train_acc=0.756560,    eval_loss=2.692392,    eval_acc=0.489784\n",
      "epoch: 257,    lr=0.000016,    loss=0.757052,    train_acc=0.757107,    eval_loss=2.677734,    eval_acc=0.486607\n",
      "epoch: 258,    lr=0.000016,    loss=0.757003,    train_acc=0.758275,    eval_loss=2.716953,    eval_acc=0.483173\n",
      "epoch: 259,    lr=0.000015,    loss=0.757035,    train_acc=0.756730,    eval_loss=2.695689,    eval_acc=0.483345\n",
      "epoch: 260,    lr=0.000015,    loss=0.754828,    train_acc=0.758354,    eval_loss=2.717534,    eval_acc=0.483688\n",
      "epoch: 261,    lr=0.000014,    loss=0.751484,    train_acc=0.759168,    eval_loss=2.693906,    eval_acc=0.490900\n",
      "epoch: 262,    lr=0.000014,    loss=0.755886,    train_acc=0.758196,    eval_loss=2.731876,    eval_acc=0.484804\n",
      "epoch: 263,    lr=0.000014,    loss=0.753272,    train_acc=0.759035,    eval_loss=2.715186,    eval_acc=0.483602\n",
      "epoch: 264,    lr=0.000013,    loss=0.749902,    train_acc=0.759667,    eval_loss=2.724876,    eval_acc=0.487552\n",
      "epoch: 265,    lr=0.000013,    loss=0.749508,    train_acc=0.759563,    eval_loss=2.724749,    eval_acc=0.486521\n",
      "epoch: 266,    lr=0.000013,    loss=0.749718,    train_acc=0.760184,    eval_loss=2.711628,    eval_acc=0.487723\n",
      "epoch: 267,    lr=0.000012,    loss=0.747034,    train_acc=0.759922,    eval_loss=2.715925,    eval_acc=0.489097\n",
      "epoch: 268,    lr=0.000012,    loss=0.745360,    train_acc=0.762141,    eval_loss=2.725554,    eval_acc=0.484976\n",
      "epoch: 269,    lr=0.000011,    loss=0.747560,    train_acc=0.760372,    eval_loss=2.708238,    eval_acc=0.485491\n",
      "epoch: 270,    lr=0.000011,    loss=0.749214,    train_acc=0.760506,    eval_loss=2.745248,    eval_acc=0.484203\n",
      "epoch: 271,    lr=0.000011,    loss=0.747137,    train_acc=0.760506,    eval_loss=2.720730,    eval_acc=0.493304\n",
      "epoch: 272,    lr=0.000010,    loss=0.743879,    train_acc=0.761977,    eval_loss=2.709294,    eval_acc=0.486264\n",
      "epoch: 273,    lr=0.000010,    loss=0.740907,    train_acc=0.763619,    eval_loss=2.749006,    eval_acc=0.488152\n",
      "epoch: 274,    lr=0.000010,    loss=0.741387,    train_acc=0.762378,    eval_loss=2.736281,    eval_acc=0.486607\n",
      "epoch: 275,    lr=0.000009,    loss=0.741192,    train_acc=0.762695,    eval_loss=2.750549,    eval_acc=0.482916\n",
      "epoch: 276,    lr=0.000009,    loss=0.740182,    train_acc=0.763424,    eval_loss=2.749835,    eval_acc=0.485062\n",
      "epoch: 277,    lr=0.000009,    loss=0.740528,    train_acc=0.762664,    eval_loss=2.740249,    eval_acc=0.491501\n",
      "epoch: 278,    lr=0.000008,    loss=0.738479,    train_acc=0.763546,    eval_loss=2.732881,    eval_acc=0.489097\n",
      "epoch: 279,    lr=0.000008,    loss=0.741100,    train_acc=0.762141,    eval_loss=2.748994,    eval_acc=0.486521\n",
      "epoch: 280,    lr=0.000007,    loss=0.738661,    train_acc=0.763831,    eval_loss=2.759257,    eval_acc=0.484117\n",
      "epoch: 281,    lr=0.000007,    loss=0.740102,    train_acc=0.763880,    eval_loss=2.752843,    eval_acc=0.489269\n",
      "epoch: 282,    lr=0.000007,    loss=0.736039,    train_acc=0.764974,    eval_loss=2.758357,    eval_acc=0.484890\n",
      "epoch: 283,    lr=0.000006,    loss=0.736387,    train_acc=0.764373,    eval_loss=2.780365,    eval_acc=0.483087\n",
      "epoch: 284,    lr=0.000006,    loss=0.734467,    train_acc=0.764768,    eval_loss=2.765773,    eval_acc=0.487466\n",
      "epoch: 285,    lr=0.000006,    loss=0.735641,    train_acc=0.764008,    eval_loss=2.764148,    eval_acc=0.487552\n",
      "epoch: 286,    lr=0.000005,    loss=0.735144,    train_acc=0.764373,    eval_loss=2.770183,    eval_acc=0.486693\n",
      "epoch: 287,    lr=0.000005,    loss=0.733511,    train_acc=0.765236,    eval_loss=2.778670,    eval_acc=0.482400\n",
      "epoch: 288,    lr=0.000004,    loss=0.732098,    train_acc=0.765893,    eval_loss=2.770758,    eval_acc=0.486435\n",
      "epoch: 289,    lr=0.000004,    loss=0.734549,    train_acc=0.764871,    eval_loss=2.756768,    eval_acc=0.488324\n",
      "epoch: 290,    lr=0.000004,    loss=0.733596,    train_acc=0.765133,    eval_loss=2.760911,    eval_acc=0.488410\n",
      "epoch: 291,    lr=0.000003,    loss=0.733315,    train_acc=0.765206,    eval_loss=2.768363,    eval_acc=0.486607\n",
      "epoch: 292,    lr=0.000003,    loss=0.731059,    train_acc=0.766525,    eval_loss=2.780367,    eval_acc=0.483001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 293,    lr=0.000003,    loss=0.730578,    train_acc=0.766762,    eval_loss=2.760309,    eval_acc=0.487294\n",
      "epoch: 294,    lr=0.000002,    loss=0.729227,    train_acc=0.767224,    eval_loss=2.779840,    eval_acc=0.484375\n",
      "epoch: 295,    lr=0.000002,    loss=0.729555,    train_acc=0.765777,    eval_loss=2.788429,    eval_acc=0.484289\n",
      "epoch: 296,    lr=0.000001,    loss=0.729703,    train_acc=0.766914,    eval_loss=2.774443,    eval_acc=0.487981\n",
      "epoch: 297,    lr=0.000001,    loss=0.729008,    train_acc=0.766111,    eval_loss=2.779016,    eval_acc=0.485663\n",
      "epoch: 298,    lr=0.000001,    loss=0.730030,    train_acc=0.766513,    eval_loss=2.774753,    eval_acc=0.485491\n",
      "epoch: 299,    lr=0.000000,    loss=0.727641,    train_acc=0.767777,    eval_loss=2.777169,    eval_acc=0.486693\n",
      "epoch: 300,    lr=0.000000,    loss=0.726772,    train_acc=0.767437,    eval_loss=2.790295,    eval_acc=0.485577\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, eval_dataloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "if(multi_task == 'true'):\n",
    "    model_g.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "n = 0\n",
    "\n",
    "for i in range(len(eval_samples)):\n",
    "    try:\n",
    "        X_new = preprocessing(eval_samples[i], method='mfcc', sr=16000, n_mfcc=n_mfcc)\n",
    "        X_new = convert_tensor(X_new).to(device)\n",
    "        y_new = model(X_new)\n",
    "        y_new = torch.argmax(nn.Softmax(dim=-1)(torch.mean(y_new, dim=0)))\n",
    "        #y_new = sorted(dict(collections.Counter(torch.argmax(nn.Softmax(dim=-1)(y_new), dim=1).cpu().numpy()))\n",
    "        #               .items(), key=(lambda x: x[1]), reverse=True)[0][0]\n",
    "        y_new = 1 if (y_new.item() == y[eval_idx][i].item()) else 0\n",
    "        correct += y_new\n",
    "        n += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "acc = correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuray: 0.88611\n"
     ]
    }
   ],
   "source": [
    "print('Test accuray:', round(acc, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuray: 0.85278\n"
     ]
    }
   ],
   "source": [
    "print('Test accuray:', round(acc, 5))  # 0.7111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1DCNN  \n",
    "Test accuray: 0.64722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(eval_samples) - set(train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
