{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# server3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from preprocessing import preprocessing, convert_spectrograms, convert_tensor\n",
    "from model_ae import Encoder\n",
    "from utils.optimization import WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dim = '1d'\n",
    "checkpoint = './model/aae_1d_step_300.pt'\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "bidirectional = 'true'\n",
    "with_focus_attn = 'true'\n",
    "\n",
    "batch_size = 256\n",
    "num_epochs = 300\n",
    "learning_rate = 0.0001\n",
    "\n",
    "use_warmup = 'true'\n",
    "train_dir = './wav_data/pretrain/wav_train/'\n",
    "eval_dir = './wav_data/pretrain/wav_eval/'\n",
    "multi_task = 'false'\n",
    "augmentation = 'true'\n",
    "\n",
    "save_checkpoint_steps = 10\n",
    "output_dir = 'model_check'\n",
    "\n",
    "bidirectional = True if(bidirectional == 'true') else False\n",
    "with_focus_attn = True if(with_focus_attn == 'true') else False\n",
    "n_mfcc = 40 if(conv_dim == '1d') else 128\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "train_datas = glob.glob(os.path.join(train_dir, '**', '*wav'), recursive=True)\n",
    "eval_datas = glob.glob(os.path.join(eval_dir, '**', '*wav'), recursive=True)\n",
    "train_datas = sorted(train_datas, key=len)\n",
    "eval_datas = sorted(eval_datas, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1228, 165)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_datas), len(eval_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_attn_heads, attn_hidden_size, dropout_prob, with_focus_attn):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        self.hidden_size = attn_hidden_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.with_focus_attn = with_focus_attn\n",
    "        \n",
    "        self.attn_head_size = int(self.hidden_size / self.num_attn_heads)\n",
    "        self.all_head_size = self.num_attn_heads * self.attn_head_size\n",
    "\n",
    "        self.query = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        if(with_focus_attn == True):\n",
    "            self.tanh = nn.Tanh()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "            self.linear_focus_query = nn.Linear(num_attn_heads * self.attn_head_size, \n",
    "                                                num_attn_heads * self.attn_head_size)\n",
    "            self.linear_focus_global = nn.Linear(num_attn_heads * self.attn_head_size, \n",
    "                                                 num_attn_heads * self.attn_head_size)\n",
    "            \n",
    "            up = torch.randn(num_attn_heads, 1, self.attn_head_size)\n",
    "            self.up = Variable(up, requires_grad=True).cuda()\n",
    "            torch.nn.init.xavier_uniform_(self.up)\n",
    "            \n",
    "            uz = torch.randn(num_attn_heads, 1, self.attn_head_size)\n",
    "            self.uz = Variable(uz, requires_grad=True).cuda()\n",
    "            torch.nn.init.xavier_uniform_(self.uz)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attn_heads, self.attn_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        key_len = hidden_states.size(1)\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        if(self.with_focus_attn == True):\n",
    "            glo = torch.mean(mixed_query_layer, dim=1, keepdim=True)\n",
    "            \n",
    "            c = self.tanh(self.linear_focus_query(mixed_query_layer) + self.linear_focus_global(glo))\n",
    "            c = self.transpose_for_scores(c)\n",
    "            \n",
    "            p = c * self.up\n",
    "            p = p.sum(3).squeeze()\n",
    "            z = c * self.uz\n",
    "            z = z.sum(3).squeeze()\n",
    "            \n",
    "            P = self.sigmoid(p) * key_len\n",
    "            Z = self.sigmoid(z) * key_len\n",
    "            \n",
    "            j = torch.arange(start=0, end=key_len, dtype=P.dtype).unsqueeze(0).unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "            P = P.unsqueeze(-1)\n",
    "            Z = Z.unsqueeze(-1)\n",
    "            \n",
    "            G = -(j - P)**2 * 2 / (Z**2)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attn_head_size)\n",
    "        \n",
    "        if(self.with_focus_attn == True):\n",
    "            attention_scores = attention_scores + G\n",
    "            \n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.o_proj(context_layer)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDNN(nn.Module):\n",
    "    def __init__(self, conv_dim, checkpoint=None, hidden_size=64, num_layers=2,\n",
    "                 bidirectional=True, with_focus_attn=False):\n",
    "        super(CLDNN, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        if(conv_dim == '1d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=8, dropout_prob=0.1,\n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.lstm = nn.LSTM(8, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif(conv_dim == '2d'):\n",
    "            self.encoder = Encoder(conv_dim)\n",
    "            if checkpoint:\n",
    "                self.encoder.load_state_dict(torch.load(checkpoint))\n",
    "            self.attn = MultiHeadedAttention(num_attn_heads=4, attn_hidden_size=176, dropout_prob=0.1, \n",
    "                                             with_focus_attn=with_focus_attn)\n",
    "            self.gap = nn.AdaptiveAvgPool2d((1, 11))\n",
    "            self.lstm = nn.LSTM(11, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size*2 if bidirectional else hidden_size, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Convolution dimension not found: %s\" % (conv_dim))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if(self.conv_dim == '1d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 40, 100) -> (batch, 8, 1, 100)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 100) -> (batch, 8, 100)\n",
    "            out = out.permute(0, 2, 1)  # (batch, 8, 100) -> (batch, 100, 8)\n",
    "            h = out\n",
    "            out = self.attn(out)  # (batch, 100, 8) -> (batch, 100, 8)\n",
    "            out = h + out\n",
    "            out = out.permute(1, 0, 2)  # (batch, 100, 8) -> (100, batch, 8)\n",
    "            self.lstm.flatten_parameters()\n",
    "            out, _ = self.lstm(out)  # (100, batch, 8) -> (100, batch, num_directions*hidden_size)\n",
    "            out = out[-1]  # (100, batch, num_directions*hidden_size) -> (batch, num_directions*hidden_size)\n",
    "            out = self.fc(out)  # (batch, num_directions*hidden_size) -> (batch, 1)\n",
    "        elif(self.conv_dim == '2d'):\n",
    "            out = self.encoder(x)  # (batch, 1, 128, 100) -> (batch, 16, 11, 8)\n",
    "            out = out.permute(0, 3, 1, 2)  # (batch, 16, 11, 8) -> (batch, 8, 16, 11)\n",
    "            h = out\n",
    "            new_out_shape = out.size()[:2] + (out.size()[2] * out.size()[3],)\n",
    "            out = out.view(*new_out_shape)  # (batch, 8, 16, 11) -> (batch, 8, 176)\n",
    "            out = self.attn(out)  # (batch, 8, 176) -> (batch, 8, 176)\n",
    "            out = out.view(h.size())  # (batch, 8, 176) -> (batch, 8, 16, 11)\n",
    "            out = h + out\n",
    "            out = self.gap(out)  # (batch, 8, 16, 11) -> (batch, 8, 1, 11)\n",
    "            out = torch.squeeze(out, 2)  # (batch, 8, 1, 11) -> (batch, 8, 11)\n",
    "            out = out.permute(1, 0, 2)  # (batch, 8, 11) -> (8, batch, 11)\n",
    "            self.lstm.flatten_parameters()\n",
    "            out, _ = self.lstm(out)  # (8, batch, 11) -> (8, batch, num_directions*hidden_size)\n",
    "            out = out[-1]  # (8, batch, num_directions*hidden_size) -> (batch, num_directions*hidden_size)\n",
    "            out = self.fc(out)  # (batch, num_directions*hidden_size) -> (batch, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "idx = np.random.permutation(int(len(sample_datas)))  # add\n",
    "\n",
    "train_idx = idx[:int((len(sample_datas))*0.75)]  # add\n",
    "eval_idx = idx[int((len(sample_datas))*0.75):]  # add\n",
    "#noise_idx = np.arange((int(len(sample_datas)/2)), len(sample_datas))  # add\n",
    "#train_idx = np.r_[train_idx, noise_idx]  # add\n",
    "\n",
    "train_samples = list(np.array(sample_datas)[train_idx])\n",
    "eval_samples = list(np.array(sample_datas)[eval_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1228it [00:54, 22.63it/s]\n",
      "165it [00:06, 23.67it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = convert_spectrograms(train_datas, conv_dim=conv_dim, sr=16000)\n",
    "X_eval = convert_spectrograms(eval_datas, conv_dim=conv_dim, sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_list = []\n",
    "for i in range(len(train_datas)):\n",
    "    if 'noise' in train_datas[i]:\n",
    "        label_filename = train_datas[i].split('/')[-1].split('_')[0] + '.wav' + '.npy'\n",
    "    else:\n",
    "        label_filename = train_datas[i].split('/')[-1] + '.npy'\n",
    "    \n",
    "    label_filepath = os.path.join('./wav_data/pretrain/label/', label_filename)\n",
    "    label = np.load(label_filepath)\n",
    "    train_label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate(train_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_label_list = []\n",
    "for i in range(len(eval_datas)):\n",
    "    if 'noise' in eval_datas[i]:\n",
    "        label_filename = eval_datas[i].split('/')[-1].split('_')[0] + '.wav' + '.npy'\n",
    "    else:\n",
    "        label_filename = eval_datas[i].split('/')[-1] + '.npy'\n",
    "    \n",
    "    label_filepath = os.path.join('./wav_data/pretrain/label/', label_filename)\n",
    "    label = np.load(label_filepath)\n",
    "    eval_label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval = np.concatenate(eval_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((486288, 40, 50, 1), (486288,), (65340, 40, 50, 1), (65340,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multi_task == 'true'):\n",
    "    speaker = np.array(list(map(lambda x: int(x.split('/')[-1].split('-')[-1].split('.')[0].split('_')[0]), \n",
    "                                sample_datas)))  # add\n",
    "    y_gender = np.array(list(map(lambda x: 1 if x % 2 ==0 else 0, speaker)))\n",
    "\n",
    "    y_g_train = y_gender[train_idx]\n",
    "    y_g_eval = y_gender[eval_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(augmentation == 'true'):\n",
    "    X_train_flip = X_train[:, :, :, ::-1]\n",
    "    y_train_flip = y_train.copy()\n",
    "\n",
    "    X_train = np.concatenate((X_train, X_train_flip), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_train_flip), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = convert_tensor(X_train, y_train)\n",
    "X_eval, y_eval = convert_tensor(X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.unsqueeze(-1)\n",
    "y_eval = y_eval.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multi_task == 'true'):\n",
    "    _, y_g_train = convert_spectrograms(train_samples, conv_dim=conv_dim, sr=16000, labels=y_g_train)\n",
    "    _, y_g_eval = convert_spectrograms(eval_samples, conv_dim=conv_dim, sr=16000, labels=y_g_eval)\n",
    "    \n",
    "    if(augmentation == 'true'):\n",
    "        y_g_train_flip = y_g_train.copy()\n",
    "        y_g_train = np.concatenate((y_g_train, y_g_train_flip))\n",
    "    \n",
    "    y_g_train = torch.tensor(y_g_train).float()\n",
    "    y_g_eval = torch.tensor(y_g_eval).float()\n",
    "\n",
    "    y_g_train = y_g_train.unsqueeze(-1)\n",
    "    y_g_eval = y_g_eval.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([972576, 1, 40, 50]) torch.Size([972576, 1]) torch.Size([65340, 1, 40, 50]) torch.Size([65340, 1])\n"
     ]
    }
   ],
   "source": [
    "if(multi_task == 'true'):\n",
    "    print(X_train.shape, y_train.shape, y_g_train.shape, X_eval.shape, y_eval.shape, y_g_eval.shape)\n",
    "else:\n",
    "    print(X_train.shape, y_train.shape, X_eval.shape, y_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multi_task == 'true'):\n",
    "    train_ds = TensorDataset(X_train, y_train, y_g_train)\n",
    "    eval_ds = TensorDataset(X_eval, y_eval, y_g_eval)\n",
    "else:\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    eval_ds = TensorDataset(X_eval, y_eval)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "eval_dataloader = DataLoader(eval_ds, batch_size=batch_size, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multi_task == 'true'):\n",
    "    model_g = CLDNN_G(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "                        num_layers=num_layers, bidirectional=bidirectional,\n",
    "                        with_focus_attn=with_focus_attn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multi_task == 'true'):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loss_func_g = nn.BCELoss()\n",
    "    optimizer = optim.Adam(list(model.parameters()) + list(model_g.parameters()), lr=learning_rate)\n",
    "else:\n",
    "    loss_func = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_warmup == 'true'):\n",
    "    t_total = len(train_dataloader) // 1 * num_epochs\n",
    "    opt_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=t_total * 0.1, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, eval_dataloader, epochs):\n",
    "        print('Start training')\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            nb_train_steps = 0\n",
    "            correct = 0\n",
    "            num_samples = 0\n",
    "            \n",
    "            if(multi_task == 'true'):\n",
    "                for X_batch, y_batch, y_g_batch in train_dataloader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    y_g_batch = y_g_batch.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    outputs = model(X_batch)\n",
    "                    outputs_g = model_g(X_batch)\n",
    "\n",
    "                    loss_1 = loss_func(outputs, y_batch)\n",
    "                    loss_2 = loss_func_g(outputs_g, y_g_batch)\n",
    "                    loss = loss_1 + 0.8 * loss_2\n",
    "                    loss.backward(retain_graph=True)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    opt_scheduler.step()\n",
    "\n",
    "                    train_loss += loss.mean().item()\n",
    "                    nb_train_steps += 1\n",
    "\n",
    "                    outputs = softmax(outputs)\n",
    "                    outputs = torch.argmax(outputs, dim=1)\n",
    "                    correct += (outputs == y_batch).float().sum()\n",
    "                    num_samples += len(X_batch)\n",
    "\n",
    "                train_loss = train_loss / nb_train_steps\n",
    "                train_accuracy = correct / num_samples\n",
    "\n",
    "                model.eval()\n",
    "                eval_loss = 0\n",
    "                nb_eval_steps = 0\n",
    "                correct = 0\n",
    "                num_samples = 0\n",
    "\n",
    "                for X_batch, y_batch, y_g_batch in eval_dataloader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    y_g_batch = y_g_batch.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(X_batch)\n",
    "                        outputs_g = model_g(X_batch)\n",
    "\n",
    "                    tmp_eval_loss_1 = loss_func(outputs, y_batch)\n",
    "                    tmp_eval_loss_2 = loss_func_g(outputs_g, y_g_batch)\n",
    "                    tmp_eval_loss = tmp_eval_loss_1 + 0.8 * tmp_eval_loss_2\n",
    "                    eval_loss += tmp_eval_loss.mean().item()\n",
    "                    nb_eval_steps += 1\n",
    "\n",
    "                    outputs = softmax(outputs)\n",
    "                    outputs = torch.argmax(outputs, dim=1)\n",
    "                    correct += (outputs == y_batch).float().sum()\n",
    "                    num_samples += len(X_batch)\n",
    "\n",
    "                eval_loss = eval_loss / nb_eval_steps\n",
    "                eval_accuracy = correct / num_samples\n",
    "            else:\n",
    "                for X_batch, y_batch in train_dataloader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    outputs = model(X_batch)\n",
    "\n",
    "                    loss = loss_func(outputs, y_batch)\n",
    "                    loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                    opt_scheduler.step()\n",
    "\n",
    "                    train_loss += loss.mean().item()\n",
    "                    nb_train_steps += 1\n",
    "\n",
    "                    outputs = (outputs >= 0.5).float()\n",
    "                    correct += (outputs == y_batch).float().sum()\n",
    "                    num_samples += len(X_batch)\n",
    "\n",
    "                train_loss = train_loss / nb_train_steps\n",
    "                train_accuracy = correct / num_samples\n",
    "\n",
    "                model.eval()\n",
    "                eval_loss = 0\n",
    "                nb_eval_steps = 0\n",
    "                correct = 0\n",
    "                num_samples = 0\n",
    "\n",
    "                for X_batch, y_batch in eval_dataloader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(X_batch)\n",
    "\n",
    "                    tmp_eval_loss = loss_func(outputs, y_batch)\n",
    "                    eval_loss += tmp_eval_loss.mean().item()\n",
    "                    nb_eval_steps += 1\n",
    "\n",
    "                    outputs = (outputs >= 0.5).float()\n",
    "                    correct += (outputs == y_batch).float().sum()\n",
    "                    num_samples += len(X_batch)\n",
    "\n",
    "                eval_loss = eval_loss / nb_eval_steps\n",
    "                eval_accuracy = correct / num_samples\n",
    "\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "            print('epoch: {:3d},    lr={:6f},    loss={:5f},    train_acc={:5f},    eval_loss={:5f},    eval_acc={:5f}'\n",
    "                  .format(epoch+1, lr, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
    "\n",
    "            \n",
    "            if((epoch+1) % save_checkpoint_steps == 0):\n",
    "                model_checkpoint = \"%s_%s_step_%d.pt\" % ('CLDNN', conv_dim, epoch+1)\n",
    "                output_model_file = os.path.join(output_dir, model_checkpoint)\n",
    "                torch.save(model.state_dict(), output_model_file)\n",
    "                print(\"Saving checkpoint %s\" % output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "epoch:   1,    lr=0.000003,    loss=0.686816,    train_acc=0.531667,    eval_loss=0.676886,    eval_acc=0.589997\n",
      "epoch:   2,    lr=0.000007,    loss=0.674499,    train_acc=0.595937,    eval_loss=0.676570,    eval_acc=0.589997\n",
      "epoch:   3,    lr=0.000010,    loss=0.670660,    train_acc=0.596986,    eval_loss=0.662371,    eval_acc=0.602512\n",
      "epoch:   4,    lr=0.000013,    loss=0.660583,    train_acc=0.611931,    eval_loss=0.659504,    eval_acc=0.611995\n",
      "epoch:   5,    lr=0.000017,    loss=0.657842,    train_acc=0.617348,    eval_loss=0.659252,    eval_acc=0.612607\n",
      "epoch:   6,    lr=0.000020,    loss=0.656114,    train_acc=0.619829,    eval_loss=0.665010,    eval_acc=0.611290\n",
      "epoch:   7,    lr=0.000023,    loss=0.654776,    train_acc=0.621233,    eval_loss=0.660871,    eval_acc=0.613205\n",
      "epoch:   8,    lr=0.000027,    loss=0.653519,    train_acc=0.622847,    eval_loss=0.664339,    eval_acc=0.613281\n",
      "epoch:   9,    lr=0.000030,    loss=0.651852,    train_acc=0.624621,    eval_loss=0.663330,    eval_acc=0.614032\n",
      "epoch:  10,    lr=0.000033,    loss=0.650142,    train_acc=0.626418,    eval_loss=0.662064,    eval_acc=0.614017\n",
      "Saving checkpoint model_check/CLDNN_1d_step_10.pt\n",
      "epoch:  11,    lr=0.000037,    loss=0.648599,    train_acc=0.627496,    eval_loss=0.662112,    eval_acc=0.614446\n",
      "epoch:  12,    lr=0.000040,    loss=0.647283,    train_acc=0.628612,    eval_loss=0.663669,    eval_acc=0.611121\n",
      "epoch:  13,    lr=0.000043,    loss=0.646341,    train_acc=0.629453,    eval_loss=0.666685,    eval_acc=0.612347\n",
      "epoch:  14,    lr=0.000047,    loss=0.645336,    train_acc=0.630181,    eval_loss=0.665268,    eval_acc=0.609498\n",
      "epoch:  15,    lr=0.000050,    loss=0.644194,    train_acc=0.631277,    eval_loss=0.666250,    eval_acc=0.610034\n",
      "epoch:  16,    lr=0.000053,    loss=0.643149,    train_acc=0.632252,    eval_loss=0.666040,    eval_acc=0.609023\n",
      "epoch:  17,    lr=0.000057,    loss=0.641829,    train_acc=0.633592,    eval_loss=0.668781,    eval_acc=0.601440\n",
      "epoch:  18,    lr=0.000060,    loss=0.640794,    train_acc=0.634211,    eval_loss=0.666648,    eval_acc=0.607292\n",
      "epoch:  19,    lr=0.000063,    loss=0.639964,    train_acc=0.635158,    eval_loss=0.666947,    eval_acc=0.604948\n",
      "epoch:  20,    lr=0.000067,    loss=0.638843,    train_acc=0.636141,    eval_loss=0.668581,    eval_acc=0.607598\n",
      "Saving checkpoint model_check/CLDNN_1d_step_20.pt\n",
      "epoch:  21,    lr=0.000070,    loss=0.637982,    train_acc=0.637361,    eval_loss=0.670301,    eval_acc=0.607828\n",
      "epoch:  22,    lr=0.000073,    loss=0.637012,    train_acc=0.637990,    eval_loss=0.671381,    eval_acc=0.606066\n",
      "epoch:  23,    lr=0.000077,    loss=0.636169,    train_acc=0.639182,    eval_loss=0.672735,    eval_acc=0.607521\n",
      "epoch:  24,    lr=0.000080,    loss=0.635163,    train_acc=0.640820,    eval_loss=0.669325,    eval_acc=0.604228\n",
      "epoch:  25,    lr=0.000083,    loss=0.634246,    train_acc=0.641460,    eval_loss=0.667272,    eval_acc=0.608824\n",
      "epoch:  26,    lr=0.000087,    loss=0.633384,    train_acc=0.642208,    eval_loss=0.667669,    eval_acc=0.608165\n",
      "epoch:  27,    lr=0.000090,    loss=0.632849,    train_acc=0.643025,    eval_loss=0.669676,    eval_acc=0.601624\n",
      "epoch:  28,    lr=0.000093,    loss=0.631843,    train_acc=0.643989,    eval_loss=0.670237,    eval_acc=0.608808\n",
      "epoch:  29,    lr=0.000097,    loss=0.631013,    train_acc=0.644356,    eval_loss=0.671250,    eval_acc=0.603508\n",
      "epoch:  30,    lr=0.000100,    loss=0.630039,    train_acc=0.645533,    eval_loss=0.676263,    eval_acc=0.611121\n",
      "Saving checkpoint model_check/CLDNN_1d_step_30.pt\n",
      "epoch:  31,    lr=0.000100,    loss=0.629158,    train_acc=0.646184,    eval_loss=0.671246,    eval_acc=0.610248\n",
      "epoch:  32,    lr=0.000099,    loss=0.628312,    train_acc=0.647183,    eval_loss=0.674869,    eval_acc=0.606587\n",
      "epoch:  33,    lr=0.000099,    loss=0.627412,    train_acc=0.647489,    eval_loss=0.670649,    eval_acc=0.613143\n",
      "epoch:  34,    lr=0.000099,    loss=0.626789,    train_acc=0.647830,    eval_loss=0.670749,    eval_acc=0.610447\n",
      "epoch:  35,    lr=0.000098,    loss=0.625926,    train_acc=0.649086,    eval_loss=0.670698,    eval_acc=0.609972\n",
      "epoch:  36,    lr=0.000098,    loss=0.625236,    train_acc=0.649386,    eval_loss=0.676336,    eval_acc=0.608640\n",
      "epoch:  37,    lr=0.000097,    loss=0.624700,    train_acc=0.649688,    eval_loss=0.678429,    eval_acc=0.608027\n",
      "epoch:  38,    lr=0.000097,    loss=0.623986,    train_acc=0.650835,    eval_loss=0.679999,    eval_acc=0.611627\n",
      "epoch:  39,    lr=0.000097,    loss=0.623542,    train_acc=0.651319,    eval_loss=0.677141,    eval_acc=0.606449\n",
      "epoch:  40,    lr=0.000096,    loss=0.622670,    train_acc=0.651791,    eval_loss=0.677265,    eval_acc=0.606725\n",
      "Saving checkpoint model_check/CLDNN_1d_step_40.pt\n",
      "epoch:  41,    lr=0.000096,    loss=0.622189,    train_acc=0.652627,    eval_loss=0.677054,    eval_acc=0.609268\n",
      "epoch:  42,    lr=0.000096,    loss=0.621482,    train_acc=0.652845,    eval_loss=0.679800,    eval_acc=0.605331\n",
      "epoch:  43,    lr=0.000095,    loss=0.620811,    train_acc=0.653693,    eval_loss=0.679456,    eval_acc=0.600888\n",
      "epoch:  44,    lr=0.000095,    loss=0.620160,    train_acc=0.654265,    eval_loss=0.681333,    eval_acc=0.606204\n",
      "epoch:  45,    lr=0.000094,    loss=0.619533,    train_acc=0.655229,    eval_loss=0.678190,    eval_acc=0.608977\n",
      "epoch:  46,    lr=0.000094,    loss=0.618787,    train_acc=0.655674,    eval_loss=0.677843,    eval_acc=0.604412\n",
      "epoch:  47,    lr=0.000094,    loss=0.618061,    train_acc=0.656444,    eval_loss=0.678017,    eval_acc=0.608364\n",
      "epoch:  48,    lr=0.000093,    loss=0.617354,    train_acc=0.657373,    eval_loss=0.682709,    eval_acc=0.607460\n",
      "epoch:  49,    lr=0.000093,    loss=0.616553,    train_acc=0.657564,    eval_loss=0.681112,    eval_acc=0.600506\n",
      "epoch:  50,    lr=0.000093,    loss=0.616060,    train_acc=0.658547,    eval_loss=0.683671,    eval_acc=0.603968\n",
      "Saving checkpoint model_check/CLDNN_1d_step_50.pt\n",
      "epoch:  51,    lr=0.000092,    loss=0.615067,    train_acc=0.659108,    eval_loss=0.678678,    eval_acc=0.608272\n",
      "epoch:  52,    lr=0.000092,    loss=0.614066,    train_acc=0.660030,    eval_loss=0.686007,    eval_acc=0.602880\n",
      "epoch:  53,    lr=0.000091,    loss=0.613243,    train_acc=0.660755,    eval_loss=0.685958,    eval_acc=0.602328\n",
      "epoch:  54,    lr=0.000091,    loss=0.612320,    train_acc=0.661242,    eval_loss=0.687804,    eval_acc=0.603033\n",
      "epoch:  55,    lr=0.000091,    loss=0.611166,    train_acc=0.662256,    eval_loss=0.688075,    eval_acc=0.606710\n",
      "epoch:  56,    lr=0.000090,    loss=0.609998,    train_acc=0.663118,    eval_loss=0.687685,    eval_acc=0.597350\n",
      "epoch:  57,    lr=0.000090,    loss=0.609065,    train_acc=0.664200,    eval_loss=0.682722,    eval_acc=0.594133\n",
      "epoch:  58,    lr=0.000090,    loss=0.607850,    train_acc=0.665007,    eval_loss=0.689032,    eval_acc=0.599372\n",
      "epoch:  59,    lr=0.000089,    loss=0.606527,    train_acc=0.666216,    eval_loss=0.693221,    eval_acc=0.602298\n",
      "epoch:  60,    lr=0.000089,    loss=0.605298,    train_acc=0.667311,    eval_loss=0.692455,    eval_acc=0.599403\n",
      "Saving checkpoint model_check/CLDNN_1d_step_60.pt\n",
      "epoch:  61,    lr=0.000089,    loss=0.604014,    train_acc=0.668145,    eval_loss=0.688364,    eval_acc=0.600184\n",
      "epoch:  62,    lr=0.000088,    loss=0.602539,    train_acc=0.669323,    eval_loss=0.691215,    eval_acc=0.600720\n",
      "epoch:  63,    lr=0.000088,    loss=0.601108,    train_acc=0.670413,    eval_loss=0.696524,    eval_acc=0.601486\n",
      "epoch:  64,    lr=0.000087,    loss=0.599331,    train_acc=0.671858,    eval_loss=0.691511,    eval_acc=0.600444\n",
      "epoch:  65,    lr=0.000087,    loss=0.597751,    train_acc=0.673151,    eval_loss=0.698247,    eval_acc=0.595129\n",
      "epoch:  66,    lr=0.000087,    loss=0.595914,    train_acc=0.674868,    eval_loss=0.710008,    eval_acc=0.599617\n",
      "epoch:  67,    lr=0.000086,    loss=0.594257,    train_acc=0.676066,    eval_loss=0.699537,    eval_acc=0.599418\n",
      "epoch:  68,    lr=0.000086,    loss=0.592407,    train_acc=0.677464,    eval_loss=0.699169,    eval_acc=0.604136\n",
      "epoch:  69,    lr=0.000086,    loss=0.590759,    train_acc=0.679128,    eval_loss=0.707571,    eval_acc=0.599556\n",
      "epoch:  70,    lr=0.000085,    loss=0.588549,    train_acc=0.680292,    eval_loss=0.706175,    eval_acc=0.596400\n",
      "Saving checkpoint model_check/CLDNN_1d_step_70.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  71,    lr=0.000085,    loss=0.586844,    train_acc=0.682008,    eval_loss=0.707217,    eval_acc=0.598407\n",
      "epoch:  72,    lr=0.000084,    loss=0.584913,    train_acc=0.683394,    eval_loss=0.714081,    eval_acc=0.595803\n",
      "epoch:  73,    lr=0.000084,    loss=0.582976,    train_acc=0.684781,    eval_loss=0.710791,    eval_acc=0.593658\n",
      "epoch:  74,    lr=0.000084,    loss=0.580805,    train_acc=0.686373,    eval_loss=0.717950,    eval_acc=0.596094\n",
      "epoch:  75,    lr=0.000083,    loss=0.578778,    train_acc=0.688302,    eval_loss=0.725075,    eval_acc=0.592862\n",
      "epoch:  76,    lr=0.000083,    loss=0.576678,    train_acc=0.689262,    eval_loss=0.726011,    eval_acc=0.599571\n",
      "epoch:  77,    lr=0.000083,    loss=0.574377,    train_acc=0.691413,    eval_loss=0.721984,    eval_acc=0.590028\n",
      "epoch:  78,    lr=0.000082,    loss=0.571998,    train_acc=0.693084,    eval_loss=0.721935,    eval_acc=0.591360\n",
      "epoch:  79,    lr=0.000082,    loss=0.570115,    train_acc=0.694541,    eval_loss=0.730858,    eval_acc=0.598070\n",
      "epoch:  80,    lr=0.000081,    loss=0.567987,    train_acc=0.696109,    eval_loss=0.732994,    eval_acc=0.597917\n",
      "Saving checkpoint model_check/CLDNN_1d_step_80.pt\n",
      "epoch:  81,    lr=0.000081,    loss=0.565732,    train_acc=0.697711,    eval_loss=0.732853,    eval_acc=0.592004\n",
      "epoch:  82,    lr=0.000081,    loss=0.563465,    train_acc=0.699358,    eval_loss=0.740729,    eval_acc=0.595956\n",
      "epoch:  83,    lr=0.000080,    loss=0.560988,    train_acc=0.701484,    eval_loss=0.737113,    eval_acc=0.594577\n",
      "epoch:  84,    lr=0.000080,    loss=0.558395,    train_acc=0.703300,    eval_loss=0.746414,    eval_acc=0.594930\n",
      "epoch:  85,    lr=0.000080,    loss=0.556201,    train_acc=0.704733,    eval_loss=0.748773,    eval_acc=0.594593\n",
      "epoch:  86,    lr=0.000079,    loss=0.554121,    train_acc=0.706144,    eval_loss=0.750981,    eval_acc=0.586581\n",
      "epoch:  87,    lr=0.000079,    loss=0.551608,    train_acc=0.708076,    eval_loss=0.760081,    eval_acc=0.591299\n",
      "epoch:  88,    lr=0.000079,    loss=0.549340,    train_acc=0.710044,    eval_loss=0.754775,    eval_acc=0.589078\n",
      "epoch:  89,    lr=0.000078,    loss=0.547048,    train_acc=0.711214,    eval_loss=0.755561,    eval_acc=0.586903\n",
      "epoch:  90,    lr=0.000078,    loss=0.544519,    train_acc=0.712641,    eval_loss=0.773756,    eval_acc=0.591942\n",
      "Saving checkpoint model_check/CLDNN_1d_step_90.pt\n",
      "epoch:  91,    lr=0.000077,    loss=0.541991,    train_acc=0.714744,    eval_loss=0.766204,    eval_acc=0.589997\n",
      "epoch:  92,    lr=0.000077,    loss=0.539674,    train_acc=0.716388,    eval_loss=0.772779,    eval_acc=0.593061\n",
      "epoch:  93,    lr=0.000077,    loss=0.537201,    train_acc=0.717843,    eval_loss=0.783413,    eval_acc=0.590074\n",
      "epoch:  94,    lr=0.000076,    loss=0.534588,    train_acc=0.719695,    eval_loss=0.774928,    eval_acc=0.587868\n",
      "epoch:  95,    lr=0.000076,    loss=0.532594,    train_acc=0.720990,    eval_loss=0.783372,    eval_acc=0.583732\n",
      "epoch:  96,    lr=0.000076,    loss=0.530405,    train_acc=0.722658,    eval_loss=0.798447,    eval_acc=0.592096\n",
      "epoch:  97,    lr=0.000075,    loss=0.527503,    train_acc=0.724474,    eval_loss=0.783818,    eval_acc=0.583885\n",
      "epoch:  98,    lr=0.000075,    loss=0.525350,    train_acc=0.726108,    eval_loss=0.797887,    eval_acc=0.589813\n",
      "epoch:  99,    lr=0.000074,    loss=0.522475,    train_acc=0.728095,    eval_loss=0.796440,    eval_acc=0.586474\n",
      "epoch: 100,    lr=0.000074,    loss=0.520158,    train_acc=0.729272,    eval_loss=0.806586,    eval_acc=0.583961\n",
      "Saving checkpoint model_check/CLDNN_1d_step_100.pt\n",
      "epoch: 101,    lr=0.000074,    loss=0.517648,    train_acc=0.730975,    eval_loss=0.805158,    eval_acc=0.587975\n",
      "epoch: 102,    lr=0.000073,    loss=0.515059,    train_acc=0.732820,    eval_loss=0.808852,    eval_acc=0.584191\n",
      "epoch: 103,    lr=0.000073,    loss=0.512784,    train_acc=0.734543,    eval_loss=0.813975,    eval_acc=0.583318\n",
      "epoch: 104,    lr=0.000073,    loss=0.510484,    train_acc=0.735677,    eval_loss=0.810211,    eval_acc=0.587653\n",
      "epoch: 105,    lr=0.000072,    loss=0.507899,    train_acc=0.737886,    eval_loss=0.825348,    eval_acc=0.584421\n",
      "epoch: 106,    lr=0.000072,    loss=0.505508,    train_acc=0.739086,    eval_loss=0.825144,    eval_acc=0.582996\n",
      "epoch: 107,    lr=0.000071,    loss=0.503425,    train_acc=0.740556,    eval_loss=0.850708,    eval_acc=0.584252\n",
      "epoch: 108,    lr=0.000071,    loss=0.501106,    train_acc=0.741730,    eval_loss=0.845194,    eval_acc=0.586045\n",
      "epoch: 109,    lr=0.000071,    loss=0.499210,    train_acc=0.743170,    eval_loss=0.841549,    eval_acc=0.587561\n",
      "epoch: 110,    lr=0.000070,    loss=0.496543,    train_acc=0.745329,    eval_loss=0.847865,    eval_acc=0.579305\n",
      "Saving checkpoint model_check/CLDNN_1d_step_110.pt\n",
      "epoch: 111,    lr=0.000070,    loss=0.494363,    train_acc=0.746377,    eval_loss=0.843297,    eval_acc=0.587332\n",
      "epoch: 112,    lr=0.000070,    loss=0.491969,    train_acc=0.747616,    eval_loss=0.861159,    eval_acc=0.581526\n",
      "epoch: 113,    lr=0.000069,    loss=0.489502,    train_acc=0.749550,    eval_loss=0.856002,    eval_acc=0.584896\n",
      "epoch: 114,    lr=0.000069,    loss=0.488245,    train_acc=0.750712,    eval_loss=0.877458,    eval_acc=0.583364\n",
      "epoch: 115,    lr=0.000069,    loss=0.485026,    train_acc=0.752368,    eval_loss=0.871003,    eval_acc=0.581817\n",
      "epoch: 116,    lr=0.000068,    loss=0.482572,    train_acc=0.754090,    eval_loss=0.884768,    eval_acc=0.581281\n",
      "epoch: 117,    lr=0.000068,    loss=0.480983,    train_acc=0.754779,    eval_loss=0.879026,    eval_acc=0.580637\n",
      "epoch: 118,    lr=0.000067,    loss=0.478821,    train_acc=0.756418,    eval_loss=0.889002,    eval_acc=0.586106\n",
      "epoch: 119,    lr=0.000067,    loss=0.477161,    train_acc=0.757403,    eval_loss=0.893379,    eval_acc=0.583686\n",
      "epoch: 120,    lr=0.000067,    loss=0.473925,    train_acc=0.759361,    eval_loss=0.896014,    eval_acc=0.577083\n",
      "Saving checkpoint model_check/CLDNN_1d_step_120.pt\n",
      "epoch: 121,    lr=0.000066,    loss=0.472493,    train_acc=0.760686,    eval_loss=0.905383,    eval_acc=0.581357\n",
      "epoch: 122,    lr=0.000066,    loss=0.469811,    train_acc=0.762080,    eval_loss=0.909607,    eval_acc=0.583272\n",
      "epoch: 123,    lr=0.000066,    loss=0.467355,    train_acc=0.763619,    eval_loss=0.900105,    eval_acc=0.582246\n",
      "epoch: 124,    lr=0.000065,    loss=0.465616,    train_acc=0.764818,    eval_loss=0.917003,    eval_acc=0.573667\n",
      "epoch: 125,    lr=0.000065,    loss=0.463770,    train_acc=0.765875,    eval_loss=0.924172,    eval_acc=0.579412\n",
      "epoch: 126,    lr=0.000064,    loss=0.461729,    train_acc=0.766954,    eval_loss=0.941399,    eval_acc=0.582292\n",
      "epoch: 127,    lr=0.000064,    loss=0.460280,    train_acc=0.768238,    eval_loss=0.937963,    eval_acc=0.581510\n",
      "epoch: 128,    lr=0.000064,    loss=0.457226,    train_acc=0.769804,    eval_loss=0.931249,    eval_acc=0.578983\n",
      "epoch: 129,    lr=0.000063,    loss=0.455303,    train_acc=0.771373,    eval_loss=0.945510,    eval_acc=0.582904\n",
      "epoch: 130,    lr=0.000063,    loss=0.453337,    train_acc=0.772407,    eval_loss=0.951742,    eval_acc=0.584758\n",
      "Saving checkpoint model_check/CLDNN_1d_step_130.pt\n",
      "epoch: 131,    lr=0.000063,    loss=0.451628,    train_acc=0.773450,    eval_loss=0.964539,    eval_acc=0.577635\n",
      "epoch: 132,    lr=0.000062,    loss=0.449203,    train_acc=0.774853,    eval_loss=0.950257,    eval_acc=0.581265\n",
      "epoch: 133,    lr=0.000062,    loss=0.447860,    train_acc=0.775871,    eval_loss=0.970180,    eval_acc=0.576379\n",
      "epoch: 134,    lr=0.000061,    loss=0.445195,    train_acc=0.777122,    eval_loss=0.986596,    eval_acc=0.580806\n",
      "epoch: 135,    lr=0.000061,    loss=0.443507,    train_acc=0.778293,    eval_loss=0.968331,    eval_acc=0.574786\n",
      "epoch: 136,    lr=0.000061,    loss=0.441622,    train_acc=0.779571,    eval_loss=0.988964,    eval_acc=0.581265\n",
      "epoch: 137,    lr=0.000060,    loss=0.439473,    train_acc=0.781077,    eval_loss=0.968843,    eval_acc=0.580040\n",
      "epoch: 138,    lr=0.000060,    loss=0.437472,    train_acc=0.782224,    eval_loss=0.994955,    eval_acc=0.579703\n",
      "epoch: 139,    lr=0.000060,    loss=0.435255,    train_acc=0.783331,    eval_loss=0.990064,    eval_acc=0.576976\n",
      "epoch: 140,    lr=0.000059,    loss=0.434078,    train_acc=0.784293,    eval_loss=0.989413,    eval_acc=0.575414\n",
      "Saving checkpoint model_check/CLDNN_1d_step_140.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 141,    lr=0.000059,    loss=0.432113,    train_acc=0.785074,    eval_loss=0.996871,    eval_acc=0.580668\n",
      "epoch: 142,    lr=0.000059,    loss=0.430496,    train_acc=0.786387,    eval_loss=1.013242,    eval_acc=0.576900\n",
      "epoch: 143,    lr=0.000058,    loss=0.428674,    train_acc=0.787482,    eval_loss=1.021653,    eval_acc=0.578998\n",
      "epoch: 144,    lr=0.000058,    loss=0.426369,    train_acc=0.788909,    eval_loss=1.020313,    eval_acc=0.576961\n",
      "epoch: 145,    lr=0.000057,    loss=0.425496,    train_acc=0.789375,    eval_loss=1.026765,    eval_acc=0.578983\n",
      "epoch: 146,    lr=0.000057,    loss=0.422416,    train_acc=0.790417,    eval_loss=1.017006,    eval_acc=0.573223\n",
      "epoch: 147,    lr=0.000057,    loss=0.421018,    train_acc=0.791982,    eval_loss=1.027991,    eval_acc=0.576900\n",
      "epoch: 148,    lr=0.000056,    loss=0.419246,    train_acc=0.792941,    eval_loss=1.041621,    eval_acc=0.578431\n",
      "epoch: 149,    lr=0.000056,    loss=0.417550,    train_acc=0.793838,    eval_loss=1.044054,    eval_acc=0.578263\n",
      "epoch: 150,    lr=0.000056,    loss=0.416591,    train_acc=0.794930,    eval_loss=1.044079,    eval_acc=0.581863\n",
      "Saving checkpoint model_check/CLDNN_1d_step_150.pt\n",
      "epoch: 151,    lr=0.000055,    loss=0.414776,    train_acc=0.795509,    eval_loss=1.055973,    eval_acc=0.579534\n",
      "epoch: 152,    lr=0.000055,    loss=0.412655,    train_acc=0.796798,    eval_loss=1.067643,    eval_acc=0.577420\n",
      "epoch: 153,    lr=0.000054,    loss=0.410649,    train_acc=0.797958,    eval_loss=1.057386,    eval_acc=0.577788\n",
      "epoch: 154,    lr=0.000054,    loss=0.410002,    train_acc=0.798378,    eval_loss=1.064297,    eval_acc=0.583241\n",
      "epoch: 155,    lr=0.000054,    loss=0.407838,    train_acc=0.800132,    eval_loss=1.074910,    eval_acc=0.579856\n",
      "epoch: 156,    lr=0.000053,    loss=0.406794,    train_acc=0.800265,    eval_loss=1.081007,    eval_acc=0.576854\n",
      "epoch: 157,    lr=0.000053,    loss=0.404226,    train_acc=0.801755,    eval_loss=1.094467,    eval_acc=0.577313\n",
      "epoch: 158,    lr=0.000053,    loss=0.402344,    train_acc=0.802689,    eval_loss=1.084495,    eval_acc=0.581434\n",
      "epoch: 159,    lr=0.000052,    loss=0.401127,    train_acc=0.803730,    eval_loss=1.085879,    eval_acc=0.578477\n",
      "epoch: 160,    lr=0.000052,    loss=0.399893,    train_acc=0.804429,    eval_loss=1.098835,    eval_acc=0.578814\n",
      "Saving checkpoint model_check/CLDNN_1d_step_160.pt\n",
      "epoch: 161,    lr=0.000051,    loss=0.397609,    train_acc=0.805464,    eval_loss=1.121954,    eval_acc=0.578340\n",
      "epoch: 162,    lr=0.000051,    loss=0.397369,    train_acc=0.805752,    eval_loss=1.133995,    eval_acc=0.574142\n",
      "epoch: 163,    lr=0.000051,    loss=0.395497,    train_acc=0.807074,    eval_loss=1.112207,    eval_acc=0.572580\n",
      "epoch: 164,    lr=0.000050,    loss=0.393602,    train_acc=0.807981,    eval_loss=1.122782,    eval_acc=0.576639\n",
      "epoch: 165,    lr=0.000050,    loss=0.392496,    train_acc=0.808627,    eval_loss=1.136936,    eval_acc=0.578768\n",
      "epoch: 166,    lr=0.000050,    loss=0.391303,    train_acc=0.809153,    eval_loss=1.137751,    eval_acc=0.580254\n",
      "epoch: 167,    lr=0.000049,    loss=0.388858,    train_acc=0.810436,    eval_loss=1.136223,    eval_acc=0.574939\n",
      "epoch: 168,    lr=0.000049,    loss=0.387707,    train_acc=0.811160,    eval_loss=1.135934,    eval_acc=0.581036\n",
      "epoch: 169,    lr=0.000049,    loss=0.386963,    train_acc=0.812063,    eval_loss=1.132361,    eval_acc=0.580009\n",
      "epoch: 170,    lr=0.000048,    loss=0.385613,    train_acc=0.812703,    eval_loss=1.139358,    eval_acc=0.578738\n",
      "Saving checkpoint model_check/CLDNN_1d_step_170.pt\n",
      "epoch: 171,    lr=0.000048,    loss=0.383959,    train_acc=0.813422,    eval_loss=1.150767,    eval_acc=0.572350\n",
      "epoch: 172,    lr=0.000047,    loss=0.382391,    train_acc=0.814226,    eval_loss=1.169288,    eval_acc=0.571033\n",
      "epoch: 173,    lr=0.000047,    loss=0.379948,    train_acc=0.815676,    eval_loss=1.164404,    eval_acc=0.570925\n",
      "epoch: 174,    lr=0.000047,    loss=0.379304,    train_acc=0.816161,    eval_loss=1.176910,    eval_acc=0.577528\n",
      "epoch: 175,    lr=0.000046,    loss=0.378511,    train_acc=0.816425,    eval_loss=1.176664,    eval_acc=0.573269\n",
      "epoch: 176,    lr=0.000046,    loss=0.376814,    train_acc=0.817371,    eval_loss=1.196422,    eval_acc=0.578003\n",
      "epoch: 177,    lr=0.000046,    loss=0.375570,    train_acc=0.817886,    eval_loss=1.179731,    eval_acc=0.577650\n",
      "epoch: 178,    lr=0.000045,    loss=0.374055,    train_acc=0.818985,    eval_loss=1.189161,    eval_acc=0.573407\n",
      "epoch: 179,    lr=0.000045,    loss=0.372018,    train_acc=0.820111,    eval_loss=1.203751,    eval_acc=0.573820\n",
      "epoch: 180,    lr=0.000044,    loss=0.371634,    train_acc=0.820179,    eval_loss=1.190834,    eval_acc=0.572825\n",
      "Saving checkpoint model_check/CLDNN_1d_step_180.pt\n",
      "epoch: 181,    lr=0.000044,    loss=0.369483,    train_acc=0.821530,    eval_loss=1.221783,    eval_acc=0.574387\n",
      "epoch: 182,    lr=0.000044,    loss=0.368820,    train_acc=0.821830,    eval_loss=1.221680,    eval_acc=0.573759\n",
      "epoch: 183,    lr=0.000043,    loss=0.367789,    train_acc=0.822372,    eval_loss=1.223448,    eval_acc=0.574908\n",
      "epoch: 184,    lr=0.000043,    loss=0.367381,    train_acc=0.822615,    eval_loss=1.238574,    eval_acc=0.576379\n",
      "epoch: 185,    lr=0.000043,    loss=0.364714,    train_acc=0.824074,    eval_loss=1.235775,    eval_acc=0.574694\n",
      "epoch: 186,    lr=0.000042,    loss=0.363309,    train_acc=0.825162,    eval_loss=1.252988,    eval_acc=0.578707\n",
      "epoch: 187,    lr=0.000042,    loss=0.362731,    train_acc=0.825078,    eval_loss=1.248993,    eval_acc=0.578431\n",
      "epoch: 188,    lr=0.000041,    loss=0.360996,    train_acc=0.826164,    eval_loss=1.254105,    eval_acc=0.576869\n",
      "epoch: 189,    lr=0.000041,    loss=0.360624,    train_acc=0.826325,    eval_loss=1.252732,    eval_acc=0.572335\n",
      "epoch: 190,    lr=0.000041,    loss=0.359215,    train_acc=0.827184,    eval_loss=1.257995,    eval_acc=0.575475\n",
      "Saving checkpoint model_check/CLDNN_1d_step_190.pt\n",
      "epoch: 191,    lr=0.000040,    loss=0.358297,    train_acc=0.827696,    eval_loss=1.252278,    eval_acc=0.571262\n",
      "epoch: 192,    lr=0.000040,    loss=0.357009,    train_acc=0.828642,    eval_loss=1.261183,    eval_acc=0.577175\n",
      "epoch: 193,    lr=0.000040,    loss=0.355486,    train_acc=0.829329,    eval_loss=1.267859,    eval_acc=0.574556\n",
      "epoch: 194,    lr=0.000039,    loss=0.354005,    train_acc=0.829702,    eval_loss=1.278242,    eval_acc=0.572212\n",
      "epoch: 195,    lr=0.000039,    loss=0.353205,    train_acc=0.830373,    eval_loss=1.276571,    eval_acc=0.575291\n",
      "epoch: 196,    lr=0.000039,    loss=0.352747,    train_acc=0.831098,    eval_loss=1.285717,    eval_acc=0.570987\n",
      "epoch: 197,    lr=0.000038,    loss=0.351049,    train_acc=0.831957,    eval_loss=1.289425,    eval_acc=0.573820\n",
      "epoch: 198,    lr=0.000038,    loss=0.350925,    train_acc=0.831672,    eval_loss=1.321061,    eval_acc=0.572151\n",
      "epoch: 199,    lr=0.000037,    loss=0.349118,    train_acc=0.832925,    eval_loss=1.331314,    eval_acc=0.570741\n",
      "epoch: 200,    lr=0.000037,    loss=0.348223,    train_acc=0.833277,    eval_loss=1.308611,    eval_acc=0.574510\n",
      "Saving checkpoint model_check/CLDNN_1d_step_200.pt\n",
      "epoch: 201,    lr=0.000037,    loss=0.347572,    train_acc=0.833768,    eval_loss=1.305749,    eval_acc=0.575046\n",
      "epoch: 202,    lr=0.000036,    loss=0.346247,    train_acc=0.834434,    eval_loss=1.326515,    eval_acc=0.573790\n",
      "epoch: 203,    lr=0.000036,    loss=0.345701,    train_acc=0.834781,    eval_loss=1.324353,    eval_acc=0.570941\n",
      "epoch: 204,    lr=0.000036,    loss=0.344508,    train_acc=0.834958,    eval_loss=1.346116,    eval_acc=0.573790\n",
      "epoch: 205,    lr=0.000035,    loss=0.343198,    train_acc=0.835871,    eval_loss=1.319964,    eval_acc=0.570619\n",
      "epoch: 206,    lr=0.000035,    loss=0.342001,    train_acc=0.836612,    eval_loss=1.331189,    eval_acc=0.575352\n",
      "epoch: 207,    lr=0.000034,    loss=0.341403,    train_acc=0.836597,    eval_loss=1.338408,    eval_acc=0.572963\n",
      "epoch: 208,    lr=0.000034,    loss=0.340510,    train_acc=0.837193,    eval_loss=1.353619,    eval_acc=0.573422\n",
      "epoch: 209,    lr=0.000034,    loss=0.338986,    train_acc=0.838053,    eval_loss=1.353255,    eval_acc=0.574969\n",
      "epoch: 210,    lr=0.000033,    loss=0.338914,    train_acc=0.838314,    eval_loss=1.345051,    eval_acc=0.567325\n",
      "Saving checkpoint model_check/CLDNN_1d_step_210.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 211,    lr=0.000033,    loss=0.337742,    train_acc=0.838517,    eval_loss=1.391519,    eval_acc=0.570757\n",
      "epoch: 212,    lr=0.000033,    loss=0.336680,    train_acc=0.839244,    eval_loss=1.381087,    eval_acc=0.571232\n",
      "epoch: 213,    lr=0.000032,    loss=0.335117,    train_acc=0.840225,    eval_loss=1.376592,    eval_acc=0.574219\n",
      "epoch: 214,    lr=0.000032,    loss=0.334610,    train_acc=0.840329,    eval_loss=1.371699,    eval_acc=0.569792\n",
      "epoch: 215,    lr=0.000031,    loss=0.334008,    train_acc=0.840564,    eval_loss=1.382538,    eval_acc=0.571324\n",
      "epoch: 216,    lr=0.000031,    loss=0.332918,    train_acc=0.841209,    eval_loss=1.388898,    eval_acc=0.570267\n",
      "epoch: 217,    lr=0.000031,    loss=0.331756,    train_acc=0.842185,    eval_loss=1.392808,    eval_acc=0.569455\n",
      "epoch: 218,    lr=0.000030,    loss=0.331379,    train_acc=0.842206,    eval_loss=1.378594,    eval_acc=0.569930\n",
      "epoch: 219,    lr=0.000030,    loss=0.330711,    train_acc=0.842372,    eval_loss=1.396689,    eval_acc=0.575153\n",
      "epoch: 220,    lr=0.000030,    loss=0.329474,    train_acc=0.843446,    eval_loss=1.418260,    eval_acc=0.569501\n",
      "Saving checkpoint model_check/CLDNN_1d_step_220.pt\n",
      "epoch: 221,    lr=0.000029,    loss=0.329004,    train_acc=0.843512,    eval_loss=1.407784,    eval_acc=0.576547\n",
      "epoch: 222,    lr=0.000029,    loss=0.327434,    train_acc=0.844085,    eval_loss=1.390270,    eval_acc=0.574372\n",
      "epoch: 223,    lr=0.000029,    loss=0.326681,    train_acc=0.844352,    eval_loss=1.407400,    eval_acc=0.567525\n",
      "epoch: 224,    lr=0.000028,    loss=0.325824,    train_acc=0.845027,    eval_loss=1.416097,    eval_acc=0.573284\n",
      "epoch: 225,    lr=0.000028,    loss=0.325779,    train_acc=0.845002,    eval_loss=1.437146,    eval_acc=0.572748\n",
      "epoch: 226,    lr=0.000027,    loss=0.324663,    train_acc=0.845672,    eval_loss=1.414963,    eval_acc=0.576792\n",
      "epoch: 227,    lr=0.000027,    loss=0.323865,    train_acc=0.846018,    eval_loss=1.424076,    eval_acc=0.572855\n",
      "epoch: 228,    lr=0.000027,    loss=0.322836,    train_acc=0.846329,    eval_loss=1.448646,    eval_acc=0.573315\n",
      "epoch: 229,    lr=0.000026,    loss=0.322175,    train_acc=0.847181,    eval_loss=1.460737,    eval_acc=0.568765\n",
      "epoch: 230,    lr=0.000026,    loss=0.321361,    train_acc=0.847268,    eval_loss=1.431617,    eval_acc=0.569853\n",
      "Saving checkpoint model_check/CLDNN_1d_step_230.pt\n",
      "epoch: 231,    lr=0.000026,    loss=0.320678,    train_acc=0.847822,    eval_loss=1.471365,    eval_acc=0.569378\n",
      "epoch: 232,    lr=0.000025,    loss=0.320321,    train_acc=0.847841,    eval_loss=1.459687,    eval_acc=0.570680\n",
      "epoch: 233,    lr=0.000025,    loss=0.318816,    train_acc=0.848742,    eval_loss=1.465599,    eval_acc=0.569409\n",
      "epoch: 234,    lr=0.000024,    loss=0.318587,    train_acc=0.848827,    eval_loss=1.473187,    eval_acc=0.570404\n",
      "epoch: 235,    lr=0.000024,    loss=0.317797,    train_acc=0.849228,    eval_loss=1.489413,    eval_acc=0.574908\n",
      "epoch: 236,    lr=0.000024,    loss=0.316995,    train_acc=0.849633,    eval_loss=1.496921,    eval_acc=0.573621\n",
      "epoch: 237,    lr=0.000023,    loss=0.316078,    train_acc=0.849817,    eval_loss=1.492383,    eval_acc=0.570190\n",
      "epoch: 238,    lr=0.000023,    loss=0.316200,    train_acc=0.849823,    eval_loss=1.468376,    eval_acc=0.571354\n",
      "epoch: 239,    lr=0.000023,    loss=0.315295,    train_acc=0.850413,    eval_loss=1.481574,    eval_acc=0.569470\n",
      "epoch: 240,    lr=0.000022,    loss=0.314105,    train_acc=0.851164,    eval_loss=1.520669,    eval_acc=0.568980\n",
      "Saving checkpoint model_check/CLDNN_1d_step_240.pt\n",
      "epoch: 241,    lr=0.000022,    loss=0.314219,    train_acc=0.850840,    eval_loss=1.477961,    eval_acc=0.570006\n",
      "epoch: 242,    lr=0.000021,    loss=0.312525,    train_acc=0.851869,    eval_loss=1.506087,    eval_acc=0.572472\n",
      "epoch: 243,    lr=0.000021,    loss=0.312958,    train_acc=0.851806,    eval_loss=1.501616,    eval_acc=0.569868\n",
      "epoch: 244,    lr=0.000021,    loss=0.311474,    train_acc=0.852378,    eval_loss=1.500692,    eval_acc=0.570190\n",
      "epoch: 245,    lr=0.000020,    loss=0.311667,    train_acc=0.852757,    eval_loss=1.511661,    eval_acc=0.572794\n",
      "epoch: 246,    lr=0.000020,    loss=0.309926,    train_acc=0.852987,    eval_loss=1.505881,    eval_acc=0.567708\n",
      "epoch: 247,    lr=0.000020,    loss=0.310098,    train_acc=0.853037,    eval_loss=1.516137,    eval_acc=0.569485\n",
      "epoch: 248,    lr=0.000019,    loss=0.309039,    train_acc=0.853659,    eval_loss=1.531646,    eval_acc=0.574112\n",
      "epoch: 249,    lr=0.000019,    loss=0.308071,    train_acc=0.854076,    eval_loss=1.545332,    eval_acc=0.567999\n",
      "epoch: 250,    lr=0.000019,    loss=0.307279,    train_acc=0.854688,    eval_loss=1.534617,    eval_acc=0.568061\n",
      "Saving checkpoint model_check/CLDNN_1d_step_250.pt\n",
      "epoch: 251,    lr=0.000018,    loss=0.306794,    train_acc=0.854813,    eval_loss=1.545375,    eval_acc=0.571492\n",
      "epoch: 252,    lr=0.000018,    loss=0.307484,    train_acc=0.854544,    eval_loss=1.542520,    eval_acc=0.570006\n",
      "epoch: 253,    lr=0.000017,    loss=0.306330,    train_acc=0.855094,    eval_loss=1.537294,    eval_acc=0.569730\n",
      "epoch: 254,    lr=0.000017,    loss=0.306321,    train_acc=0.855282,    eval_loss=1.518088,    eval_acc=0.569868\n",
      "epoch: 255,    lr=0.000017,    loss=0.304936,    train_acc=0.855824,    eval_loss=1.545189,    eval_acc=0.569577\n",
      "epoch: 256,    lr=0.000016,    loss=0.304730,    train_acc=0.855973,    eval_loss=1.553807,    eval_acc=0.570328\n",
      "epoch: 257,    lr=0.000016,    loss=0.303867,    train_acc=0.856083,    eval_loss=1.577941,    eval_acc=0.570987\n",
      "epoch: 258,    lr=0.000016,    loss=0.303477,    train_acc=0.856641,    eval_loss=1.580135,    eval_acc=0.571523\n",
      "epoch: 259,    lr=0.000015,    loss=0.302940,    train_acc=0.856701,    eval_loss=1.568706,    eval_acc=0.571507\n",
      "epoch: 260,    lr=0.000015,    loss=0.303718,    train_acc=0.856384,    eval_loss=1.595477,    eval_acc=0.570971\n",
      "Saving checkpoint model_check/CLDNN_1d_step_260.pt\n",
      "epoch: 261,    lr=0.000014,    loss=0.302570,    train_acc=0.857088,    eval_loss=1.572349,    eval_acc=0.569102\n",
      "epoch: 262,    lr=0.000014,    loss=0.302150,    train_acc=0.857360,    eval_loss=1.570111,    eval_acc=0.568903\n",
      "epoch: 263,    lr=0.000014,    loss=0.300886,    train_acc=0.857941,    eval_loss=1.579403,    eval_acc=0.570588\n",
      "epoch: 264,    lr=0.000013,    loss=0.300782,    train_acc=0.857791,    eval_loss=1.587242,    eval_acc=0.571293\n",
      "epoch: 265,    lr=0.000013,    loss=0.300149,    train_acc=0.858244,    eval_loss=1.591733,    eval_acc=0.572442\n",
      "epoch: 266,    lr=0.000013,    loss=0.299984,    train_acc=0.858443,    eval_loss=1.559566,    eval_acc=0.572825\n",
      "epoch: 267,    lr=0.000012,    loss=0.299035,    train_acc=0.858893,    eval_loss=1.627128,    eval_acc=0.567862\n",
      "epoch: 268,    lr=0.000012,    loss=0.298729,    train_acc=0.858811,    eval_loss=1.564237,    eval_acc=0.569547\n",
      "epoch: 269,    lr=0.000011,    loss=0.298571,    train_acc=0.858774,    eval_loss=1.590134,    eval_acc=0.569945\n",
      "epoch: 270,    lr=0.000011,    loss=0.298407,    train_acc=0.859077,    eval_loss=1.607690,    eval_acc=0.571033\n",
      "Saving checkpoint model_check/CLDNN_1d_step_270.pt\n",
      "epoch: 271,    lr=0.000011,    loss=0.297983,    train_acc=0.859251,    eval_loss=1.595712,    eval_acc=0.570987\n",
      "epoch: 272,    lr=0.000010,    loss=0.297399,    train_acc=0.859659,    eval_loss=1.612479,    eval_acc=0.569715\n",
      "epoch: 273,    lr=0.000010,    loss=0.297078,    train_acc=0.859523,    eval_loss=1.606285,    eval_acc=0.570374\n",
      "epoch: 274,    lr=0.000010,    loss=0.296454,    train_acc=0.860386,    eval_loss=1.625099,    eval_acc=0.570604\n",
      "epoch: 275,    lr=0.000009,    loss=0.296484,    train_acc=0.860256,    eval_loss=1.625312,    eval_acc=0.569010\n",
      "epoch: 276,    lr=0.000009,    loss=0.295695,    train_acc=0.860529,    eval_loss=1.640414,    eval_acc=0.568428\n",
      "epoch: 277,    lr=0.000009,    loss=0.295601,    train_acc=0.860482,    eval_loss=1.617810,    eval_acc=0.566805\n",
      "epoch: 278,    lr=0.000008,    loss=0.295754,    train_acc=0.860499,    eval_loss=1.630631,    eval_acc=0.571033\n",
      "epoch: 279,    lr=0.000008,    loss=0.295282,    train_acc=0.860757,    eval_loss=1.633753,    eval_acc=0.571124\n",
      "epoch: 280,    lr=0.000007,    loss=0.293514,    train_acc=0.861408,    eval_loss=1.614597,    eval_acc=0.571293\n",
      "Saving checkpoint model_check/CLDNN_1d_step_280.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 281,    lr=0.000007,    loss=0.293154,    train_acc=0.861501,    eval_loss=1.651639,    eval_acc=0.570343\n",
      "epoch: 282,    lr=0.000007,    loss=0.294121,    train_acc=0.861610,    eval_loss=1.624542,    eval_acc=0.568842\n",
      "epoch: 283,    lr=0.000006,    loss=0.293674,    train_acc=0.861502,    eval_loss=1.622840,    eval_acc=0.568045\n",
      "epoch: 284,    lr=0.000006,    loss=0.292491,    train_acc=0.861913,    eval_loss=1.627914,    eval_acc=0.570665\n",
      "epoch: 285,    lr=0.000006,    loss=0.292904,    train_acc=0.861772,    eval_loss=1.619719,    eval_acc=0.570956\n",
      "epoch: 286,    lr=0.000005,    loss=0.291677,    train_acc=0.862335,    eval_loss=1.652431,    eval_acc=0.569455\n",
      "epoch: 287,    lr=0.000005,    loss=0.291339,    train_acc=0.862586,    eval_loss=1.651491,    eval_acc=0.568183\n",
      "epoch: 288,    lr=0.000004,    loss=0.292256,    train_acc=0.862147,    eval_loss=1.635071,    eval_acc=0.570818\n",
      "epoch: 289,    lr=0.000004,    loss=0.291699,    train_acc=0.862684,    eval_loss=1.658300,    eval_acc=0.569960\n",
      "epoch: 290,    lr=0.000004,    loss=0.291114,    train_acc=0.862571,    eval_loss=1.630772,    eval_acc=0.568444\n",
      "Saving checkpoint model_check/CLDNN_1d_step_290.pt\n",
      "epoch: 291,    lr=0.000003,    loss=0.290421,    train_acc=0.862915,    eval_loss=1.667631,    eval_acc=0.567754\n",
      "epoch: 292,    lr=0.000003,    loss=0.290471,    train_acc=0.863188,    eval_loss=1.651765,    eval_acc=0.571615\n",
      "epoch: 293,    lr=0.000003,    loss=0.290909,    train_acc=0.862878,    eval_loss=1.654470,    eval_acc=0.571339\n",
      "epoch: 294,    lr=0.000002,    loss=0.290587,    train_acc=0.863008,    eval_loss=1.634694,    eval_acc=0.569684\n",
      "epoch: 295,    lr=0.000002,    loss=0.289553,    train_acc=0.863520,    eval_loss=1.652076,    eval_acc=0.571124\n",
      "epoch: 296,    lr=0.000001,    loss=0.289565,    train_acc=0.863820,    eval_loss=1.650626,    eval_acc=0.571370\n",
      "epoch: 297,    lr=0.000001,    loss=0.290220,    train_acc=0.863459,    eval_loss=1.650151,    eval_acc=0.571400\n",
      "epoch: 298,    lr=0.000001,    loss=0.289407,    train_acc=0.863760,    eval_loss=1.647450,    eval_acc=0.569914\n",
      "epoch: 299,    lr=0.000000,    loss=0.288607,    train_acc=0.863849,    eval_loss=1.661025,    eval_acc=0.569975\n",
      "epoch: 300,    lr=0.000000,    loss=0.288951,    train_acc=0.863929,    eval_loss=1.686730,    eval_acc=0.568229\n",
      "Saving checkpoint model_check/CLDNN_1d_step_300.pt\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, eval_dataloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLDNN_6: 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from data_loader import AudioInferenceDataset, two_hot_encode\n",
    "from preprocessing import preprocessing, convert_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLDNN(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1d(1, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1d(4, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1d(4, 8, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv1d(8, 8, kernel_size=(10, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (attn): MultiHeadedAttention(\n",
       "    (query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (key): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (value): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (o_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (softmax): Softmax()\n",
       "    (tanh): Tanh()\n",
       "    (sigmoid): Sigmoid()\n",
       "    (linear_focus_query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (linear_focus_global): Linear(in_features=8, out_features=8, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(8, 128, num_layers=2, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)\n",
    "model.load_state_dict(torch.load('./model/CLDNN_1d_step_300.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = AudioInferenceDataset(root_dir='./wav_data/pretrain/sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(pred):\n",
    "    num_zero = 0\n",
    "    num_one = 0\n",
    "    start = 0\n",
    "    end = 1\n",
    "    starts = []\n",
    "    ends = []\n",
    "    num_ones = []\n",
    "\n",
    "    for k, value in enumerate(pred):\n",
    "        if((value == 1) & (num_zero > 16)):\n",
    "            starts.append(start)\n",
    "            ends.append(end)\n",
    "            num_ones.append(num_one)\n",
    "            start = k\n",
    "            num_one = 0\n",
    "            num_zero = 0\n",
    "            num_one += 1\n",
    "        elif(value == 1):\n",
    "            if(start == 0):\n",
    "                start = k\n",
    "            num_zero = 0\n",
    "            num_one += 1\n",
    "            end = k if(k != start) else start+1\n",
    "        elif(value == 0):\n",
    "            num_zero += 1\n",
    "    starts.append(start)\n",
    "    ends.append(end)\n",
    "    num_ones.append(num_one)\n",
    "    \n",
    "    if(len(starts) == 1):\n",
    "        start = 0\n",
    "        end = 0\n",
    "    else:\n",
    "        idx = np.argmax(np.array([num_ones[i]**2/(ends[i]-starts[i]) for i in range(len(starts))]))\n",
    "        start = round(starts[idx] * 0.1)\n",
    "        end = np.ceil(ends[idx] * 0.1).astype('int')\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model) -> pd.DataFrame:\n",
    "    # testset = AudioInferenceDataset(root_dir=f'{test_path}/test_data')\n",
    "    testset = AudioInferenceDataset(root_dir='./wav_data/pretrain/sample')\n",
    "\n",
    "    test_loader = DataLoader(dataset=testset, batch_size=64, shuffle=False)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    y_start = []\n",
    "    y_end = []\n",
    "    filename_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            x = data['audio']\n",
    "            x = x.to(device)\n",
    "            starts = []\n",
    "            ends = []\n",
    "            for j in range(len(x)):\n",
    "                pred = model(x[j])\n",
    "                pred = pred.detach().cpu().squeeze().numpy()\n",
    "                pred = (pred >= 0.5).astype('float32')\n",
    "                start, end = calculate(pred)\n",
    "                starts.append(start)\n",
    "                ends.append(end)\n",
    "            \n",
    "            filename_list += data['file_name']\n",
    "            y_start += starts\n",
    "            y_end += ends\n",
    "            \n",
    "        '''\n",
    "        for i, data in enumerate(test_loader):\n",
    "            x = data['audio']\n",
    "            x = x.to(device)\n",
    "            _, pred = model(x)\n",
    "\n",
    "            filename_list += data['file_name']\n",
    "            y_start += pred.detach().cpu()[:, 0].squeeze().tolist()\n",
    "            y_end += pred.detach().cpu()[:, 1].squeeze().tolist()\n",
    "        '''\n",
    "\n",
    "    ret = pd.DataFrame({'file_name': filename_list, 'start': y_start, 'end': y_end})\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.wav</td>\n",
       "      <td>16</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.wav</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.wav</td>\n",
       "      <td>21</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.wav</td>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>924.wav</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>925.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>926.wav</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>927.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>930.wav</td>\n",
       "      <td>29</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>903 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_name  start  end\n",
       "0     122.wav      0   20\n",
       "1       1.wav     16   39\n",
       "2       2.wav     22   30\n",
       "3       3.wav     21   40\n",
       "4       4.wav     27   33\n",
       "..        ...    ...  ...\n",
       "898   924.wav     20   40\n",
       "899   925.wav      0    0\n",
       "900   926.wav     13   40\n",
       "901   927.wav      0   17\n",
       "902   930.wav     29   40\n",
       "\n",
       "[903 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLDNN(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1d(1, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1d(4, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1d(4, 8, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv1d(8, 8, kernel_size=(10, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (attn): MultiHeadedAttention(\n",
       "    (query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (key): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (value): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (o_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (softmax): Softmax()\n",
       "    (tanh): Tanh()\n",
       "    (sigmoid): Sigmoid()\n",
       "    (linear_focus_query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (linear_focus_global): Linear(in_features=8, out_features=8, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(8, 128, num_layers=2, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)\n",
    "model_1.load_state_dict(torch.load('./ensemble/CLDNN_1.pt'))\n",
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLDNN(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1d(1, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1d(4, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1d(4, 8, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv1d(8, 8, kernel_size=(10, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (attn): MultiHeadedAttention(\n",
       "    (query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (key): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (value): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (o_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (softmax): Softmax()\n",
       "    (tanh): Tanh()\n",
       "    (sigmoid): Sigmoid()\n",
       "    (linear_focus_query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (linear_focus_global): Linear(in_features=8, out_features=8, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(8, 128, num_layers=2, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)\n",
    "model_2.load_state_dict(torch.load('./ensemble/CLDNN_2.pt'))\n",
    "model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLDNN(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1d(1, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1d(4, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1d(4, 8, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv1d(8, 8, kernel_size=(10, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (attn): MultiHeadedAttention(\n",
       "    (query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (key): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (value): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (o_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (softmax): Softmax()\n",
       "    (tanh): Tanh()\n",
       "    (sigmoid): Sigmoid()\n",
       "    (linear_focus_query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (linear_focus_global): Linear(in_features=8, out_features=8, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(8, 128, num_layers=2, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)\n",
    "model_3.load_state_dict(torch.load('./ensemble/CLDNN_3.pt'))\n",
    "model_3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLDNN(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1d(1, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1d(4, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1d(4, 8, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv1d(8, 8, kernel_size=(10, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (attn): MultiHeadedAttention(\n",
       "    (query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (key): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (value): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (o_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (softmax): Softmax()\n",
       "    (tanh): Tanh()\n",
       "    (sigmoid): Sigmoid()\n",
       "    (linear_focus_query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (linear_focus_global): Linear(in_features=8, out_features=8, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(8, 128, num_layers=2, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)\n",
    "model_1.load_state_dict(torch.load('./model/CLDNN_1d_step_200_1.pt'))\n",
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLDNN(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1d(1, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1d(4, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1d(4, 8, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv1d(8, 8, kernel_size=(10, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (attn): MultiHeadedAttention(\n",
       "    (query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (key): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (value): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (o_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "  (lstm): LSTM(8, 64, num_layers=2, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=64,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=False).to(device)\n",
    "model_2.load_state_dict(torch.load('./model/VAD_model.pt'))\n",
    "model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_1': model_1.state_dict(),\n",
    "    'model_2': model_2.state_dict(),\n",
    "    #'model_3': model_3.state_dict(),\n",
    "    #'model_4': model_4.state_dict(),\n",
    "    #'model_5': model_5.state_dict(),\n",
    "    #'model_6': model.state_dict(),\n",
    "}, './model/model_ensemble.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)\n",
    "model_2 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)\n",
    "model_3 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)\n",
    "model_4 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)\n",
    "model_5 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)\n",
    "model_6 = CLDNN(conv_dim=conv_dim, checkpoint=checkpoint, hidden_size=hidden_size,\n",
    "              num_layers=num_layers, bidirectional=bidirectional,\n",
    "              with_focus_attn=with_focus_attn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLDNN(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1d(1, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1d(4, 4, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1d(4, 8, kernel_size=(11, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv1d(8, 8, kernel_size=(10, 1), stride=(1,))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (attn): MultiHeadedAttention(\n",
       "    (query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (key): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (value): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (o_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (softmax): Softmax()\n",
       "    (tanh): Tanh()\n",
       "    (sigmoid): Sigmoid()\n",
       "    (linear_focus_query): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (linear_focus_global): Linear(in_features=8, out_features=8, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(8, 128, num_layers=2, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.load_state_dict(torch.load('./model_all_data/model_ensemble.pt')['model_1'])\n",
    "model_2.load_state_dict(torch.load('./model_all_data/model_ensemble.pt')['model_2'])\n",
    "model_3.load_state_dict(torch.load('./model_all_data/model_ensemble.pt')['model_3'])\n",
    "model_4.load_state_dict(torch.load('./model_all_data/model_ensemble.pt')['model_4'])\n",
    "model_5.load_state_dict(torch.load('./model_all_data/model_ensemble.pt')['model_5'])\n",
    "model_6.load_state_dict(torch.load('./model_all_data/model_ensemble.pt')['model_6'])\n",
    "\n",
    "model_1.eval()\n",
    "model_2.eval()\n",
    "model_3.eval()\n",
    "model_4.eval()\n",
    "model_5.eval()\n",
    "model_6.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0905],\n",
      "          [-0.0448],\n",
      "          [-0.0492],\n",
      "          [-0.1189],\n",
      "          [-0.1578],\n",
      "          [-0.1203],\n",
      "          [ 0.0539],\n",
      "          [ 0.6332],\n",
      "          [ 1.1040],\n",
      "          [ 1.2801],\n",
      "          [ 0.8445]]],\n",
      "\n",
      "\n",
      "        [[[-0.8201],\n",
      "          [ 0.2812],\n",
      "          [-0.0367],\n",
      "          [ 0.3458],\n",
      "          [-0.1257],\n",
      "          [ 0.1367],\n",
      "          [-0.2569],\n",
      "          [-0.0129],\n",
      "          [-0.0209],\n",
      "          [ 0.2502],\n",
      "          [-0.0149]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6740],\n",
      "          [-0.3954],\n",
      "          [ 0.0535],\n",
      "          [-0.2752],\n",
      "          [-0.0772],\n",
      "          [-0.0634],\n",
      "          [ 0.1458],\n",
      "          [-0.0594],\n",
      "          [-0.0542],\n",
      "          [-0.4355],\n",
      "          [-0.0871]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7231],\n",
      "          [-0.3695],\n",
      "          [ 0.1311],\n",
      "          [-0.3867],\n",
      "          [-0.0832],\n",
      "          [-0.1032],\n",
      "          [-0.0713],\n",
      "          [ 0.0707],\n",
      "          [ 0.1685],\n",
      "          [ 0.1891],\n",
      "          [ 0.1545]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0467, -0.0651, -0.1023,  0.1044], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3034, 0.4641, 0.3682, 0.2555], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6379,  0.1339, -0.0762,  0.2036], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for par in model_1.encoder.conv1.parameters():\n",
    "    print(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0905],\n",
      "          [-0.0448],\n",
      "          [-0.0492],\n",
      "          [-0.1189],\n",
      "          [-0.1578],\n",
      "          [-0.1203],\n",
      "          [ 0.0539],\n",
      "          [ 0.6332],\n",
      "          [ 1.1040],\n",
      "          [ 1.2801],\n",
      "          [ 0.8445]]],\n",
      "\n",
      "\n",
      "        [[[-0.8201],\n",
      "          [ 0.2812],\n",
      "          [-0.0367],\n",
      "          [ 0.3458],\n",
      "          [-0.1257],\n",
      "          [ 0.1367],\n",
      "          [-0.2569],\n",
      "          [-0.0129],\n",
      "          [-0.0209],\n",
      "          [ 0.2502],\n",
      "          [-0.0149]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6740],\n",
      "          [-0.3954],\n",
      "          [ 0.0535],\n",
      "          [-0.2752],\n",
      "          [-0.0772],\n",
      "          [-0.0634],\n",
      "          [ 0.1458],\n",
      "          [-0.0594],\n",
      "          [-0.0542],\n",
      "          [-0.4355],\n",
      "          [-0.0871]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7231],\n",
      "          [-0.3695],\n",
      "          [ 0.1311],\n",
      "          [-0.3867],\n",
      "          [-0.0832],\n",
      "          [-0.1032],\n",
      "          [-0.0713],\n",
      "          [ 0.0707],\n",
      "          [ 0.1685],\n",
      "          [ 0.1891],\n",
      "          [ 0.1545]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0467, -0.0651, -0.1023,  0.1044], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3034, 0.4641, 0.3682, 0.2555], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6379,  0.1339, -0.0762,  0.2036], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for par in models[3].encoder.conv1.parameters():\n",
    "    print(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "models = [model_1, model_2, model_3, model_4, model_5, model_6]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    models[i].load_state_dict(torch.load('./model_all_data/model_ensemble.pt')['model_%d'%(i+1)])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = model_1(X_train[0:10].to(device))\n",
    "pred_2 = model_2(X_train[0:10].to(device))\n",
    "pred_3 = model_3(X_train[0:10].to(device))\n",
    "pred_4 = model_4(X_train[0:10].to(device))\n",
    "pred_5 = model_5(X_train[0:10].to(device))\n",
    "pred_6 = model_6(X_train[0:10].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3260],\n",
       "        [0.3904],\n",
       "        [0.9989],\n",
       "        [0.9996],\n",
       "        [0.6323],\n",
       "        [0.5515],\n",
       "        [0.4103],\n",
       "        [0.0939],\n",
       "        [0.1353],\n",
       "        [0.0649]], device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_1 + pred_2 + pred_3 + pred_4 + pred_5 + pred_6) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 150, 188, 217, 315, 367, 390]\n",
      "[110, 158, 199, 297, 349, 349, 395]\n",
      "[0, 38, 181]\n",
      "[1, 161, 395]\n",
      "[0, 196, 268]\n",
      "[1, 213, 328]\n",
      "[1, 266]\n",
      "[245, 395]\n",
      "[9, 28, 56, 126, 180]\n",
      "[10, 35, 75, 75, 395]\n",
      "[1, 314, 382]\n",
      "[296, 355, 395]\n",
      "[1, 212]\n",
      "[185, 395]\n",
      "[1, 308, 333, 363]\n",
      "[288, 309, 336, 390]\n",
      "[1, 62, 153, 183, 299, 337, 360]\n",
      "[42, 135, 164, 234, 318, 318, 373]\n",
      "[0, 34, 95, 173, 344, 391]\n",
      "[1, 60, 146, 316, 358, 395]\n",
      "[6, 52, 113, 312, 331]\n",
      "[27, 94, 284, 313, 352]\n",
      "[1, 87, 149, 168]\n",
      "[63, 127, 127, 386]\n",
      "[1, 62, 199, 310]\n",
      "[42, 181, 291, 394]\n",
      "[6, 106]\n",
      "[66, 395]\n",
      "[4, 206, 264, 388]\n",
      "[176, 237, 347, 389]\n",
      "[6, 106, 175, 209]\n",
      "[85, 132, 177, 387]\n",
      "[0, 28, 160, 195]\n",
      "[1, 44, 44, 395]\n",
      "[0, 26, 61, 84, 111, 155, 195, 330, 387]\n",
      "[1, 29, 63, 85, 134, 158, 287, 341, 391]\n",
      "[0, 50, 104]\n",
      "[1, 84, 328]\n",
      "[5, 68, 96, 124, 155, 223, 270]\n",
      "[38, 38, 38, 132, 196, 228, 395]\n",
      "[4, 46]\n",
      "[26, 395]\n",
      "[2, 241, 342, 371]\n",
      "[223, 320, 344, 395]\n",
      "[10, 77, 119, 239]\n",
      "[16, 93, 218, 395]\n",
      "[0, 36, 79]\n",
      "[1, 44, 386]\n",
      "[5, 51, 92, 216, 254, 354, 394]\n",
      "[26, 53, 195, 231, 315, 374, 395]\n",
      "[0, 31, 129, 183]\n",
      "[1, 56, 138, 259]\n",
      "[5, 60, 119, 182, 294, 326, 364]\n",
      "[38, 92, 92, 267, 304, 304, 392]\n",
      "[1, 206]\n",
      "[184, 392]\n",
      "[0, 81, 102, 162, 200, 234, 269]\n",
      "[1, 1, 120, 179, 216, 249, 371]\n",
      "[16, 122]\n",
      "[94, 395]\n",
      "[0, 30, 130, 205, 327, 356]\n",
      "[1, 1, 183, 287, 330, 357]\n",
      "[14, 120, 336]\n",
      "[15, 278, 349]\n",
      "[5, 48]\n",
      "[7, 395]\n",
      "[4, 83, 106, 313, 342]\n",
      "[60, 85, 294, 324, 395]\n",
      "[5, 62, 366]\n",
      "[6, 346, 391]\n",
      "[1, 69, 98, 140, 368]\n",
      "[46, 77, 77, 336, 336]\n",
      "[0, 28, 59, 144, 163, 205]\n",
      "[1, 1, 96, 96, 184, 366]\n",
      "[1, 54, 201, 230, 258]\n",
      "[9, 106, 204, 238, 393]\n",
      "[3, 238, 269]\n",
      "[218, 251, 395]\n",
      "[16, 52, 117, 219]\n",
      "[17, 90, 169, 395]\n",
      "[2, 29, 119, 196, 234, 274, 297, 381]\n",
      "[3, 42, 168, 205, 247, 247, 333, 390]\n",
      "[4, 39, 395]\n",
      "[21, 375, 375]\n",
      "[14, 38, 115]\n",
      "[15, 82, 395]\n",
      "[6, 42]\n",
      "[21, 395]\n",
      "[1, 60]\n",
      "[38, 395]\n",
      "[1, 24, 381]\n",
      "[3, 360, 391]\n",
      "[10, 311]\n",
      "[277, 372]\n",
      "[4, 31, 100, 167]\n",
      "[7, 79, 148, 393]\n",
      "[0, 22, 78, 155, 177, 240, 269]\n",
      "[1, 40, 135, 156, 212, 212, 395]\n",
      "[0, 120, 264, 394]\n",
      "[1, 136, 267, 395]\n",
      "[0, 104, 145, 267]\n",
      "[1, 122, 236, 269]\n",
      "[0, 37, 186, 262]\n",
      "[1, 152, 236, 296]\n",
      "[1, 238, 343]\n",
      "[203, 281, 395]\n",
      "[1, 179, 235]\n",
      "[151, 186, 388]\n",
      "[11, 180, 225]\n",
      "[162, 206, 393]\n",
      "[1, 55, 287, 308, 378]\n",
      "[37, 268, 268, 318, 318]\n",
      "[0, 159]\n",
      "[1, 161]\n",
      "[1, 50, 323, 360, 392]\n",
      "[7, 292, 338, 338, 338]\n",
      "[1, 32, 71, 177]\n",
      "[5, 43, 121, 395]\n",
      "[0, 30, 60, 88, 138, 170, 207, 274, 315]\n",
      "[1, 39, 39, 89, 144, 144, 256, 275, 392]\n",
      "[1, 19, 61, 154, 183]\n",
      "[2, 20, 136, 164, 395]\n",
      "[0, 23, 52, 159, 187, 302]\n",
      "[1, 33, 119, 160, 284, 395]\n",
      "[1, 185, 278, 384]\n",
      "[158, 241, 364, 395]\n",
      "[1, 94, 150]\n",
      "[73, 127, 395]\n",
      "[12, 54, 138, 191, 262, 293, 350]\n",
      "[30, 59, 163, 233, 264, 322, 390]\n",
      "[5, 177]\n",
      "[158, 391]\n",
      "[0, 45, 83, 149, 174, 326]\n",
      "[1, 64, 97, 97, 195, 351]\n",
      "[7, 54, 131, 327, 377]\n",
      "[19, 19, 296, 355, 388]\n",
      "[0, 33, 76, 105, 134, 268, 302, 351]\n",
      "[1, 39, 82, 82, 247, 280, 322, 390]\n",
      "[1, 87, 160, 322]\n",
      "[67, 135, 281, 395]\n",
      "[7, 339]\n",
      "[321, 389]\n",
      "[1, 81]\n",
      "[61, 395]\n",
      "[1, 380]\n",
      "[362, 395]\n",
      "[0, 19, 211]\n",
      "[1, 189, 393]\n",
      "[1, 286, 321]\n",
      "[242, 290, 360]\n",
      "[0, 40, 62, 102, 149]\n",
      "[1, 43, 84, 124, 392]\n",
      "[1, 103, 178]\n",
      "[84, 156, 386]\n",
      "[2, 59, 79, 194]\n",
      "[41, 60, 169, 394]\n",
      "[1, 221]\n",
      "[202, 395]\n",
      "[1, 21, 45, 130, 208, 378]\n",
      "[2, 22, 106, 186, 355, 395]\n",
      "[1, 23, 70, 131, 200, 269, 321]\n",
      "[3, 42, 74, 134, 249, 286, 353]\n",
      "[1, 36, 116, 175, 209, 228, 272]\n",
      "[2, 68, 154, 176, 210, 210, 393]\n",
      "[8, 45, 120, 260, 295]\n",
      "[12, 101, 168, 268, 330]\n",
      "[0, 21, 51, 119]\n",
      "[1, 27, 53, 383]\n",
      "[1, 281, 324, 360, 382]\n",
      "[205, 205, 327, 361, 395]\n",
      "[1, 40, 149, 207, 243, 315, 361]\n",
      "[22, 41, 187, 225, 250, 340, 379]\n",
      "[1, 211, 233]\n",
      "[164, 213, 379]\n",
      "[8, 72, 99, 217, 238, 288, 376]\n",
      "[40, 78, 100, 218, 218, 300, 395]\n",
      "[2, 42, 73, 163, 269, 321, 388]\n",
      "[22, 51, 138, 236, 274, 323, 393]\n",
      "[7, 280, 336]\n",
      "[255, 293, 383]\n",
      "[1, 81, 107, 158, 230, 279, 319, 381]\n",
      "[59, 89, 139, 212, 258, 288, 361, 395]\n",
      "[1, 21, 191]\n",
      "[2, 160, 395]\n",
      "[0, 19]\n",
      "[1, 395]\n",
      "[2, 153]\n",
      "[126, 388]\n",
      "[12, 45]\n",
      "[25, 393]\n",
      "[10, 153, 222]\n",
      "[11, 165, 395]\n",
      "[0, 33, 261]\n",
      "[1, 239, 395]\n",
      "[1, 383]\n",
      "[327, 395]\n",
      "[1, 155]\n",
      "[129, 394]\n",
      "[5, 126, 177]\n",
      "[107, 141, 395]\n",
      "[6, 41]\n",
      "[22, 395]\n",
      "[2, 347]\n",
      "[329, 395]\n",
      "[1, 154]\n",
      "[78, 395]\n",
      "[1, 218, 284]\n",
      "[196, 196, 287]\n",
      "[0, 17, 66, 128, 186, 281]\n",
      "[1, 1, 104, 129, 226, 395]\n",
      "[1, 342, 390]\n",
      "[317, 352, 391]\n",
      "[1, 86, 182, 201, 244, 295, 320]\n",
      "[4, 146, 146, 212, 275, 275, 395]\n",
      "[6, 41, 92, 135, 188, 248, 317]\n",
      "[8, 8, 114, 136, 222, 296, 340]\n",
      "[0, 91, 178, 312]\n",
      "[1, 133, 294, 363]\n",
      "[11, 348, 372]\n",
      "[281, 354, 395]\n",
      "[5, 241]\n",
      "[215, 395]\n",
      "[2, 66]\n",
      "[48, 395]\n",
      "[1, 294]\n",
      "[271, 337]\n",
      "[10, 105, 144, 186]\n",
      "[87, 117, 158, 395]\n",
      "[1, 20, 73]\n",
      "[2, 55, 357]\n",
      "[1, 270]\n",
      "[148, 391]\n",
      "[0, 21, 68, 269, 368]\n",
      "[1, 23, 236, 236, 370]\n",
      "[2, 49, 71, 112, 383]\n",
      "[16, 52, 92, 333, 385]\n",
      "[6, 191, 232, 345]\n",
      "[145, 210, 251, 393]\n",
      "[1, 390]\n",
      "[365, 365]\n",
      "[1, 94, 154, 198, 258, 301]\n",
      "[73, 109, 157, 233, 233, 356]\n",
      "[11, 81, 122, 142, 264, 375]\n",
      "[52, 87, 87, 234, 335, 386]\n",
      "[1, 148, 192]\n",
      "[87, 87, 378]\n",
      "[0, 30]\n",
      "[1, 395]\n",
      "[4, 68, 392]\n",
      "[18, 372, 395]\n",
      "[14, 70, 126]\n",
      "[47, 99, 395]\n",
      "[10, 328]\n",
      "[272, 395]\n",
      "[4, 100, 392]\n",
      "[41, 360, 360]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(test_loader):\n",
    "    x = data['audio']\n",
    "    x = x.to(device)\n",
    "    starts = []\n",
    "    ends = []\n",
    "    for j in range(len(x)):\n",
    "        pred = model(x[j])\n",
    "        pred = pred.detach().cpu().squeeze().numpy()\n",
    "        pred = (pred >= 0.5).astype('float32')\n",
    "        start, end = calculate(pred)\n",
    "        starts.append(start)\n",
    "        ends.append(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6],\n",
       "       [  7],\n",
       "       [  8],\n",
       "       [  9],\n",
       "       [ 10],\n",
       "       [ 11],\n",
       "       [ 12],\n",
       "       [ 13],\n",
       "       [ 14],\n",
       "       [ 15],\n",
       "       [ 17],\n",
       "       [ 21],\n",
       "       [ 22],\n",
       "       [ 23],\n",
       "       [ 24],\n",
       "       [ 25],\n",
       "       [ 26],\n",
       "       [ 27],\n",
       "       [ 29],\n",
       "       [ 36],\n",
       "       [ 37],\n",
       "       [ 38],\n",
       "       [ 39],\n",
       "       [ 45],\n",
       "       [ 46],\n",
       "       [ 48],\n",
       "       [ 50],\n",
       "       [ 51],\n",
       "       [ 52],\n",
       "       [ 55],\n",
       "       [ 60],\n",
       "       [ 76],\n",
       "       [ 77],\n",
       "       [ 85],\n",
       "       [ 86],\n",
       "       [ 87],\n",
       "       [ 96],\n",
       "       [109],\n",
       "       [110],\n",
       "       [150],\n",
       "       [151],\n",
       "       [152],\n",
       "       [158],\n",
       "       [188],\n",
       "       [190],\n",
       "       [199],\n",
       "       [217],\n",
       "       [218],\n",
       "       [219],\n",
       "       [220],\n",
       "       [221],\n",
       "       [222],\n",
       "       [223],\n",
       "       [224],\n",
       "       [225],\n",
       "       [226],\n",
       "       [227],\n",
       "       [228],\n",
       "       [229],\n",
       "       [230],\n",
       "       [231],\n",
       "       [232],\n",
       "       [233],\n",
       "       [234],\n",
       "       [235],\n",
       "       [236],\n",
       "       [237],\n",
       "       [238],\n",
       "       [239],\n",
       "       [240],\n",
       "       [241],\n",
       "       [243],\n",
       "       [244],\n",
       "       [245],\n",
       "       [246],\n",
       "       [247],\n",
       "       [248],\n",
       "       [249],\n",
       "       [250],\n",
       "       [251],\n",
       "       [252],\n",
       "       [253],\n",
       "       [254],\n",
       "       [255],\n",
       "       [256],\n",
       "       [257],\n",
       "       [258],\n",
       "       [259],\n",
       "       [262],\n",
       "       [263],\n",
       "       [264],\n",
       "       [265],\n",
       "       [266],\n",
       "       [267],\n",
       "       [268],\n",
       "       [269],\n",
       "       [270],\n",
       "       [272],\n",
       "       [274],\n",
       "       [275],\n",
       "       [276],\n",
       "       [277],\n",
       "       [278],\n",
       "       [279],\n",
       "       [280],\n",
       "       [281],\n",
       "       [282],\n",
       "       [283],\n",
       "       [284],\n",
       "       [285],\n",
       "       [286],\n",
       "       [287],\n",
       "       [288],\n",
       "       [289],\n",
       "       [290],\n",
       "       [291],\n",
       "       [292],\n",
       "       [293],\n",
       "       [294],\n",
       "       [295],\n",
       "       [296],\n",
       "       [297],\n",
       "       [315],\n",
       "       [316],\n",
       "       [318],\n",
       "       [328],\n",
       "       [338],\n",
       "       [339],\n",
       "       [341],\n",
       "       [349],\n",
       "       [367],\n",
       "       [390],\n",
       "       [395]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(x[0])\n",
    "pred = pred.detach().cpu().squeeze().numpy()\n",
    "pred = (pred >= 0.5).astype('float32')\n",
    "np.argwhere(pred == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 30)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='./wav_data/pretrain/sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [img for img in os.listdir(root_dir) if not img.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    x = preprocessing(path, method='mfcc', sr=16000, n_mels=40, n_mfcc=40)\n",
    "    x = convert_tensor(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./wav_data/pretrain/sample/2.wav'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_name = data_list[0]\n",
    "audio_path = os.path.join(root_dir, audio_name)\n",
    "audio = load_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([396, 1, 40, 50])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "if(multi_task == 'true'):\n",
    "    model_g.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "n = 0\n",
    "\n",
    "for i in range(len(eval_samples)):\n",
    "    try:\n",
    "        X_new = preprocessing(eval_samples[i], method='mfcc', sr=16000, n_mfcc=n_mfcc)\n",
    "        X_new = convert_tensor(X_new).to(device)\n",
    "        y_new = model(X_new)\n",
    "        y_new = torch.argmax(nn.Softmax(dim=-1)(torch.mean(y_new, dim=0)))\n",
    "        #y_new = sorted(dict(collections.Counter(torch.argmax(nn.Softmax(dim=-1)(y_new), dim=1).cpu().numpy()))\n",
    "        #               .items(), key=(lambda x: x[1]), reverse=True)[0][0]\n",
    "        y_new = 1 if (y_new.item() == y[eval_idx][i].item()) else 0\n",
    "        correct += y_new\n",
    "        n += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "acc = correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuray: 0.88611\n"
     ]
    }
   ],
   "source": [
    "print('Test accuray:', round(acc, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuray: 0.85278\n"
     ]
    }
   ],
   "source": [
    "print('Test accuray:', round(acc, 5))  # 0.7111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1DCNN  \n",
    "Test accuray: 0.64722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(eval_samples) - set(train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
